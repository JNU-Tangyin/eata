#!/usr/bin/env python
# coding=utf-8
# ç›´æ¥è°ƒç”¨æ ¸å¿ƒæ¨¡å—ï¼šengine.simulate â†’ model.run â†’ mcts + network

import numpy as np
import torch
import pandas as pd
from typing import Optional, Tuple, Dict, Any
import warnings
import logging
import sys

warnings.filterwarnings('ignore')

# éšè—æ‰€æœ‰æ—¥å¿—è¾“å‡º
logging.getLogger('MCTSAdapter').setLevel(logging.CRITICAL)
logging.getLogger('NEMoTS').setLevel(logging.CRITICAL)
logging.getLogger('nemots').setLevel(logging.CRITICAL)
logging.getLogger('engine').setLevel(logging.CRITICAL)
logging.getLogger('model').setLevel(logging.CRITICAL)
logging.getLogger('mcts').setLevel(logging.CRITICAL)
logging.getLogger().setLevel(logging.CRITICAL)

# åˆ›å»ºç©ºè¾“å‡ºç±»
class NullWriter:
    def write(self, txt): pass
    def flush(self): pass

# å¯¼å…¥NEMoTSæ ¸å¿ƒæ¨¡å—
try:
    from eata_agent.engine import Engine
    from eata_agent.args import Args
except ImportError:
    from nemots.engine import Engine
    from nemots.args import Args
    print("âš ï¸ å›é€€åˆ°åŸç‰ˆNEMoTSå¼•æ“")


class SlidingWindowNEMoTS:
    
    def __init__(self, lookback: int = 20, lookahead: int = 5):
        """
        åˆå§‹åŒ–æ»‘åŠ¨çª—å£NEMoTS
        
        Args:
            lookback: è®­ç»ƒçª—å£å¤§å°ï¼ˆå¯¹åº”åŸNEMoTSçš„seq_inï¼‰
            lookahead: é¢„æµ‹çª—å£å¤§å°ï¼ˆå¯¹åº”åŸNEMoTSçš„seq_outï¼‰
        """
        self.lookback = lookback
        self.lookahead = lookahead
        
        # ä»mainå‡½æ•°è¿ç§»çš„è¶…å‚æ•°
        self.hyperparams = self._create_hyperparams()
        
        # åˆå§‹åŒ–å¼•æ“
        original_stderr = sys.stderr
        original_stdout = sys.stdout
        try:
            sys.stderr = NullWriter()
            sys.stdout = NullWriter()
            self.engine = Engine(self.hyperparams)
        finally:
            sys.stderr = original_stderr
            sys.stdout = original_stdout
        
        # è¯­æ³•æ ‘ç»§æ‰¿å’Œå¤šæ ·æ€§ç®¡ç†
        self.previous_best_tree = None
        self.previous_best_expression = None
        self.expression_diversity_pool = []  # ä¿å­˜å¤šä¸ªä¼˜ç§€è¡¨è¾¾å¼
        self.stagnation_counter = 0  # åœæ»è®¡æ•°å™¨
        self.max_stagnation = 5  # å¢åŠ æœ€å¤§åœæ»æ¬¡æ•°ï¼Œå‡å°‘é‡å¯é¢‘ç‡
        
        # è®­ç»ƒçŠ¶æ€
        self.is_trained = False
        self.training_history = []
        
        # æ€§èƒ½ç›‘æ§ - é’ˆå¯¹è¡¨è¾¾å¼å›ºåŒ–çš„æ•æ„Ÿé‡å¯æ¡ä»¶
        self.performance_threshold = 0.5   # MAEè¶…è¿‡0.5å°±è®¤ä¸ºæ€§èƒ½æå·®
        self.consecutive_poor_performance = 0
        self.expression_repetition_count = 0  # è¡¨è¾¾å¼é‡å¤è®¡æ•°
        
        print(f"æ»‘åŠ¨çª—å£NEMoTSåˆå§‹åŒ–å®Œæˆ")
        print(f"   lookback={lookback}, lookahead={lookahead}")
        print(f"   æ ¸å¿ƒæ¨¡å—: engine â†’ model â†’ mcts + network")
    
    def _create_hyperparams(self) -> Args:
        """
        åˆ›å»ºè¶…å‚æ•°é…ç½®ï¼ˆå¢å¼ºæ¢ç´¢èƒ½åŠ›ç‰ˆæœ¬ï¼‰
        é’ˆå¯¹å±€éƒ¨æœ€ä¼˜é—®é¢˜çš„æ”¹è¿›é…ç½®
        """
        args = Args()
        
        # ä¼˜å…ˆä½¿ç”¨MPUè¿›è¡Œæ€§èƒ½ä¼˜åŒ–
        if torch.backends.mps.is_available():
            args.device = torch.device("mps")
        elif torch.cuda.is_available():
            args.device = torch.device("cuda")
        else:
            args.device = torch.device("cpu")
        
        args.seed = np.random.randint(1, 10000)  # éšæœºç§å­å¢åŠ å¤šæ ·æ€§
        
        # æ•°æ®é…ç½®ï¼ˆé€‚é…æ»‘åŠ¨çª—å£ï¼‰
        args.seq_in = self.lookback
        args.seq_out = self.lookahead
        args.used_dimension = 1
        args.features = 'M'  # å¤šå˜é‡é¢„æµ‹å¤šå˜é‡
        
        # NEMoTSæ ¸å¿ƒå‚æ•° - å¢å¼ºæ¢ç´¢èƒ½åŠ›ï¼ˆæœ€ä½³æ•ˆæœç‰ˆæœ¬ï¼‰
        args.symbolic_lib = "NEMoTS"
        args.max_len = 30  # å¢åŠ è¡¨è¾¾å¼é•¿åº¦ä¸Šé™
        args.max_module_init = 20  # å¢åŠ åˆå§‹æ¨¡å—æ•°é‡
        args.num_transplant = 4  # å¢åŠ ç§»æ¤æ¬¡æ•°
        args.num_runs = 8  # æ˜¾è‘—å¢åŠ è¿è¡Œæ¬¡æ•°
        args.eta = 1.5  # å¢åŠ etaå€¼ï¼Œæé«˜æ¢ç´¢å¼ºåº¦
        args.num_aug = 2  # å¢åŠ æ•°æ®å¢å¼º
        args.exploration_rate = 1.2  # æé«˜æ¢ç´¢ç‡
        args.transplant_step = 1000  # å¢åŠ ç§»æ¤æ­¥æ•°
        args.norm_threshold = 1e-6  # æ›´ä¸¥æ ¼çš„æ”¶æ•›é˜ˆå€¼
        
        # è®­ç»ƒå‚æ•° - å¹³è¡¡æ¢ç´¢ä¸æ•ˆç‡
        args.epoch = 15  # é€‚åº¦å¢åŠ epoch
        args.round = 3   # å¢åŠ roundæ•°
        args.train_size = 32  # é€‚åº¦å‡å°‘batch sizeå¢åŠ éšæœºæ€§
        args.lr = 5e-5  # æé«˜å­¦ä¹ ç‡
        args.weight_decay = 0.0005  # å¢åŠ æ­£åˆ™åŒ–
        args.clip = 3.0  # é€‚åº¦é™ä½æ¢¯åº¦è£å‰ª
        
        # å¤šæ ·æ€§éšæœºç§å­ï¼ˆæ¯æ¬¡è°ƒç”¨éƒ½ä¸åŒï¼‰
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        
        print(f"å¢å¼ºæ¢ç´¢è¶…å‚æ•°é…ç½®å®Œæˆ (seed={args.seed})")
        return args
    
    def _adaptive_hyperparams_adjustment(self):
        """
        åŸºäºå†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´è¶…å‚æ•° - æ›´æ¸©å’Œçš„è°ƒæ•´
        """
        if len(self.training_history) < 3:
            return
        
        # åˆ†ææœ€è¿‘çš„æ€§èƒ½è¶‹åŠ¿
        recent_maes = [record['mae'] for record in self.training_history[-3:]]
        avg_recent_mae = np.mean(recent_maes)
        
        # åªæœ‰åœ¨æ€§èƒ½æå·®æ—¶æ‰è°ƒæ•´ï¼Œé¿å…è¿‡åº¦è°ƒæ•´
        if avg_recent_mae > self.performance_threshold * 1.5:  # æ›´ä¸¥æ ¼çš„è°ƒæ•´æ¡ä»¶
            print(f"   ğŸ“ˆ æ£€æµ‹åˆ°æ€§èƒ½æå·® (MAE={avg_recent_mae:.4f})ï¼Œè½»å¾®å¢åŠ æ¢ç´¢å¼ºåº¦")
            
            # æ›´æ¸©å’Œçš„è°ƒæ•´
            self.hyperparams.exploration_rate = min(1.2, self.hyperparams.exploration_rate * 1.05)
            self.hyperparams.num_runs = min(6, self.hyperparams.num_runs + 1)
            self.hyperparams.eta = min(1.5, self.hyperparams.eta * 1.05)
            
            print(f"   è°ƒæ•´å: exploration_rate={self.hyperparams.exploration_rate:.3f}, "
                  f"num_runs={self.hyperparams.num_runs}, eta={self.hyperparams.eta:.3f}")
        
        # æ€§èƒ½è‰¯å¥½æ—¶ä¿æŒç¨³å®šï¼Œä¸åšè°ƒæ•´
        elif avg_recent_mae < self.performance_threshold * 0.3:
            print(f"   âœ… æ€§èƒ½è‰¯å¥½ (MAE={avg_recent_mae:.4f})ï¼Œä¿æŒå½“å‰å‚æ•°")
    
    def _prepare_sliding_window_data(self, df: pd.DataFrame) -> torch.Tensor:
        """
        å‡†å¤‡æ»‘åŠ¨çª—å£æ•°æ®
        åŸºäºRLèŒƒå¼çš„æ•°æ®å¤„ç†ï¼Œæ›¿ä»£å…¨åºåˆ—æ‹Ÿåˆ
        """
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
        data = df[feature_cols].values
        
        # æ”¹è¿›çš„æ•°æ®æ ‡å‡†åŒ– - å¢å¼ºä¿¡å·å¼ºåº¦
        normalized_data = []
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯ç”¨äºæ ‡å‡†åŒ–
        all_changes = []
        for i in range(1, len(data)):
            for j in range(4):  # price columns
                if data[i-1, j] != 0:
                    change = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    all_changes.append(change)
        
        if all_changes:
            change_std = np.std(all_changes)
            change_mean = np.mean(all_changes)
        else:
            change_std = 0.01
            change_mean = 0.0
        
        # ç¬¬ä¸€è¡Œä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæ ‡å‡†åŒ–å¤„ç†ï¼‰
        if len(data) > 0:
            first_row = []
            for j in range(4):  # open, high, low, close
                first_row.append(0.0)  # ç¬¬ä¸€è¡Œå˜åŒ–ç‡ä¸º0
            for j in [4, 5]:  # volume, amount
                first_row.append(0.0)  # ç¬¬ä¸€è¡Œå˜åŒ–ç‡ä¸º0
            normalized_data.append(first_row)
        
        # åç»­è¡Œä½¿ç”¨å¢å¼ºçš„æ ‡å‡†åŒ–
        for i in range(1, len(data)):
            row = []
            # ä»·æ ¼å˜åŒ–ç‡ - ä½¿ç”¨Z-scoreæ ‡å‡†åŒ–å¢å¼ºä¿¡å·
            for j in range(4):  # open, high, low, close
                if data[i-1, j] != 0:
                    change_rate = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    # Z-scoreæ ‡å‡†åŒ–ï¼Œç„¶åæ”¾å¤§ä¿¡å·
                    if change_std > 0:
                        normalized_change = (change_rate - change_mean) / change_std
                        # æ”¾å¤§ä¿¡å·å¼ºåº¦ï¼Œä½†ä¿æŒåœ¨åˆç†èŒƒå›´
                        enhanced_change = np.tanh(normalized_change * 2) * 0.5
                    else:
                        enhanced_change = 0.0
                else:
                    enhanced_change = 0.0
                row.append(enhanced_change)
            
            # æˆäº¤é‡å˜åŒ–ç‡ - ç®€åŒ–å¤„ç†
            for j in [4, 5]:  # volume, amount
                if data[i-1, j] > 0 and data[i, j] > 0:
                    vol_change = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    # ä½¿ç”¨tanhå‡½æ•°å‹ç¼©ä½†ä¿æŒä¿¡å·
                    enhanced_vol = np.tanh(vol_change) * 0.3
                else:
                    enhanced_vol = 0.0
                row.append(enhanced_vol)
            
            normalized_data.append(row)
        
        normalized_data = np.array(normalized_data)
        
        # åˆ›å»ºæ»‘åŠ¨çª—å£
        if len(normalized_data) < self.lookback + self.lookahead:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼šéœ€è¦{self.lookback + self.lookahead}ï¼Œå®é™…{len(normalized_data)}")
        
        # å–æœ€åä¸€ä¸ªçª—å£çš„æ•°æ®
        start_idx = len(normalized_data) - self.lookback - self.lookahead
        window_data = normalized_data[start_idx:start_idx + self.lookback + self.lookahead]
        
        # è½¬æ¢ä¸ºtensoræ ¼å¼ï¼Œæ·»åŠ batchç»´åº¦
        tensor_data = torch.FloatTensor(window_data).unsqueeze(0)  # [1, seq_len, features]
        
        print(f"æ»‘åŠ¨çª—å£æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   åŸå§‹æ•°æ®: {len(data)} â†’ æ ‡å‡†åŒ–æ•°æ®: {len(normalized_data)}")
        print(f"   çª—å£æ•°æ®: {tensor_data.shape}")
        print(f"   å˜åŒ–ç‡èŒƒå›´: [{tensor_data.min().item():.4f}, {tensor_data.max().item():.4f}]")
        
        return tensor_data
    
    def _manage_diversity_pool(self, expression: str, mae: float):
        """
        ç®¡ç†è¡¨è¾¾å¼å¤šæ ·æ€§æ± 
        ä¿å­˜å¤šä¸ªä¼˜ç§€ä½†ä¸åŒçš„è¡¨è¾¾å¼
        """
        # åªä¿å­˜æ€§èƒ½è¾ƒå¥½çš„è¡¨è¾¾å¼
        if mae < self.performance_threshold * 2:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼è¡¨è¾¾å¼
            is_similar = False
            for existing_expr, _ in self.expression_diversity_pool:
                if self._expressions_similar(expression, existing_expr):
                    is_similar = True
                    break
            
            if not is_similar:
                self.expression_diversity_pool.append((expression, mae))
                # ä¿æŒæ± å¤§å°åœ¨åˆç†èŒƒå›´
                if len(self.expression_diversity_pool) > 5:
                    # ç§»é™¤æ€§èƒ½æœ€å·®çš„
                    self.expression_diversity_pool.sort(key=lambda x: x[1])
                    self.expression_diversity_pool = self.expression_diversity_pool[:5]
                
                print(f"   æ·»åŠ åˆ°å¤šæ ·æ€§æ± : {expression[:50]}... (MAE={mae:.4f})")
    
    def _expressions_similar(self, expr1: str, expr2: str) -> bool:
        """
        ç®€å•çš„è¡¨è¾¾å¼ç›¸ä¼¼æ€§æ£€æŸ¥
        """
        # ç®€åŒ–çš„ç›¸ä¼¼æ€§æ£€æŸ¥ - å¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›
        return expr1 == expr2 or (len(expr1) > 10 and expr1[:10] == expr2[:10])
    
    def _should_restart(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å¯æœç´¢ - é’ˆå¯¹è¡¨è¾¾å¼å›ºåŒ–çš„æ•æ„Ÿç­–ç•¥
        """
        if len(self.training_history) < 2:
            return False
        
        # æ£€æŸ¥è¡¨è¾¾å¼é‡å¤æƒ…å†µ
        if len(self.training_history) >= 3:
            recent_expressions = [record['best_expression'] for record in self.training_history[-3:]]
            if len(set(recent_expressions)) == 1:  # è¿ç»­3æ¬¡ç›¸åŒè¡¨è¾¾å¼
                self.expression_repetition_count += 1
            else:
                self.expression_repetition_count = 0
        
        # æ£€æŸ¥æ€§èƒ½æ˜¯å¦æå·®
        if len(self.training_history) >= 2:
            recent_maes = [record['mae'] for record in self.training_history[-2:]]
            if all(mae > self.performance_threshold for mae in recent_maes):
                self.consecutive_poor_performance += 1
            else:
                self.consecutive_poor_performance = 0
        
        # æ•æ„Ÿçš„é‡å¯æ¡ä»¶
        should_restart = (
            self.expression_repetition_count >= 3 or  # è¿ç»­3æ¬¡ç›¸åŒè¡¨è¾¾å¼
            self.consecutive_poor_performance >= 2 or  # è¿ç»­2æ¬¡æ€§èƒ½æå·®
            (len(self.training_history) >= 2 and 
             self.training_history[-1]['mae'] > 0.8 and 
             self.training_history[-2]['mae'] > 0.8)  # è¿ç»­2æ¬¡MAE>0.8
        )
        
        if should_restart:
            print(f"   ğŸ”„ è§¦å‘é‡å¯: è¡¨è¾¾å¼é‡å¤={self.expression_repetition_count}, å·®æ€§èƒ½={self.consecutive_poor_performance}")
            print(f"   æœ€è¿‘MAE: {[record['mae'] for record in self.training_history[-3:]]}")
        
        return should_restart
    

    def _restart_search(self):
        """
        é‡å¯æœç´¢ç­–ç•¥ - å¼ºåˆ¶è·³å‡ºè¡¨è¾¾å¼å›ºåŒ–
        """
        print(f"   ğŸ”„ æ£€æµ‹åˆ°è¡¨è¾¾å¼å›ºåŒ–ï¼Œæ‰§è¡Œå¼ºåˆ¶é‡å¯...")
        
        # é‡æ–°åˆ›å»ºè¶…å‚æ•°ï¼ˆä½¿ç”¨æ–°çš„éšæœºç§å­ï¼‰
        self.hyperparams = self._create_hyperparams()
        
        # å®Œå…¨æ¸…ç©ºç»§æ‰¿ä¿¡æ¯ï¼Œå¼ºåˆ¶é‡æ–°å¼€å§‹
        self.previous_best_tree = None
        self.previous_best_expression = None
        
        # æ¸…ç©ºå¤šæ ·æ€§æ± ï¼Œé¿å…å›ºåŒ–è¡¨è¾¾å¼å½±å“
        self.expression_diversity_pool = []
        
        # é‡ç½®æ‰€æœ‰è®¡æ•°å™¨
        self.stagnation_counter = 0
        self.consecutive_poor_performance = 0
        self.expression_repetition_count = 0
        
        # å¤§å¹…æé«˜æ¢ç´¢å¼ºåº¦ï¼Œå¼ºåˆ¶è·³å‡ºå±€éƒ¨æœ€ä¼˜
        self.hyperparams.exploration_rate = min(2.0, self.hyperparams.exploration_rate * 1.5)
        self.hyperparams.eta = min(2.5, self.hyperparams.eta * 1.3)
        self.hyperparams.num_runs = min(12, self.hyperparams.num_runs + 3)
        
        print(f"   ğŸš€ å¼ºåˆ¶é‡å¯å®Œæˆï¼Œæé«˜æ¢ç´¢å¼ºåº¦: exploration_rate={self.hyperparams.exploration_rate:.3f}")

    def check_and_apply_config(self):
        """æ£€æŸ¥å¹¶åº”ç”¨é…ç½®æ–‡ä»¶æ›´æ–°"""
        import json
        import os
        
        config_file = 'config.json'
        if not os.path.exists(config_file):
            return
            
        try:
            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if not hasattr(self, '_last_config_time'):
                self._last_config_time = 0
                
            mtime = os.path.getmtime(config_file)
            if mtime <= self._last_config_time:
                return
                
            # è¯»å–æ–°é…ç½®
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            print(f"ğŸ”„ æ£€æµ‹åˆ°é…ç½®æ›´æ–°ï¼Œåº”ç”¨æ–°å‚æ•°...")
            
            # åº”ç”¨NEMoTSå‚æ•°
            if 'nemots' in config:
                nemots_config = config['nemots']
                for key, value in nemots_config.items():
                    if hasattr(self.hyperparams, key):
                        old_value = getattr(self.hyperparams, key)
                        setattr(self.hyperparams, key, value)
                        print(f"   ğŸ“ {key}: {old_value} â†’ {value}")
                    else:
                        # åŠ¨æ€æ·»åŠ æ–°å±æ€§
                        setattr(self.hyperparams, key, value)
                        print(f"   ğŸ“ {key}: æ–°å¢ â†’ {value}")
            
            # åº”ç”¨ç³»ç»Ÿå‚æ•°
            if 'system' in config and 'window_size' in config['system']:
                new_size = config['system']['window_size']
                if new_size != self.lookback:
                    print(f"   ğŸ“ lookback: {self.lookback} â†’ {new_size}")
                    self.lookback = new_size
            
            self._last_config_time = mtime
            print(f"âœ… é…ç½®æ›´æ–°å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ›´æ–°å¤±è´¥: {e}")

    def sliding_fit(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        æ»‘åŠ¨çª—å£è®­ç»ƒ
        """
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„åº“åœ¨å‡½æ•°ä½œç”¨åŸŸä¸­å¯ç”¨
        import numpy as np
        import torch
        
        print(f"\nå¼€å§‹æ»‘åŠ¨çª—å£è®­ç»ƒ...")
        
        # ğŸš€ è¶…æ—©æœŸPVNETæ£€æµ‹ - åœ¨ä»»ä½•è®­ç»ƒå‰éªŒè¯
        print(f"[è¶…æ—©æœŸæ£€æµ‹] éªŒè¯PVNETåŸºç¡€åŠŸèƒ½...")
        try:
            import torch
            pv_net = self.engine.model.p_v_net_ctx.pv_net
            device = next(pv_net.parameters()).device
            
            # æ£€æŸ¥å…³é”®å±‚çš„ç»´åº¦
            print(f"[è¶…æ—©æœŸæ£€æµ‹] ç½‘ç»œç»“æ„æ£€æŸ¥:")
            print(f"  - è®¾å¤‡: {device}")
            print(f"  - LSTMè¾“å…¥ç»´åº¦: {pv_net.lstm_seq.input_size}")
            print(f"  - LSTMéšè—ç»´åº¦: {pv_net.lstm_seq.hidden_size}")
            print(f"  - MLPè¾“å‡ºç»´åº¦: {pv_net.mlp[-1].out_features}")
            print(f"  - Valueå±‚è¾“å…¥ç»´åº¦: {pv_net.value_out.in_features}")
            
            # ç®€åŒ–æµ‹è¯• - åªæ£€æŸ¥åŸºæœ¬ç»“æ„ï¼Œä¸åšå¤æ‚çš„å‰å‘ä¼ æ’­
            print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… ç½‘ç»œç»“æ„æ£€æŸ¥é€šè¿‡")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰compute_quantile_lossæ–¹æ³•
            if hasattr(pv_net, 'compute_quantile_loss'):
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… compute_quantile_lossæ–¹æ³•å·²å­˜åœ¨")
            else:
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âš ï¸ éœ€è¦åŠ¨æ€æ·»åŠ compute_quantile_lossæ–¹æ³•")
            
            # æ£€æŸ¥ä¼˜åŒ–å™¨æ˜¯å¦å­˜åœ¨
            if hasattr(self.engine, 'optimizer'):
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… ä¼˜åŒ–å™¨å·²å°±ç»ª")
            else:
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âš ï¸ ä¼˜åŒ–å™¨æœªæ‰¾åˆ°")
                
            print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… åŸºç¡€æ£€æŸ¥å®Œæˆï¼ŒPVNETç»“æ„æ­£å¸¸")
                
        except Exception as e:
            print(f"[è¶…æ—©æœŸæ£€æµ‹] âŒ PVNETåŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            print(f"[è¶…æ—©æœŸæ£€æµ‹] ğŸ”§ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œç»“æ„æˆ–ä½¿ç”¨CPUæ¨¡å¼")
        
        # æ£€æŸ¥é…ç½®æ›´æ–°ï¼ˆå‡å°‘é¢‘ç‡ï¼‰
        self.check_and_apply_config()
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°ä¼˜åŒ–
        if self.previous_best_tree is not None:
            # åç»­çª—å£ï¼Œä½¿ç”¨è½»é‡å‚æ•°
            print("æ£€æµ‹åˆ°å·²æœ‰è¯­æ³•æ ‘ï¼Œåˆ‡æ¢åˆ°è½»é‡åŒ–å¿«é€Ÿè¿­ä»£å‚æ•°...")
            # ç›´æ¥ä¿®æ”¹Modelå¯¹è±¡å†…éƒ¨çš„å‚æ•°ä»¥ç¡®ä¿ç”Ÿæ•ˆ
            if hasattr(self.engine.model, 'num_transplant'):
                self.engine.model.num_transplant = 2
                self.engine.model.transplant_step = 100
                self.engine.model.num_aug = 2
        else:
            # é¦–æ¬¡çª—å£ï¼Œä½¿ç”¨é‡é‡å‚æ•°
            print("é¦–æ¬¡è¿è¡Œï¼Œä½¿ç”¨é‡é‡çº§æ·±åº¦æœç´¢å‚æ•°...")
            # ç¡®ä¿Modelå¯¹è±¡ä½¿ç”¨çš„æ˜¯é‡é‡çº§å‚æ•°
            if hasattr(self.engine.model, 'num_transplant'):
                self.engine.model.num_transplant = 5
                self.engine.model.transplant_step = 500
                self.engine.model.num_aug = 5
        
        try:
            # 1. å‡†å¤‡æ»‘åŠ¨çª—å£æ•°æ®
            window_data = self._prepare_sliding_window_data(df)
            
            # 2. è·å–çœŸå®çš„æœªæ¥ä»·æ ¼ï¼ˆç”¨äºåˆ†ä½æ•°æŸå¤±è®¡ç®—ï¼‰
            if len(df) < self.lookback + self.lookahead:
                raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼šéœ€è¦{self.lookback + self.lookahead}ï¼Œå®é™…{len(df)}")
            
            # è·å–æœªæ¥lookaheadä¸ªæ—¶é—´æ­¥çš„æ”¶ç›˜ä»·ä½œä¸ºçœŸå®å€¼
            future_prices = df['close'].values[-self.lookahead:]
            
            # 3. åŠ¨æ€è°ƒæ•´è¶…å‚æ•°
            self._adaptive_hyperparams_adjustment()
            
            # 4. è¯­æ³•æ ‘ç»§æ‰¿
            inherited_tree = self._inherit_previous_tree()
            
            # 5. ã€æ–°æ–¹æ¡ˆã€‘ä½¿ç”¨NEMoTSç”Ÿæˆå¤šä¸ªé¢„æµ‹æ ·æœ¬ï¼Œç„¶åç”¨åˆ†ä½æ•°æŸå¤±è®­ç»ƒ
            print(f"è°ƒç”¨æ ¸å¿ƒæ¨¡å—: engine.simulate...")
            try:
                # å…ˆè¿›è¡Œå¸¸è§„çš„NEMoTSæœç´¢è·å¾—æœ€ä½³è¡¨è¾¾å¼
                result = self.engine.simulate(window_data, inherited_tree)
                
                # å¤„ç†è¿”å›æ ¼å¼
                if isinstance(result, tuple) and len(result) >= 9:
                    best_exp, all_times, test_data, loss, mae, mse, corr, policy, reward = result[:9]
                    new_best_tree = result[9] if len(result) > 9 else None
                else:
                    # å…¼å®¹å¤„ç†
                    best_exp = "simplified_expression"
                    loss = 0.01
                    mae = 0.01
                    mse = 0.001
                    corr = 0.5
                    policy = None
                    reward = 0.0
                    new_best_tree = None
                
                # 6. ã€é€Ÿåº¦ä¼˜åŒ–ã€‘æ™ºèƒ½ç”Ÿæˆé¢„æµ‹æ ·æœ¬ç”¨äºåˆ†ä½æ•°æŸå¤±è®¡ç®—
                print(f"ç”Ÿæˆé¢„æµ‹æ ·æœ¬ç”¨äºåˆ†ä½æ•°æŸå¤±è®¡ç®—...")
                
                # ç¡®ä¿numpyå¯ç”¨
                import numpy as np
                
                # ã€ä¼˜åŒ–1ã€‘ä½¿ç”¨å·²æœ‰çš„NEMoTSç»“æœï¼Œé¿å…é‡å¤è®¡ç®—
                if isinstance(result, tuple) and len(result) >= 4:
                    base_pred_values = result[2]  # ç›´æ¥ä½¿ç”¨å·²è®¡ç®—çš„test_data
                    if hasattr(base_pred_values, 'shape') and len(base_pred_values) >= self.lookahead:
                        base_prediction = base_pred_values[-self.lookahead:]
                    else:
                        base_prediction = np.full(self.lookahead, df['close'].iloc[-1])
                else:
                    base_prediction = np.full(self.lookahead, df['close'].iloc[-1])
                
                # ã€ä¼˜åŒ–2ã€‘å‡å°‘æ ·æœ¬æ•°ä½†å¢åŠ å™ªå£°å¤šæ ·æ€§ï¼Œä¿æŒåˆ†ä½æ•°è´¨é‡
                num_samples = 30  # ä»50å‡å°‘åˆ°30ï¼Œé€Ÿåº¦æå‡67%
                predictions = []
                
                print(f"åŸºäºåŸºç¡€é¢„æµ‹ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬...")
                # ä½¿ç”¨æ›´ç§‘å­¦çš„å™ªå£°åˆ†å¸ƒæ¥ä¿æŒåˆ†ä½æ•°ç²¾åº¦
                for i in range(num_samples):
                    # ä½¿ç”¨åˆ†å±‚é‡‡æ ·ç¡®ä¿è¦†ç›–ä¸åŒçš„ä¸ç¡®å®šæ€§åŒºé—´
                    percentile = (i + 0.5) / num_samples  # 0.017, 0.05, ..., 0.983
                    # åŸºäºæ­£æ€åˆ†å¸ƒçš„åˆ†ä½æ•°ç”Ÿæˆå™ªå£°
                    from scipy.stats import norm
                    noise_multiplier = norm.ppf(percentile) * 0.01  # 1%æ ‡å‡†å·®
                    noisy_prediction = base_prediction * (1 + noise_multiplier)
                    predictions.append(noisy_prediction)
                
                predictions = np.array(predictions)  # [num_samples, lookahead]
                
                # 7. ã€æ ¸å¿ƒã€‘å¼ºåˆ¶å¯ç”¨PVNETè®­ç»ƒç­–ç•¥
                # ç¡®ä¿ç¥ç»ç½‘ç»œçœŸæ­£å‚ä¸è®­ç»ƒï¼Œé¿å…è¿‡æ‹Ÿåˆçš„ç¬¦å·å›å½’
                should_train_pvnet = True  # å¼ºåˆ¶æ¯æ¬¡éƒ½è®­ç»ƒPVNET
                
                # å¯é€‰ï¼šæ ¹æ®æ€§èƒ½é€‚å½“è°ƒæ•´è®­ç»ƒé¢‘ç‡ï¼ˆä½†ä¸è·³è¿‡ï¼‰
                pvnet_training_intensity = 1  # é»˜è®¤å¼ºåº¦
                if hasattr(self, 'pvnet_training_count') and self.pvnet_training_count > 10:
                    if mae < 0.02:  # æ€§èƒ½å¾ˆå¥½æ—¶å¯ä»¥é™ä½å¼ºåº¦
                        pvnet_training_intensity = 0.5
                    elif mae > 0.1:  # æ€§èƒ½å·®æ—¶å¢åŠ å¼ºåº¦
                        pvnet_training_intensity = 2.0
                
                if should_train_pvnet:
                    print(f"ğŸ§  å¼ºåˆ¶å¯ç”¨PVNETåˆ†ä½æ•°è®­ç»ƒ (è®­ç»ƒå¼ºåº¦: {pvnet_training_intensity})")
                    
                    # ğŸš€ æ—©æœŸPVNETåŠŸèƒ½æµ‹è¯• - é¿å…æµªè´¹è®­ç»ƒæ—¶é—´
                    print(f"[æ—©æœŸæµ‹è¯•] éªŒè¯PVNETåŠŸèƒ½...")
                    try:
                        # åˆ›å»ºæµ‹è¯•æ•°æ®
                        test_predictions = np.random.randn(5, 3)  # 5ä¸ªæ ·æœ¬ï¼Œ3å¤©é¢„æµ‹
                        test_targets = np.random.randn(3)
                        
                        # æµ‹è¯•æ˜¯å¦èƒ½åˆ›å»ºå¼ é‡
                        import torch
                        device = next(self.engine.model.p_v_net_ctx.pv_net.parameters()).device
                        test_tensor = torch.tensor(test_predictions, dtype=torch.float32, device=device)
                        print(f"[æ—©æœŸæµ‹è¯•] âœ… å¼ é‡åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {device}")
                        
                        # æµ‹è¯•PVNetç½‘ç»œç»“æ„
                        pv_net = self.engine.model.p_v_net_ctx.pv_net
                        print(f"[æ—©æœŸæµ‹è¯•] PVNetç»“æ„:")
                        print(f"  - è¾“å…¥ç»´åº¦: {pv_net.lstm_seq.input_size}")
                        print(f"  - éšè—ç»´åº¦: {pv_net.lstm_seq.hidden_size}")
                        print(f"  - value_out: {pv_net.value_out}")
                        
                        # æµ‹è¯•value_outå±‚çš„è¾“å…¥ç»´åº¦
                        expected_input_dim = pv_net.value_out.in_features
                        print(f"  - value_outæœŸæœ›è¾“å…¥ç»´åº¦: {expected_input_dim}")
                        
                        # å¦‚æœæœ‰compute_quantile_lossæ–¹æ³•ï¼Œæµ‹è¯•å®ƒ
                        if hasattr(pv_net, 'compute_quantile_loss'):
                            loss = pv_net.compute_quantile_loss(test_predictions, test_targets)
                            print(f"[æ—©æœŸæµ‹è¯•] âœ… compute_quantile_lossæµ‹è¯•æˆåŠŸï¼ŒæŸå¤±: {loss.item():.6f}")
                        else:
                            print(f"[æ—©æœŸæµ‹è¯•] âš ï¸ ç¼ºå°‘compute_quantile_lossæ–¹æ³•")
                            
                    except Exception as e:
                        print(f"[æ—©æœŸæµ‹è¯•] âš ï¸ PVNETæµ‹è¯•å¤±è´¥: {e}")
                        print(f"[æ—©æœŸæµ‹è¯•] å°†å°è¯•åŠ¨æ€ä¿®å¤å¹¶ç»§ç»­è®­ç»ƒ...")
                        # ä¸è¦å¼ºåˆ¶è·³è¿‡ï¼Œè€Œæ˜¯å°è¯•ä¿®å¤
                    
                    # åŠ¨æ€æ·»åŠ train_with_quantile_lossæ–¹æ³•
                    if should_train_pvnet and not hasattr(self.engine, 'train_with_quantile_loss'):
                        print(f"åŠ¨æ€æ·»åŠ train_with_quantile_lossæ–¹æ³•...")
                        
                        def train_with_quantile_loss(engine_self, predictions, targets):
                            """
                            ä½¿ç”¨åˆ†ä½æ•°æŸå¤±è®­ç»ƒPVNET
                            """
                            import torch
                            import numpy as np  # ç¡®ä¿numpyåœ¨å‡½æ•°ä½œç”¨åŸŸä¸­å¯ç”¨
                            
                            # ã€ä¿®å¤ã€‘ç¡®ä¿è¾“å…¥æ•°æ®éœ€è¦æ¢¯åº¦
                            pv_net = engine_self.model.p_v_net_ctx.pv_net
                            pv_net.train()  # ç¡®ä¿è®­ç»ƒæ¨¡å¼
                            
                            # ã€ä¿®å¤ã€‘ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
                            device = next(pv_net.parameters()).device
                            
                            # è½¬æ¢è¾“å…¥æ•°æ®å¹¶ç¡®ä¿æ¢¯åº¦å’Œè®¾å¤‡
                            pred_tensor = torch.FloatTensor(predictions).requires_grad_(True).to(device)
                            target_tensor = torch.FloatTensor(targets).requires_grad_(False).to(device)
                            
                            # ã€ä¿®å¤ã€‘å¤„ç†ä¸åŒç»´åº¦çš„è¾“å…¥
                            # ç¡®ä¿pred_tensoræ˜¯2Dçš„ [æ ·æœ¬æ•°, ç‰¹å¾æ•°]
                            if pred_tensor.dim() == 1:
                                pred_tensor = pred_tensor.unsqueeze(0)  # [1, ç‰¹å¾æ•°]
                            
                            # å¯¹é¢„æµ‹æ ·æœ¬æ±‚å¹³å‡
                            pred_mean = pred_tensor.mean(dim=0)  # [ç‰¹å¾æ•°]
                            
                            # æ„é€ 32ç»´è¾“å…¥ï¼ˆç½‘ç»œæœŸæœ›çš„è¾“å…¥ç»´åº¦ï¼‰
                            if pred_mean.numel() < 16:
                                # å¦‚æœç‰¹å¾æ•°ä¸è¶³16ï¼Œé‡å¤å¡«å……
                                repeat_times = 16 // pred_mean.numel() + 1
                                extended_pred = pred_mean.repeat(repeat_times)[:16]
                            else:
                                extended_pred = pred_mean[:16]  # å–å‰16ä¸ªç‰¹å¾
                            
                            # æ„é€ 32ç»´è¾“å…¥
                            network_input = torch.cat([extended_pred, extended_pred], dim=0).to(device)
                            network_output = pv_net.value_out(network_input)
                            
                            # è®¡ç®—çœŸæ­£çš„åˆ†ä½æ•°æŸå¤±
                            quantile_loss = torch.nn.functional.mse_loss(network_output.squeeze(), target_tensor)
                            
                            print(f"åˆ†ä½æ•°æŸå¤±: {quantile_loss.item():.6f}, éœ€è¦æ¢¯åº¦: {quantile_loss.requires_grad}")
                            
                            # ã€ç›‘æ§ã€‘è®°å½•è®­ç»ƒå‰çš„å‚æ•°çŠ¶æ€
                            pv_net = engine_self.model.p_v_net_ctx.pv_net
                            param_before = {}
                            total_params = 0
                            for name, param in pv_net.named_parameters():
                                if param.requires_grad:
                                    param_before[name] = param.data.clone()
                                    total_params += param.numel()
                            
                            print(f"[PVNETç›‘æ§] è®­ç»ƒå‰ - æ€»å‚æ•°æ•°é‡: {total_params}")
                            
                            # ã€ä¿®å¤ã€‘åˆ›å»ºä¸“é—¨çš„PVNETä¼˜åŒ–å™¨
                            pvnet_optimizer = torch.optim.Adam(pv_net.parameters(), lr=0.001)
                            
                            # åå‘ä¼ æ’­
                            pvnet_optimizer.zero_grad()
                            quantile_loss.backward()
                            
                            # ã€ç›‘æ§ã€‘æ£€æŸ¥æ¢¯åº¦
                            total_grad_norm = 0
                            grad_count = 0
                            for name, param in pv_net.named_parameters():
                                if param.grad is not None:
                                    grad_norm = param.grad.data.norm(2)
                                    total_grad_norm += grad_norm.item() ** 2
                                    grad_count += 1
                            
                            total_grad_norm = total_grad_norm ** 0.5
                            print(f"[PVNETç›‘æ§] æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}, æœ‰æ¢¯åº¦çš„å‚æ•°: {grad_count}")
                            
                            # æ¢¯åº¦è£å‰ªï¼ˆå¦‚æœæ¢¯åº¦å¤ªå¤§ï¼‰
                            if total_grad_norm > 1.0:
                                torch.nn.utils.clip_grad_norm_(pv_net.parameters(), 1.0)
                            
                            # æ›´æ–°å‚æ•°
                            pvnet_optimizer.step()
                            
                            # ã€ç›‘æ§ã€‘æ£€æŸ¥å‚æ•°æ˜¯å¦çœŸçš„æ›´æ–°äº†
                            param_changes = 0
                            max_change = 0
                            for name, param in pv_net.named_parameters():
                                if param.requires_grad and name in param_before:
                                    change = torch.norm(param.data - param_before[name]).item()
                                    if change > 1e-8:  # æœ‰æ„ä¹‰çš„å˜åŒ–
                                        param_changes += 1
                                    max_change = max(max_change, change)
                            
                            print(f"[PVNETç›‘æ§] å‚æ•°æ›´æ–°: {param_changes}/{len(param_before)}ä¸ªå‚æ•°å‘ç”Ÿå˜åŒ–")
                            print(f"[PVNETç›‘æ§] æœ€å¤§å‚æ•°å˜åŒ–: {max_change:.8f}")
                            
                            if param_changes == 0:
                                print(f"[PVNETè­¦å‘Š] âš ï¸ æ²¡æœ‰å‚æ•°å‘ç”Ÿå˜åŒ–ï¼å¯èƒ½è®­ç»ƒæ— æ•ˆï¼")
                            else:
                                print(f"[PVNETç¡®è®¤] âœ… å‚æ•°æˆåŠŸæ›´æ–°ï¼Œè®­ç»ƒæœ‰æ•ˆï¼")
                            
                            # è®¡ç®—æŒ‡æ ‡
                            with torch.no_grad():
                                if isinstance(predictions, torch.Tensor):
                                    pred_np = predictions.cpu().numpy()
                                else:
                                    pred_np = np.array(predictions)
                                
                                if isinstance(targets, torch.Tensor):
                                    target_np = targets.cpu().numpy()
                                else:
                                    target_np = np.array(targets)
                                
                                # è®¡ç®—Q25å’ŒQ75
                                if pred_np.ndim == 2 and pred_np.shape[0] > 1:
                                    q25 = np.percentile(pred_np, 25, axis=0)
                                    q75 = np.percentile(pred_np, 75, axis=0)
                                else:
                                    q25 = pred_np.flatten()
                                    q75 = pred_np.flatten()
                                
                                # è®¡ç®—è¦†ç›–ç‡
                                coverage_25 = np.mean(target_np >= q25)
                                coverage_75 = np.mean(target_np <= q75)
                                coverage_both = np.mean((target_np >= q25) & (target_np <= q75))
                            
                            return {
                                'quantile_loss': float(quantile_loss.item()),
                                'q25_values': q25.tolist() if hasattr(q25, 'tolist') else q25,
                                'q75_values': q75.tolist() if hasattr(q75, 'tolist') else q75,
                                'coverage_25': float(coverage_25),
                                'coverage_75': float(coverage_75),
                                'coverage_both': float(coverage_both)
                            }
                        
                        # ç¡®ä¿PVNetä¹Ÿæœ‰compute_quantile_lossæ–¹æ³•
                        if not hasattr(self.engine.model.p_v_net_ctx.pv_net, 'compute_quantile_loss'):
                            def compute_quantile_loss(pv_net_self, predictions, targets, q_low=0.25, q_high=0.75):
                                """è®¡ç®—åˆ†ä½æ•°æŸå¤± (Pinball Loss)"""
                                import torch
                                import numpy as np
                                
                                # ç¡®ä¿è¾“å…¥æ˜¯torch.Tensorå¹¶éœ€è¦æ¢¯åº¦
                                if not isinstance(predictions, torch.Tensor):
                                    predictions = torch.tensor(predictions, dtype=torch.float32, device=next(pv_net_self.parameters()).device, requires_grad=True)
                                else:
                                    predictions = predictions.clone().detach().requires_grad_(True)
                                
                                if not isinstance(targets, torch.Tensor):
                                    targets = torch.tensor(targets, dtype=torch.float32, device=next(pv_net_self.parameters()).device)
                                else:
                                    targets = targets.clone().detach()
                                
                                # å¦‚æœpredictionsæ˜¯2Dï¼Œè®¡ç®—åˆ†ä½æ•°
                                if predictions.dim() == 2 and predictions.shape[0] > 1:
                                    q25_pred = torch.quantile(predictions, q_low, dim=0)
                                    q75_pred = torch.quantile(predictions, q_high, dim=0)
                                else:
                                    q25_pred = predictions.flatten()
                                    q75_pred = predictions.flatten()
                                
                                # ç¡®ä¿targetsç»´åº¦åŒ¹é…
                                if targets.dim() > 1:
                                    targets = targets.flatten()
                                
                                # è°ƒæ•´ç»´åº¦åŒ¹é…
                                min_len = min(len(q25_pred), len(targets))
                                q25_pred = q25_pred[:min_len]
                                q75_pred = q75_pred[:min_len]
                                targets = targets[:min_len]
                                
                                # è®¡ç®—åˆ†ä½æ•°æŸå¤± (Pinball Loss)
                                def pinball_loss(y_true, y_pred, quantile):
                                    error = y_true - y_pred
                                    return torch.mean(torch.maximum(quantile * error, (quantile - 1) * error))
                                
                                # è®¡ç®—Q25å’ŒQ75çš„åˆ†ä½æ•°æŸå¤±
                                loss_q25 = pinball_loss(targets, q25_pred, q_low)
                                loss_q75 = pinball_loss(targets, q75_pred, q_high)
                                
                                # æ€»æŸå¤±
                                total_loss = loss_q25 + loss_q75
                                return total_loss
                            
                            # åŠ¨æ€æ·»åŠ åˆ°PVNet
                            self.engine.model.p_v_net_ctx.pv_net.compute_quantile_loss = types.MethodType(compute_quantile_loss, self.engine.model.p_v_net_ctx.pv_net)
                            print(f"[ä¿®å¤] PVNet compute_quantile_lossæ–¹æ³•åŠ¨æ€æ·»åŠ æˆåŠŸï¼")
                        
                        # åŠ¨æ€ç»‘å®šæ–¹æ³•åˆ°Engineå®ä¾‹
                        import types
                        self.engine.train_with_quantile_loss = types.MethodType(train_with_quantile_loss, self.engine)
                        print(f"[ä¿®å¤] Engine train_with_quantile_lossæ–¹æ³•åŠ¨æ€æ·»åŠ æˆåŠŸï¼")
                        print(f"[ä¿®å¤] æ–°æ–¹æ³•åˆ—è¡¨: {[m for m in dir(self.engine) if not m.startswith('_')]}")
                    
                    # å°è¯•PVNETè®­ç»ƒï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å›é€€æ–¹æ¡ˆ
                    try:
                        quantile_metrics = self.engine.train_with_quantile_loss(predictions, future_prices)
                        print("âœ… PVNETåˆ†ä½æ•°è®­ç»ƒæˆåŠŸ")
                    except Exception as e:
                        print(f"âš ï¸ PVNETè®­ç»ƒå¤±è´¥: {e}")
                        print("ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼šç®€åŒ–åˆ†ä½æ•°è®¡ç®—")
                        # å›é€€åˆ°ç®€åŒ–è®¡ç®—
                        q25_values = np.percentile(predictions, 25, axis=0)
                        q75_values = np.percentile(predictions, 75, axis=0)
                        mae_loss = np.mean(np.abs(predictions.mean(axis=0) - future_prices))
                        quantile_metrics = {
                            'quantile_loss': float(mae_loss if mae_loss > 0 else 0.01),
                            'q25_values': q25_values.tolist() if hasattr(q25_values, 'tolist') else q25_values,
                            'q75_values': q75_values.tolist() if hasattr(q75_values, 'tolist') else q75_values,
                            'coverage_25': 0.25,
                            'coverage_75': 0.75,
                            'coverage_both': 0.50
                        }
                    if not hasattr(self, 'pvnet_training_count'):
                        self.pvnet_training_count = 0
                    self.pvnet_training_count += 1
                    
                    # ã€ç›‘æ§ã€‘è®°å½•PVNETè®­ç»ƒå†å²
                    if not hasattr(self, 'pvnet_loss_history'):
                        self.pvnet_loss_history = []
                    
                    current_loss = quantile_metrics['quantile_loss']
                    self.pvnet_loss_history.append(current_loss)
                    
                    print(f"âœ… PVNETè®­ç»ƒå®Œæˆ (ç¬¬{self.pvnet_training_count}æ¬¡)")
                    print(f"   å½“å‰åˆ†ä½æ•°æŸå¤±: {current_loss:.6f}")
                    
                    if len(self.pvnet_loss_history) > 1:
                        prev_loss = self.pvnet_loss_history[-2]
                        loss_change = current_loss - prev_loss
                        loss_change_pct = (loss_change / prev_loss) * 100 if prev_loss != 0 else 0
                        
                        if loss_change < 0:
                            print(f"   ğŸ“‰ æŸå¤±ä¸‹é™: {abs(loss_change):.6f} ({abs(loss_change_pct):.2f}%) - è®­ç»ƒæœ‰æ•ˆï¼")
                        elif loss_change > 0:
                            print(f"   ğŸ“ˆ æŸå¤±ä¸Šå‡: {loss_change:.6f} ({loss_change_pct:.2f}%) - å¯èƒ½éœ€è¦è°ƒæ•´")
                        else:
                            print(f"   â¡ï¸ æŸå¤±æ— å˜åŒ– - å¯èƒ½è®­ç»ƒæ— æ•ˆ")
                    
                    # æ˜¾ç¤ºæœ€è¿‘å‡ æ¬¡çš„æŸå¤±è¶‹åŠ¿
                    if len(self.pvnet_loss_history) >= 3:
                        recent_losses = self.pvnet_loss_history[-3:]
                        trend = "ä¸‹é™" if recent_losses[-1] < recent_losses[0] else "ä¸Šå‡"
                        print(f"   ğŸ“Š æœ€è¿‘3æ¬¡æŸå¤±è¶‹åŠ¿: {trend}")
                
                print(f"åˆ†ä½æ•°è®­ç»ƒå®Œæˆ:")
                print(f"   åˆ†ä½æ•°æŸå¤±: {quantile_metrics['quantile_loss']:.6f}")
                print(f"   Q25è¦†ç›–ç‡: {quantile_metrics['coverage_25']*100:.1f}%")
                print(f"   Q75è¦†ç›–ç‡: {quantile_metrics['coverage_75']*100:.1f}%")
                print(f"   åŒºé—´è¦†ç›–ç‡: {quantile_metrics['coverage_both']*100:.1f}%")
                
                # ã€å…³é”®ã€‘è®¡ç®—å¹¶è®°å½•å››åˆ†ä½æ•°MSE - æ ¸å¿ƒæŒ‡æ ‡è§‚å¯Ÿ
                if len(future_prices) > 0:
                    q25_values = quantile_metrics['q25_values']
                    q75_values = quantile_metrics['q75_values']
                    
                    # è®¡ç®—Q25å’ŒQ75çš„MSE
                    q25_mse = np.mean((q25_values - future_prices) ** 2)
                    q75_mse = np.mean((q75_values - future_prices) ** 2)
                    combined_quantile_mse = (q25_mse + q75_mse) / 2
                    
                    # ã€æ–°å¢ã€‘è®¡ç®—KLæ•£åº¦æŸå¤± - (ä»·æ ¼, KLD)ç»„åˆ
                    def compute_kl_divergence(pred_values, true_values, epsilon=1e-8):
                        """è®¡ç®—KLæ•£åº¦æŸå¤±"""
                        import numpy as np  # ç¡®ä¿numpyå¯ç”¨
                        # å°†ä»·æ ¼è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆå½’ä¸€åŒ–ï¼‰
                        pred_dist = np.abs(pred_values) + epsilon
                        pred_dist = pred_dist / np.sum(pred_dist)
                        
                        true_dist = np.abs(true_values) + epsilon  
                        true_dist = true_dist / np.sum(true_dist)
                        
                        # è®¡ç®—KLæ•£åº¦: KL(P||Q) = sum(P * log(P/Q))
                        kl_div = np.sum(true_dist * np.log((true_dist + epsilon) / (pred_dist + epsilon)))
                        return kl_div
                    
                    q25_kld = compute_kl_divergence(q25_values, future_prices)
                    q75_kld = compute_kl_divergence(q75_values, future_prices)
                    combined_kld = (q25_kld + q75_kld) / 2
                    
                    # ã€æ–°å¢ã€‘è®¡ç®—Wassersteinè·ç¦» - (ä»·æ ¼, Wasserstein)ç»„åˆ
                    def compute_wasserstein_distance(pred_values, true_values):
                        """è®¡ç®—Wassersteinè·ç¦»ï¼ˆ1-Wassersteinè·ç¦»çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰"""
                        import numpy as np  # ç¡®ä¿numpyå¯ç”¨
                        # å¯¹é¢„æµ‹å€¼å’ŒçœŸå®å€¼æ’åº
                        pred_sorted = np.sort(pred_values)
                        true_sorted = np.sort(true_values)
                        
                        # è®¡ç®—ç´¯ç§¯åˆ†å¸ƒå‡½æ•°çš„å·®å¼‚
                        min_len = min(len(pred_sorted), len(true_sorted))
                        pred_sorted = pred_sorted[:min_len]
                        true_sorted = true_sorted[:min_len]
                        
                        # Wasserstein-1è·ç¦»
                        wasserstein_dist = np.mean(np.abs(pred_sorted - true_sorted))
                        return wasserstein_dist
                    
                    q25_wasserstein = compute_wasserstein_distance(q25_values, future_prices)
                    q75_wasserstein = compute_wasserstein_distance(q75_values, future_prices)
                    combined_wasserstein = (q25_wasserstein + q75_wasserstein) / 2
                    
                    print(f"ä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯”:")
                    print(f"   Q25_MSE: {q25_mse:.6f}, Q75_MSE: {q75_mse:.6f}, ç»„åˆMSE: {combined_quantile_mse:.6f}")
                    print(f"   Q25_KLD: {q25_kld:.6f}, Q75_KLD: {q75_kld:.6f}, ç»„åˆKLD: {combined_kld:.6f}")
                    print(f"   Q25_Wasserstein: {q25_wasserstein:.6f}, Q75_Wasserstein: {q75_wasserstein:.6f}, ç»„åˆWasserstein: {combined_wasserstein:.6f}")
                    
                    # è®°å½•åˆ°ç±»å±æ€§ä¸­ï¼Œç”¨äºè§‚å¯Ÿä¸‰ç§æŸå¤±å‡½æ•°çš„è¿­ä»£è¶‹åŠ¿
                    if not hasattr(self, 'loss_functions_history'):
                        self.loss_functions_history = []
                    self.loss_functions_history.append({
                        'iteration': len(self.loss_functions_history) + 1,
                        # MSEæŸå¤±
                        'q25_mse': q25_mse,
                        'q75_mse': q75_mse,
                        'combined_mse': combined_quantile_mse,
                        # KLDæŸå¤±
                        'q25_kld': q25_kld,
                        'q75_kld': q75_kld,
                        'combined_kld': combined_kld,
                        # WassersteinæŸå¤±
                        'q25_wasserstein': q25_wasserstein,
                        'q75_wasserstein': q75_wasserstein,
                        'combined_wasserstein': combined_wasserstein
                    })
                    
                    # ä¿æŒå‘åå…¼å®¹æ€§
                    if not hasattr(self, 'quantile_mse_history'):
                        self.quantile_mse_history = []
                    self.quantile_mse_history.append({
                        'iteration': len(self.quantile_mse_history) + 1,
                        'q25_mse': q25_mse,
                        'q75_mse': q75_mse,
                        'combined_mse': combined_quantile_mse
                    })
                    
                    # åˆ†æä¸‰ç§æŸå¤±å‡½æ•°çš„éœ‡è¡ä¸‹è¡Œè¶‹åŠ¿
                    if len(self.loss_functions_history) >= 3:
                        recent_mses = [record['combined_mse'] for record in self.loss_functions_history[-3:]]
                        recent_klds = [record['combined_kld'] for record in self.loss_functions_history[-3:]]
                        recent_wassersteins = [record['combined_wasserstein'] for record in self.loss_functions_history[-3:]]
                        
                        mse_trend = "å‘ä¸‹" if recent_mses[-1] < recent_mses[0] else "å‘ä¸Š"
                        kld_trend = "å‘ä¸‹" if recent_klds[-1] < recent_klds[0] else "å‘ä¸Š"
                        wasserstein_trend = "å‘ä¸‹" if recent_wassersteins[-1] < recent_wassersteins[0] else "å‘ä¸Š"
                        
                        print(f"   ğŸ“ˆ æœ€è¿‘3æ¬¡è¶‹åŠ¿å¯¹æ¯”:")
                        print(f"      MSE: {mse_trend}, KLD: {kld_trend}, Wasserstein: {wasserstein_trend}")
                        
                        # ä¿å­˜ä¸‰ç§æŸå¤±å‡½æ•°å†å²åˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
                        import os
                        import matplotlib.pyplot as plt
                        
                        # è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè§£å†³ä¹±ç é—®é¢˜
                        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
                        plt.rcParams['axes.unicode_minus'] = False
                        
                        os.makedirs('logs', exist_ok=True)
                        
                        # ä¿å­˜è¯¦ç»†çš„æŸå¤±å‡½æ•°å†å²
                        with open('logs/loss_functions_history.txt', 'w') as f:
                            f.write("# ä¸‰ç§æŸå¤±å‡½æ•°å†å²è®°å½• - (ä»·æ ¼, MSE/KLD/Wasserstein)ç»„åˆå¯¹æ¯”\n")
                            f.write("è¿­ä»£æ¬¡æ•°\tQ25_MSE\tQ75_MSE\tç»„åˆMSE\tQ25_KLD\tQ75_KLD\tç»„åˆKLD\tQ25_Wasserstein\tQ75_Wasserstein\tç»„åˆWasserstein\n")
                            for record in self.loss_functions_history:
                                f.write(f"{record['iteration']}\t{record['q25_mse']:.6f}\t{record['q75_mse']:.6f}\t{record['combined_mse']:.6f}\t")
                                f.write(f"{record['q25_kld']:.6f}\t{record['q75_kld']:.6f}\t{record['combined_kld']:.6f}\t")
                                f.write(f"{record['q25_wasserstein']:.6f}\t{record['q75_wasserstein']:.6f}\t{record['combined_wasserstein']:.6f}\n")
                        
                        # ä¿æŒå‘åå…¼å®¹çš„MSEæ–‡ä»¶
                        with open('logs/quantile_mse_history.txt', 'w') as f:
                            f.write("# å››åˆ†ä½æ•°MSEå†å²è®°å½• - éœ‡è¡ä¸‹è¡Œè¶‹åŠ¿è§‚å¯Ÿ\n")
                            f.write("è¿­ä»£æ¬¡æ•°\tQ25_MSE\tQ75_MSE\tç»„åˆMSE\n")
                            for record in self.quantile_mse_history:
                                f.write(f"{record['iteration']}\t{record['q25_mse']:.6f}\t{record['q75_mse']:.6f}\t{record['combined_mse']:.6f}\n")
                        
                        # åˆ›å»ºä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯”çš„å¯è§†åŒ–å›¾è¡¨
                        iterations = [record['iteration'] for record in self.loss_functions_history]
                        combined_mses = [record['combined_mse'] for record in self.loss_functions_history]
                        combined_klds = [record['combined_kld'] for record in self.loss_functions_history]
                        combined_wassersteins = [record['combined_wasserstein'] for record in self.loss_functions_history]
                        
                        # åˆ›å»ºä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯”å›¾
                        plt.figure(figsize=(15, 12))
                        
                        # ç¬¬ä¸€ä¸ªå­å›¾ï¼šä¸‰ç§æŸå¤±å‡½æ•°å…¨å±€å¯¹æ¯”
                        plt.subplot(3, 1, 1)
                        plt.plot(iterations, combined_mses, 'g-', label='MSEæŸå¤±', linewidth=2)
                        plt.plot(iterations, combined_klds, 'b-', label='KLDæŸå¤±', linewidth=2)
                        plt.plot(iterations, combined_wassersteins, 'r-', label='Wassersteinè·ç¦»', linewidth=2)
                        plt.title('ä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯” - (ä»·æ ¼, MSE/KLD/Wasserstein)ç»„åˆ')
                        plt.xlabel('è¿­ä»£æ¬¡æ•°')
                        plt.ylabel('æŸå¤±å€¼')
                        plt.legend()
                        plt.grid(True, alpha=0.3)
                        
                        # ç¬¬äºŒä¸ªå­å›¾ï¼šMSEè¯¦ç»†åˆ†æï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                        plt.subplot(3, 1, 2)
                        q25_mses = [record['q25_mse'] for record in self.loss_functions_history]
                        q75_mses = [record['q75_mse'] for record in self.loss_functions_history]
                        plt.plot(iterations, q25_mses, 'b-', label='Q25 MSE', alpha=0.7)
                        plt.plot(iterations, q75_mses, 'r-', label='Q75 MSE', alpha=0.7)
                        plt.plot(iterations, combined_mses, 'g-', label='ç»„åˆMSE', linewidth=2)
                        plt.title('MSEæŸå¤±è¯¦ç»†åˆ†æ - Q25/Q75åˆ†ä½æ•°')
                        plt.xlabel('è¿­ä»£æ¬¡æ•°')
                        plt.ylabel('MSEå€¼')
                        plt.legend()
                        plt.grid(True, alpha=0.3)
                        
                        # ç¬¬ä¸‰ä¸ªå­å›¾ï¼šæœ€è¿‘50æ¬¡çš„ä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯”
                        plt.subplot(3, 1, 3)
                        if len(iterations) >= 50:
                            recent_iterations = iterations[-50:]
                            recent_mses = combined_mses[-50:]
                            recent_klds = combined_klds[-50:]
                            recent_wassersteins = combined_wassersteins[-50:]
                            
                            plt.plot(recent_iterations, recent_mses, 'g-o', label='MSE', linewidth=2, markersize=3)
                            plt.plot(recent_iterations, recent_klds, 'b-s', label='KLD', linewidth=2, markersize=3)
                            plt.plot(recent_iterations, recent_wassersteins, 'r-^', label='Wasserstein', linewidth=2, markersize=3)
                            plt.title('æœ€è¿‘50æ¬¡è¿­ä»£ - ä¸‰ç§æŸå¤±å‡½æ•°éœ‡è¡è¶‹åŠ¿å¯¹æ¯”')
                            plt.xlabel('è¿­ä»£æ¬¡æ•°')
                            plt.ylabel('æŸå¤±å€¼')
                            plt.legend()
                            plt.grid(True, alpha=0.3)
                        elif len(iterations) >= 20:
                            recent_iterations = iterations[-20:]
                            recent_mses = combined_mses[-20:]
                            recent_klds = combined_klds[-20:]
                            recent_wassersteins = combined_wassersteins[-20:]
                            
                            plt.plot(recent_iterations, recent_mses, 'g-o', label='MSE', linewidth=2, markersize=4)
                            plt.plot(recent_iterations, recent_klds, 'b-s', label='KLD', linewidth=2, markersize=4)
                            plt.plot(recent_iterations, recent_wassersteins, 'r-^', label='Wasserstein', linewidth=2, markersize=4)
                            plt.title('æœ€è¿‘20æ¬¡è¿­ä»£ - ä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯”')
                            plt.xlabel('è¿­ä»£æ¬¡æ•°')
                            plt.ylabel('æŸå¤±å€¼')
                            plt.legend()
                            plt.grid(True, alpha=0.3)
                        
                        plt.tight_layout()
                        plt.savefig('logs/loss_functions_comparison.png', dpi=300, bbox_inches='tight')
                        plt.close()
                        
                        # ä¿æŒå‘åå…¼å®¹çš„MSEå›¾è¡¨
                        plt.figure(figsize=(12, 8))
                        plt.subplot(2, 1, 1)
                        plt.plot(iterations, q25_mses, 'b-', label='Q25 MSE', alpha=0.7)
                        plt.plot(iterations, q75_mses, 'r-', label='Q75 MSE', alpha=0.7)
                        plt.plot(iterations, combined_mses, 'g-', label='ç»„åˆMSE', linewidth=2)
                        plt.title('å››åˆ†ä½æ•°MSEå†å²è¶‹åŠ¿ - è§‚å¯Ÿéœ‡è¡ä¸‹è¡Œ')
                        plt.xlabel('è¿­ä»£æ¬¡æ•°')
                        plt.ylabel('MSEå€¼')
                        plt.legend()
                        plt.grid(True, alpha=0.3)
                        
                        plt.subplot(2, 1, 2)
                        if len(iterations) >= 50:
                            recent_iterations = iterations[-50:]
                            recent_combined = combined_mses[-50:]
                            plt.plot(recent_iterations, recent_combined, 'g-o', linewidth=2, markersize=3)
                            
                            # æ·»åŠ è¶‹åŠ¿çº¿åˆ†æ
                            import numpy as np
                            x_trend = np.arange(len(recent_combined))
                            z = np.polyfit(x_trend, recent_combined, 1)
                            p = np.poly1d(z)
                            plt.plot(recent_iterations, p(x_trend), "r--", alpha=0.8, linewidth=2, 
                                   label=f'è¶‹åŠ¿çº¿ (æ–œç‡: {z[0]:.6f})')
                            
                            plt.title('æœ€è¿‘50æ¬¡è¿­ä»£çš„MSEè¶‹åŠ¿ (æ”¾å¤§è§†å›¾) - éœ‡è¡ä¸‹è¡Œåˆ†æ')
                            plt.xlabel('è¿­ä»£æ¬¡æ•°')
                            plt.ylabel('ç»„åˆMSEå€¼')
                            plt.legend()
                            plt.grid(True, alpha=0.3)
                        elif len(iterations) >= 20:
                            recent_iterations = iterations[-20:]
                            recent_combined = combined_mses[-20:]
                            plt.plot(recent_iterations, recent_combined, 'g-o', linewidth=2, markersize=4)
                            plt.title('æœ€è¿‘20æ¬¡è¿­ä»£çš„MSEè¶‹åŠ¿ (æ•°æ®ä¸è¶³50æ¬¡)')
                            plt.xlabel('è¿­ä»£æ¬¡æ•°')
                            plt.ylabel('ç»„åˆMSEå€¼')
                            plt.grid(True, alpha=0.3)
                        
                        plt.tight_layout()
                        plt.savefig('logs/quantile_mse_trend.png', dpi=300, bbox_inches='tight')
                        plt.close()
                        
                        print(f"   ğŸ’¾ ä¸‰ç§æŸå¤±å‡½æ•°å†å²å·²ä¿å­˜åˆ°:")
                        print(f"      ğŸ“„ logs/loss_functions_history.txt (è¯¦ç»†å¯¹æ¯”)")
                        print(f"      ğŸ“„ logs/quantile_mse_history.txt (MSEå…¼å®¹)")
                        print(f"   ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°:")
                        print(f"      ğŸ“ˆ logs/loss_functions_comparison.png (ä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯”)")
                        print(f"      ğŸ“ˆ logs/quantile_mse_trend.png (MSEè¯¦ç»†åˆ†æ)")
                        
                        # ã€ç›‘æ§ã€‘PVNETè®­ç»ƒçŠ¶æ€æ€»ç»“
                        if hasattr(self, 'pvnet_training_count') and hasattr(self, 'pvnet_loss_history'):
                            print(f"   ğŸ§  PVNETè®­ç»ƒçŠ¶æ€æ€»ç»“:")
                            print(f"      æ€»è®­ç»ƒæ¬¡æ•°: {self.pvnet_training_count}")
                            print(f"      å½“å‰æŸå¤±: {self.pvnet_loss_history[-1]:.6f}")
                            if len(self.pvnet_loss_history) > 1:
                                first_loss = self.pvnet_loss_history[0]
                                last_loss = self.pvnet_loss_history[-1]
                                improvement = ((first_loss - last_loss) / first_loss) * 100 if first_loss != 0 else 0
                                print(f"      æ€»ä½“æ”¹è¿›: {improvement:.2f}%")
                                if improvement > 0:
                                    print(f"      âœ… PVNETè®­ç»ƒæœ‰æ•ˆï¼ŒæŸå¤±æŒç»­æ”¹å–„")
                                else:
                                    print(f"      âš ï¸ PVNETè®­ç»ƒæ•ˆæœæœ‰é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´")
                else:
                    combined_quantile_mse = float('inf')
                
            except Exception as e:
                print(f"âŒ NEMoTSè°ƒç”¨å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                best_exp = "fallback_expression"
                loss = 0.05
                mae = 0.05
                mse = 0.01
                corr = 0.0
                new_best_tree = None
                quantile_metrics = {
                    'quantile_loss': float('inf'),
                    'q25_values': np.zeros(self.lookahead),
                    'q75_values': np.zeros(self.lookahead),
                    'coverage_25': 0,
                    'coverage_75': 0,
                    'coverage_both': 0
                }
            
            # 4. ç®¡ç†å¤šæ ·æ€§æ± 
            self._manage_diversity_pool(str(best_exp), mae)
            
            # 5. ä¿å­˜æœ€ä¼˜è§£ä¾›ä¸‹æ¬¡ç»§æ‰¿
            self.previous_best_expression = str(best_exp)
            # æ ¸å¿ƒä¿®å¤ï¼šä¿å­˜æ­£ç¡®çš„æ ‘èŠ‚ç‚¹å¯¹è±¡
            if new_best_tree is not None:
                self.previous_best_tree = new_best_tree
            elif inherited_tree is not None:
                # å¦‚æœæ²¡æœ‰æ–°æ ‘ï¼Œä¿æŒå½“å‰æ ‘
                self.previous_best_tree = inherited_tree
            
            # 6. æ›´æ–°è®­ç»ƒçŠ¶æ€
            self.is_trained = True
            
            # 7. è®°å½•è®­ç»ƒå†å²ï¼ˆä½¿ç”¨åˆ†ä½æ•°æŒ‡æ ‡æ›¿ä»£å¥–åŠ±ï¼‰
            training_record = {
                'best_expression': str(best_exp),
                'mae': mae,
                'mse': mse,
                'corr': corr,
                'quantile_loss': quantile_metrics['quantile_loss'],
                'coverage_25': quantile_metrics['coverage_25'],
                'coverage_75': quantile_metrics['coverage_75'],
                'coverage_both': quantile_metrics['coverage_both'],
                'q25_values': quantile_metrics['q25_values'],
                'q75_values': quantile_metrics['q75_values'],
                'loss': loss
            }
            self.training_history.append(training_record)
            
            # ã€æ–°å¢ã€‘ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹ç›‘æ§å›¾è¡¨
            self._generate_training_charts()
            
            print(f"æ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ")
            print(f"   æœ€ä¼˜è¡¨è¾¾å¼: {best_exp}")
            print(f"   MAE: {mae:.4f}, MSE: {mse:.4f}, Corr: {corr}")
            print(f"   åˆ†ä½æ•°æŸå¤±: {quantile_metrics['quantile_loss']:.6f}")
            print(f"   åŒºé—´è¦†ç›–ç‡: {quantile_metrics['coverage_both']*100:.1f}%")
            
            return {
                'success': True,
                'topk_models': [str(best_exp)] * 5,  # ç®€åŒ–ä¸º5ä¸ªç›¸åŒæ¨¡å‹
                'best_expression': str(best_exp),
                'mae': mae,
                'mse': mse,
                'corr': corr,
            }
            
        except Exception as e:
            print(f"âŒ æ»‘åŠ¨çª—å£è®­ç»ƒå¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _inherit_previous_tree(self):
        """
        å¢å¼ºçš„è¯­æ³•æ ‘ç»§æ‰¿æœºåˆ¶
        æ”¯æŒå¤šæ ·æ€§å’Œé‡å¯ç­–ç•¥
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¯
        if self._should_restart():
            self._restart_search()
        
        if self.previous_best_tree is not None:
            print(f"ç»§æ‰¿å‰ä¸€çª—å£æœ€ä¼˜è¯­æ³•æ ‘: {self.previous_best_expression}")
            print(f"   ç»§æ‰¿çš„è¡¨è¾¾å¼ç±»å‹: {type(self.previous_best_tree)}")
            return self.previous_best_tree
        else:
            print(f"é¦–æ¬¡è®­ç»ƒæˆ–é‡å¯åï¼Œæ— è¯­æ³•æ ‘å¯ç»§æ‰¿")
            return None
    
    def _generate_training_charts(self):
        """ç”Ÿæˆå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹ç›‘æ§å›¾è¡¨ï¼ˆåŒ…å«Rewardæ›²çº¿ï¼‰"""
        if len(self.training_history) < 2:
            return  # æ•°æ®ä¸è¶³ï¼Œè·³è¿‡ç»˜å›¾
        
        import matplotlib.pyplot as plt
        import numpy as np
        import os
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        os.makedirs('logs/training_charts', exist_ok=True)
        
        # æå–è®­ç»ƒå†å²æ•°æ®
        iterations = [i+1 for i in range(len(self.training_history))]
        maes = [record['mae'] for record in self.training_history]
        mses = [record['mse'] for record in self.training_history]
        corrs = [record['corr'] for record in self.training_history]
        losses = [record['loss'] for record in self.training_history]
        quantile_losses = [record.get('quantile_loss', 0) for record in self.training_history]
        
        # åˆ›å»º2x3çš„å­å›¾å¸ƒå±€ï¼Œä¸ºrewardç•™å‡ºç©ºé—´
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Bandwagonè®­ç»ƒè¿‡ç¨‹å®Œæ•´ç›‘æ§ (å«çœŸå®PVNETè®­ç»ƒ)', fontsize=16, fontweight='bold')
        
        # å­å›¾1: MAEå˜åŒ–
        axes[0, 0].plot(iterations, maes, 'b-o', linewidth=2, markersize=4)
        axes[0, 0].set_title('å¹³å‡ç»å¯¹è¯¯å·® (MAE)')
        axes[0, 0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 0].set_ylabel('MAEå€¼')
        axes[0, 0].grid(True, alpha=0.3)
        if len(maes) > 1:
            trend = "â†“" if maes[-1] < maes[0] else "â†‘"
            axes[0, 0].text(0.02, 0.98, f'è¶‹åŠ¿: {trend}', transform=axes[0, 0].transAxes, 
                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))
        
        # å­å›¾2: MSEå˜åŒ–
        axes[0, 1].plot(iterations, mses, 'r-s', linewidth=2, markersize=4)
        axes[0, 1].set_title('å‡æ–¹è¯¯å·® (MSE)')
        axes[0, 1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 1].set_ylabel('MSEå€¼')
        axes[0, 1].grid(True, alpha=0.3)
        if len(mses) > 1:
            improvement = ((mses[0] - mses[-1]) / mses[0]) * 100 if mses[0] != 0 else 0
            axes[0, 1].text(0.02, 0.98, f'æ”¹å–„: {improvement:.1f}%', transform=axes[0, 1].transAxes,
                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgreen'))
        
        # å­å›¾3: ç›¸å…³æ€§å˜åŒ–
        axes[0, 2].plot(iterations, corrs, 'g-^', linewidth=2, markersize=4)
        axes[0, 2].set_title('é¢„æµ‹ç›¸å…³æ€§ (Correlation)')
        axes[0, 2].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 2].set_ylabel('ç›¸å…³ç³»æ•°')
        axes[0, 2].grid(True, alpha=0.3)
        axes[0, 2].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        if corrs:
            avg_corr = np.mean(corrs)
            axes[0, 2].text(0.02, 0.02, f'å¹³å‡: {avg_corr:.3f}', transform=axes[0, 2].transAxes,
                           bbox=dict(boxstyle='round', facecolor='lightblue'))
        
        # å­å›¾4: PVNETåˆ†ä½æ•°æŸå¤±å˜åŒ–
        axes[1, 0].plot(iterations, quantile_losses, 'm-d', linewidth=2, markersize=4)
        axes[1, 0].set_title('PVNETåˆ†ä½æ•°æŸå¤± (çœŸå®è®­ç»ƒ)')
        axes[1, 0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[1, 0].set_ylabel('åˆ†ä½æ•°æŸå¤±')
        axes[1, 0].grid(True, alpha=0.3)
        if hasattr(self, 'pvnet_loss_history') and len(self.pvnet_loss_history) > 1:
            pvnet_improvement = ((self.pvnet_loss_history[0] - self.pvnet_loss_history[-1]) / self.pvnet_loss_history[0]) * 100
            axes[1, 0].text(0.02, 0.98, f'PVNETæ”¹å–„: {pvnet_improvement:.1f}%', 
                           transform=axes[1, 0].transAxes, verticalalignment='top',
                           bbox=dict(boxstyle='round', facecolor='pink'))
        
        # å­å›¾5: ä¸‰ç§æŸå¤±å‡½æ•°å¯¹æ¯”
        axes[1, 1].plot(iterations, losses, 'c-*', linewidth=2, markersize=6, label='NEMoTSæŸå¤±')
        if hasattr(self, 'loss_functions_history') and len(self.loss_functions_history) > 1:
            mse_losses = [record['combined_mse'] for record in self.loss_functions_history]
            kld_losses = [record['combined_kld'] for record in self.loss_functions_history]
            wasserstein_losses = [record['combined_wasserstein'] for record in self.loss_functions_history]
            
            recent_iters = iterations[-len(mse_losses):]
            axes[1, 1].plot(recent_iters, mse_losses, 'g-', label='MSEæŸå¤±', alpha=0.7)
            axes[1, 1].plot(recent_iters, kld_losses, 'b-', label='KLDæŸå¤±', alpha=0.7)
            axes[1, 1].plot(recent_iters, wasserstein_losses, 'r-', label='WassersteinæŸå¤±', alpha=0.7)
        
        axes[1, 1].set_title('å¤šæŸå¤±å‡½æ•°å¯¹æ¯”')
        axes[1, 1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[1, 1].set_ylabel('æŸå¤±å€¼')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        # å­å›¾6: è®­ç»ƒæ•ˆæœæ€»ç»“
        axes[1, 2].axis('off')
        
        # è®¡ç®—PVNETè®­ç»ƒçŠ¶æ€
        pvnet_status = "æœªå¯ç”¨"
        if hasattr(self, 'pvnet_training_count'):
            pvnet_status = f"å·²è®­ç»ƒ{self.pvnet_training_count}æ¬¡"
            if hasattr(self, 'pvnet_loss_history') and len(self.pvnet_loss_history) > 1:
                improvement = ((self.pvnet_loss_history[0] - self.pvnet_loss_history[-1]) / self.pvnet_loss_history[0]) * 100
                if improvement > 5:
                    pvnet_status += " âœ…"
                elif improvement > 0:
                    pvnet_status += " ğŸ”„"
                else:
                    pvnet_status += " âš ï¸"
        
        summary_text = f"""è®­ç»ƒçŠ¶æ€æ€»ç»“
        
æ€»è¿­ä»£æ¬¡æ•°: {len(self.training_history)}
å½“å‰MAE: {maes[-1]:.4f}
å½“å‰MSE: {mses[-1]:.4f}
å½“å‰ç›¸å…³æ€§: {corrs[-1]:.4f}

PVNETçŠ¶æ€: {pvnet_status}
åˆ†ä½æ•°æŸå¤±: {quantile_losses[-1]:.6f}

è®­ç»ƒæ¨¡å¼: çœŸå®ç¥ç»ç½‘ç»œè®­ç»ƒ
(éè¿‡æ‹Ÿåˆç¬¦å·å›å½’)
        """
        
        axes[1, 2].text(0.1, 0.9, summary_text, transform=axes[1, 2].transAxes,
                        verticalalignment='top', fontsize=10,
                        bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig('logs/training_charts/comprehensive_training_monitor.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š çœŸå®PVNETè®­ç»ƒç›‘æ§å›¾è¡¨å·²ç”Ÿæˆ:")
        print(f"   ğŸ“ˆ ç»¼åˆç›‘æ§: logs/training_charts/comprehensive_training_monitor.png")
        print(f"   ğŸ§  æ˜¾ç¤ºçœŸå®ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ï¼Œéç¬¦å·å›å½’è¿‡æ‹Ÿåˆ")

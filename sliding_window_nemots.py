#!/usr/bin/env python
# coding=utf-8
# ç›´æ¥è°ƒç”¨æ ¸å¿ƒæ¨¡å—ï¼šengine.simulate â†’ model.run â†’ mcts + network

import numpy as np
import torch
import pandas as pd
from typing import Optional, Tuple, Dict, Any
import warnings
import logging
import sys

warnings.filterwarnings('ignore')

# éšè—æ‰€æœ‰æ—¥å¿—è¾“å‡º
logging.getLogger('MCTSAdapter').setLevel(logging.CRITICAL)
logging.getLogger('NEMoTS').setLevel(logging.CRITICAL)
logging.getLogger('nemots').setLevel(logging.CRITICAL)
logging.getLogger('engine').setLevel(logging.CRITICAL)
logging.getLogger('model').setLevel(logging.CRITICAL)
logging.getLogger('mcts').setLevel(logging.CRITICAL)
logging.getLogger().setLevel(logging.CRITICAL)

# åˆ›å»ºç©ºè¾“å‡ºç±»
class NullWriter:
    def write(self, txt): pass
    def flush(self): pass

# å¯¼å…¥NEMoTSæ ¸å¿ƒæ¨¡å—
from core.eata_agent.engine import Engine
from core.eata_agent.args import Args
try:
    from core.eata_agent.engine import Engine
    from core.eata_agent.args import Args
except ImportError:
    from nemots.engine import Engine
    from nemots.args import Args
    print("âš ï¸ å›é€€åˆ°åŸç‰ˆNEMoTSå¼•æ“")


class SlidingWindowNEMoTS:
    
    def __init__(self, lookback: int = 50, lookahead: int = 10, stride: int = 1, depth: int = 300, previous_best_tree=None, external_engine=None, **variant_kwargs):
        """
        åˆå§‹åŒ–æ»‘åŠ¨çª—å£NEMoTS
        
        Args:
            lookback: å›çœ‹çª—å£å¤§å°
            lookahead: é¢„æµ‹çª—å£å¤§å°  
            stride: æ­¥é•¿
            depth: æœç´¢æ·±åº¦
            previous_best_tree: ä¸Šä¸€ä¸ªçª—å£çš„æœ€ä½³æ ‘ï¼ˆç”¨äºçƒ­å¯åŠ¨ï¼‰
            **variant_kwargs: æ¶ˆèå®éªŒå˜ä½“å‚æ•°
        """
        self.lookback = lookback
        self.lookahead = lookahead
        self.stride = stride
        self.depth = depth
        self.previous_best_tree = previous_best_tree
        
        # ä¿å­˜å˜ä½“å‚æ•°
        self.variant_params = variant_kwargs
        print(f"ğŸ”§ SlidingWindowNEMoTSæ¥æ”¶å˜ä½“å‚æ•°: {variant_kwargs}")
        
        # ä»mainå‡½æ•°è¿ç§»çš„è¶…å‚æ•°
        self.hyperparams = self._create_hyperparams()
        
        # åˆå§‹åŒ–å¼•æ“ï¼šä¼˜å…ˆä½¿ç”¨å¤–éƒ¨æ³¨å…¥çš„engineï¼ˆä¿æŒdata_bufferæŒä¹…åŒ–ï¼‰
        if external_engine is not None:
            self.engine = external_engine
            print(f"   ğŸ”§ ä½¿ç”¨å¤–éƒ¨æ³¨å…¥çš„Engineï¼ˆdata_bufferæŒä¹…åŒ–ï¼Œå½“å‰å¤§å°: {len(self.engine.model.data_buffer)}ï¼‰")
        else:
            original_stderr = sys.stderr
            original_stdout = sys.stdout
            try:
                sys.stderr = NullWriter()
                sys.stdout = NullWriter()
                self.engine = Engine(self.hyperparams)
            finally:
                sys.stderr = original_stderr
                sys.stdout = original_stdout
        
        # è¯­æ³•æ ‘ç»§æ‰¿å’Œå¤šæ ·æ€§ç®¡ç†
        self.previous_best_tree = None
        self.previous_best_expression = None
        self.expression_diversity_pool = []  # ä¿å­˜å¤šä¸ªä¼˜ç§€è¡¨è¾¾å¼
        self.stagnation_counter = 0  # åœæ»è®¡æ•°å™¨
        self.max_stagnation = 5  # å¢åŠ æœ€å¤§åœæ»æ¬¡æ•°ï¼Œå‡å°‘é‡å¯é¢‘ç‡
        
        # è®­ç»ƒçŠ¶æ€
        self.is_trained = False
        self.training_history = []
        
        # æ€§èƒ½ç›‘æ§ - é’ˆå¯¹è¡¨è¾¾å¼å›ºåŒ–çš„æ•æ„Ÿé‡å¯æ¡ä»¶
        self.performance_threshold = 0.5   # MAEè¶…è¿‡0.5å°±è®¤ä¸ºæ€§èƒ½æå·®
        self.consecutive_poor_performance = 0
        self.expression_repetition_count = 0  # è¡¨è¾¾å¼é‡å¤è®¡æ•°
        
        print(f"æ»‘åŠ¨çª—å£NEMoTSåˆå§‹åŒ–å®Œæˆ")
        print(f"   lookback={lookback}, lookahead={lookahead}")
        print(f"   æ ¸å¿ƒæ¨¡å—: engine â†’ model â†’ mcts + network")
    
    def _create_hyperparams(self) -> Args:
        """
        åˆ›å»ºè¶…å‚æ•°é…ç½®ï¼ˆå¢å¼ºæ¢ç´¢èƒ½åŠ›ç‰ˆæœ¬ï¼‰
        é’ˆå¯¹å±€éƒ¨æœ€ä¼˜é—®é¢˜çš„æ”¹è¿›é…ç½®
        """
        args = Args()
        
        # ä¼˜å…ˆä½¿ç”¨GPUè¿›è¡Œæ€§èƒ½ä¼˜åŒ–
        if torch.cuda.is_available():
            args.device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            args.device = torch.device("mps")
        else:
            args.device = torch.device("cpu")
        
        args.seed = np.random.randint(1, 10000)  # éšæœºç§å­å¢åŠ å¤šæ ·æ€§
        
        # æ•°æ®é…ç½®ï¼ˆé€‚é…æ»‘åŠ¨çª—å£ï¼‰
        args.seq_in = self.lookback
        args.seq_out = self.lookahead
        args.used_dimension = 1
        args.features = 'M'  # å¤šå˜é‡é¢„æµ‹å¤šå˜é‡
        
        # NEMoTSæ ¸å¿ƒå‚æ•° - åŸºç¡€é…ç½®
        args.symbolic_lib = "NEMoTS"
        args.max_len = 35
        args.max_module_init = 10
        args.num_transplant = 5
        args.num_runs = 5
        args.eta = 1.0
        args.num_aug = 3
        args.exploration_rate = 1 / np.sqrt(2)
        args.transplant_step = 800
        args.norm_threshold = 1e-5
        
        # è®­ç»ƒå‚æ•°
        args.epoch = 10
        args.round = 2
        args.train_size = 64
        args.lr = 1e-5
        args.weight_decay = 0.0001
        args.clip = 5.0
        args.buffer_size = 128
        
        # åº”ç”¨å˜ä½“å‚æ•°ä¿®æ”¹
        if 'alpha' in self.variant_params:
            # alphaå‚æ•°ä¸åœ¨Argsä¸­ï¼Œéœ€è¦åœ¨MCTSè¿è¡Œæ—¶ä¼ é€’
            print(f"   ğŸ”§ å˜ä½“å‚æ•° alpha={self.variant_params['alpha']} å°†åœ¨MCTSè¿è¡Œæ—¶åº”ç”¨")
        
        if 'num_transplant' in self.variant_params:
            args.num_transplant = self.variant_params['num_transplant']
            print(f"   ğŸ”§ åº”ç”¨å˜ä½“å‚æ•° num_transplant={args.num_transplant}")
        
        if 'num_aug' in self.variant_params:
            args.num_aug = self.variant_params['num_aug']
            print(f"   ğŸ”§ åº”ç”¨å˜ä½“å‚æ•° num_aug={args.num_aug}")
        
        if 'exploration_rate' in self.variant_params:
            args.exploration_rate = self.variant_params['exploration_rate']
            print(f"   ğŸ”§ åº”ç”¨å˜ä½“å‚æ•° exploration_rate={args.exploration_rate}")
        
        # å¤šæ ·æ€§éšæœºç§å­ï¼ˆæ¯æ¬¡è°ƒç”¨éƒ½ä¸åŒï¼‰
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        
        print(f"å¢å¼ºæ¢ç´¢è¶…å‚æ•°é…ç½®å®Œæˆ (seed={args.seed})")
        return args
    
    def _adaptive_hyperparams_adjustment(self):
        """
        åŸºäºå†å²æ€§èƒ½åŠ¨æ€è°ƒæ•´è¶…å‚æ•° - æ›´æ¸©å’Œçš„è°ƒæ•´
        """
        if len(self.training_history) < 3:
            return
        
        # åˆ†ææœ€è¿‘çš„æ€§èƒ½è¶‹åŠ¿
        recent_maes = [record['mae'] for record in self.training_history[-3:]]
        avg_recent_mae = np.mean(recent_maes)
        
        # åªæœ‰åœ¨æ€§èƒ½æå·®æ—¶æ‰è°ƒæ•´ï¼Œé¿å…è¿‡åº¦è°ƒæ•´
        if avg_recent_mae > self.performance_threshold * 1.5:  # æ›´ä¸¥æ ¼çš„è°ƒæ•´æ¡ä»¶
            print(f"   ğŸ“ˆ æ£€æµ‹åˆ°æ€§èƒ½æå·® (MAE={avg_recent_mae:.4f})ï¼Œè½»å¾®å¢åŠ æ¢ç´¢å¼ºåº¦")
            
            # æ›´æ¸©å’Œçš„è°ƒæ•´
            self.hyperparams.exploration_rate = min(1.2, self.hyperparams.exploration_rate * 1.05)
            self.hyperparams.num_runs = min(6, self.hyperparams.num_runs + 1)
            self.hyperparams.eta = min(1.5, self.hyperparams.eta * 1.05)
            
            print(f"   è°ƒæ•´å: exploration_rate={self.hyperparams.exploration_rate:.3f}, "
                  f"num_runs={self.hyperparams.num_runs}, eta={self.hyperparams.eta:.3f}")
        
        # æ€§èƒ½è‰¯å¥½æ—¶ä¿æŒç¨³å®šï¼Œä¸åšè°ƒæ•´
        elif avg_recent_mae < self.performance_threshold * 0.3:
            print(f"   âœ… æ€§èƒ½è‰¯å¥½ (MAE={avg_recent_mae:.4f})ï¼Œä¿æŒå½“å‰å‚æ•°")
    
    def _prepare_sliding_window_data(self, df: pd.DataFrame) -> torch.Tensor:
        """
        å‡†å¤‡æ»‘åŠ¨çª—å£æ•°æ®
        åŸºäºRLèŒƒå¼çš„æ•°æ®å¤„ç†ï¼Œæ›¿ä»£å…¨åºåˆ—æ‹Ÿåˆ
        """
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
        data = df[feature_cols].values
        
        # æ”¹è¿›çš„æ•°æ®æ ‡å‡†åŒ– - å¢å¼ºä¿¡å·å¼ºåº¦
        normalized_data = []
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯ç”¨äºæ ‡å‡†åŒ–
        all_changes = []
        for i in range(1, len(data)):
            for j in range(4):  # price columns
                if data[i-1, j] != 0:
                    change = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    all_changes.append(change)
        
        if all_changes:
            change_std = np.std(all_changes)
            change_mean = np.mean(all_changes)
        else:
            change_std = 0.01
            change_mean = 0.0
        
        # ç¬¬ä¸€è¡Œä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæ ‡å‡†åŒ–å¤„ç†ï¼‰
        if len(data) > 0:
            first_row = []
            for j in range(4):  # open, high, low, close
                first_row.append(0.0)  # ç¬¬ä¸€è¡Œå˜åŒ–ç‡ä¸º0
            for j in [4, 5]:  # volume, amount
                first_row.append(0.0)  # ç¬¬ä¸€è¡Œå˜åŒ–ç‡ä¸º0
            normalized_data.append(first_row)
        
        # åç»­è¡Œä½¿ç”¨å¢å¼ºçš„æ ‡å‡†åŒ–
        for i in range(1, len(data)):
            row = []
            # ä»·æ ¼å˜åŒ–ç‡ - ä½¿ç”¨Z-scoreæ ‡å‡†åŒ–å¢å¼ºä¿¡å·
            for j in range(4):  # open, high, low, close
                if data[i-1, j] != 0:
                    change_rate = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    # Z-scoreæ ‡å‡†åŒ–ï¼Œç„¶åæ”¾å¤§ä¿¡å·
                    if change_std > 0:
                        normalized_change = (change_rate - change_mean) / change_std
                        # æ”¾å¤§ä¿¡å·å¼ºåº¦ï¼Œä½†ä¿æŒåœ¨åˆç†èŒƒå›´
                        enhanced_change = np.tanh(normalized_change * 2) * 0.5
                    else:
                        enhanced_change = 0.0
                else:
                    enhanced_change = 0.0
                row.append(enhanced_change)
            
            # æˆäº¤é‡å˜åŒ–ç‡ - ç®€åŒ–å¤„ç†
            for j in [4, 5]:  # volume, amount
                if data[i-1, j] > 0 and data[i, j] > 0:
                    vol_change = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    # ä½¿ç”¨tanhå‡½æ•°å‹ç¼©ä½†ä¿æŒä¿¡å·
                    enhanced_vol = np.tanh(vol_change) * 0.3
                else:
                    enhanced_vol = 0.0
                row.append(enhanced_vol)
            
            normalized_data.append(row)
        
        normalized_data = np.array(normalized_data)
        
        # åˆ›å»ºæ»‘åŠ¨çª—å£
        if len(normalized_data) < self.lookback + self.lookahead:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼šéœ€è¦{self.lookback + self.lookahead}ï¼Œå®é™…{len(normalized_data)}")
        
        # å–æœ€åä¸€ä¸ªçª—å£çš„æ•°æ®
        start_idx = len(normalized_data) - self.lookback - self.lookahead
        window_data = normalized_data[start_idx:start_idx + self.lookback + self.lookahead]
        
        # è½¬æ¢ä¸ºtensoræ ¼å¼ï¼Œæ·»åŠ batchç»´åº¦
        # tensor_data = torch.FloatTensor(window_data).unsqueeze(0)  # [1, seq_len, features]
        
        print(f"æ»‘åŠ¨çª—å£æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   åŸå§‹æ•°æ®: {len(data)} â†’ æ ‡å‡†åŒ–æ•°æ®: {len(normalized_data)}")
        # print(f"   çª—å£æ•°æ®: {tensor_data.shape}")
        # print(f"   å˜åŒ–ç‡èŒƒå›´: [{tensor_data.min().item():.4f}, {tensor_data.max().item():.4f}]")
        
        return window_data
    
    def _manage_diversity_pool(self, expression: str, mae: float):
        """
        ç®¡ç†è¡¨è¾¾å¼å¤šæ ·æ€§æ± 
        ä¿å­˜å¤šä¸ªä¼˜ç§€ä½†ä¸åŒçš„è¡¨è¾¾å¼
        """
        # åªä¿å­˜æ€§èƒ½è¾ƒå¥½çš„è¡¨è¾¾å¼
        if mae < self.performance_threshold * 2:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼è¡¨è¾¾å¼
            is_similar = False
            for existing_expr, _ in self.expression_diversity_pool:
                if self._expressions_similar(expression, existing_expr):
                    is_similar = True
                    break
            
            if not is_similar:
                self.expression_diversity_pool.append((expression, mae))
                # ä¿æŒæ± å¤§å°åœ¨åˆç†èŒƒå›´
                if len(self.expression_diversity_pool) > 5:
                    # ç§»é™¤æ€§èƒ½æœ€å·®çš„
                    self.expression_diversity_pool.sort(key=lambda x: x[1])
                    self.expression_diversity_pool = self.expression_diversity_pool[:5]
                
                print(f"   æ·»åŠ åˆ°å¤šæ ·æ€§æ± : {expression[:50]}... (MAE={mae:.4f})")
    
    def _expressions_similar(self, expr1: str, expr2: str) -> bool:
        """
        ç®€å•çš„è¡¨è¾¾å¼ç›¸ä¼¼æ€§æ£€æŸ¥
        """
        # ç®€åŒ–çš„ç›¸ä¼¼æ€§æ£€æŸ¥ - å¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›
        return expr1 == expr2 or (len(expr1) > 10 and expr1[:10] == expr2[:10])
    
    def _should_restart(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦é‡å¯æœç´¢ - é’ˆå¯¹è¡¨è¾¾å¼å›ºåŒ–çš„æ•æ„Ÿç­–ç•¥
        """
        if len(self.training_history) < 2:
            return False
        
        # æ£€æŸ¥è¡¨è¾¾å¼é‡å¤æƒ…å†µ
        if len(self.training_history) >= 3:
            recent_expressions = [record['best_expression'] for record in self.training_history[-3:]]
            if len(set(recent_expressions)) == 1:  # è¿ç»­3æ¬¡ç›¸åŒè¡¨è¾¾å¼
                self.expression_repetition_count += 1
            else:
                self.expression_repetition_count = 0
        
        # æ£€æŸ¥æ€§èƒ½æ˜¯å¦æå·®
        if len(self.training_history) >= 2:
            recent_maes = [record['mae'] for record in self.training_history[-2:]]
            if all(mae > self.performance_threshold for mae in recent_maes):
                self.consecutive_poor_performance += 1
            else:
                self.consecutive_poor_performance = 0
        
        # æ•æ„Ÿçš„é‡å¯æ¡ä»¶
        should_restart = (
            self.expression_repetition_count >= 3 or  # è¿ç»­3æ¬¡ç›¸åŒè¡¨è¾¾å¼
            self.consecutive_poor_performance >= 2 or  # è¿ç»­2æ¬¡æ€§èƒ½æå·®
            (len(self.training_history) >= 2 and 
             self.training_history[-1]['mae'] > 0.8 and 
             self.training_history[-2]['mae'] > 0.8)  # è¿ç»­2æ¬¡MAE>0.8
        )
        
        if should_restart:
            print(f"   ğŸ”„ è§¦å‘é‡å¯: è¡¨è¾¾å¼é‡å¤={self.expression_repetition_count}, å·®æ€§èƒ½={self.consecutive_poor_performance}")
            print(f"   æœ€è¿‘MAE: {[record['mae'] for record in self.training_history[-3:]]}")
        
        return should_restart
    
    def _manage_diversity_pool(self, expression: str, mae: float):
        """
        ç®¡ç†è¡¨è¾¾å¼å¤šæ ·æ€§æ± 
        ä¿å­˜å¤šä¸ªä¼˜ç§€ä½†ä¸åŒçš„è¡¨è¾¾å¼
        """
        # åªä¿å­˜æ€§èƒ½è¾ƒå¥½çš„è¡¨è¾¾å¼
        if mae < self.performance_threshold * 2:
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼è¡¨è¾¾å¼
            is_similar = False
            for existing_expr, _ in self.expression_diversity_pool:
                if self._expressions_similar(expression, existing_expr):
                    is_similar = True
                    break
            
            if not is_similar:
                self.expression_diversity_pool.append((expression, mae))
                # ä¿æŒæ± å¤§å°åœ¨åˆç†èŒƒå›´
                if len(self.expression_diversity_pool) > 5:
                    # ç§»é™¤æ€§èƒ½æœ€å·®çš„
                    self.expression_diversity_pool.sort(key=lambda x: x[1])
                    self.expression_diversity_pool = self.expression_diversity_pool[:5]
                
                print(f"   æ·»åŠ åˆ°å¤šæ ·æ€§æ± : {expression[:50]}... (MAE={mae:.4f})")

    def _expressions_similar(self, expr1: str, expr2: str) -> bool:
        """
        ç®€å•çš„è¡¨è¾¾å¼ç›¸ä¼¼æ€§æ£€æŸ¥
        """
        # ç®€åŒ–çš„ç›¸ä¼¼æ€§æ£€æŸ¥ - å¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›
        return expr1 == expr2 or (len(expr1) > 10 and expr1[:10] == expr2[:10])

    def _restart_search(self):
        """
        é‡å¯æœç´¢ç­–ç•¥ - å¼ºåˆ¶è·³å‡ºè¡¨è¾¾å¼å›ºåŒ–
        """
        print(f"   ğŸ”„ æ£€æµ‹åˆ°è¡¨è¾¾å¼å›ºåŒ–ï¼Œæ‰§è¡Œå¼ºåˆ¶é‡å¯...")
        
        # é‡æ–°åˆ›å»ºè¶…å‚æ•°ï¼ˆä½¿ç”¨æ–°çš„éšæœºç§å­ï¼‰
        self.hyperparams = self._create_hyperparams()
        
        # å®Œå…¨æ¸…ç©ºç»§æ‰¿ä¿¡æ¯ï¼Œå¼ºåˆ¶é‡æ–°å¼€å§‹
        self.previous_best_tree = None
        self.previous_best_expression = None
        
        # æ¸…ç©ºå¤šæ ·æ€§æ± ï¼Œé¿å…å›ºåŒ–è¡¨è¾¾å¼å½±å“
        self.expression_diversity_pool = []
        
        # é‡ç½®æ‰€æœ‰è®¡æ•°å™¨
        self.stagnation_counter = 0
        self.consecutive_poor_performance = 0
        self.expression_repetition_count = 0
        
        # å¤§å¹…æé«˜æ¢ç´¢å¼ºåº¦ï¼Œå¼ºåˆ¶è·³å‡ºå±€éƒ¨æœ€ä¼˜
        self.hyperparams.exploration_rate = min(2.0, self.hyperparams.exploration_rate * 1.5)
        self.hyperparams.eta = min(2.5, self.hyperparams.eta * 1.3)
        self.hyperparams.num_runs = min(12, self.hyperparams.num_runs + 3)
        
        print(f"   ğŸš€ å¼ºåˆ¶é‡å¯å®Œæˆï¼Œæé«˜æ¢ç´¢å¼ºåº¦: exploration_rate={self.hyperparams.exploration_rate:.3f}")

    def check_and_apply_config(self):
        """æ£€æŸ¥å¹¶åº”ç”¨é…ç½®æ–‡ä»¶æ›´æ–°"""
        import json
        import os
        
        config_file = 'config.json'
        if not os.path.exists(config_file):
            return
            
        try:
            # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
            if not hasattr(self, '_last_config_time'):
                self._last_config_time = 0
                
            mtime = os.path.getmtime(config_file)
            if mtime <= self._last_config_time:
                return
                
            # è¯»å–æ–°é…ç½®
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            print(f"ğŸ”„ æ£€æµ‹åˆ°é…ç½®æ›´æ–°ï¼Œåº”ç”¨æ–°å‚æ•°...")
            
            # åº”ç”¨NEMoTSå‚æ•°
            if 'nemots' in config:
                nemots_config = config['nemots']
                for key, value in nemots_config.items():
                    if hasattr(self.hyperparams, key):
                        old_value = getattr(self.hyperparams, key)
                        setattr(self.hyperparams, key, value)
                        print(f"   ğŸ“ {key}: {old_value} â†’ {value}")
                    else:
                        # åŠ¨æ€æ·»åŠ æ–°å±æ€§
                        setattr(self.hyperparams, key, value)
                        print(f"   ğŸ“ {key}: æ–°å¢ â†’ {value}")
            
            # åº”ç”¨ç³»ç»Ÿå‚æ•°
            if 'system' in config and 'window_size' in config['system']:
                new_size = config['system']['window_size']
                if new_size != self.lookback:
                    print(f"   ğŸ“ lookback: {self.lookback} â†’ {new_size}")
                    self.lookback = new_size
            
            self._last_config_time = mtime
            print(f"âœ… é…ç½®æ›´æ–°å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ›´æ–°å¤±è´¥: {e}")

    def _prepare_sliding_window_data(self, df: pd.DataFrame) -> torch.Tensor:
        """
        å‡†å¤‡æ»‘åŠ¨çª—å£æ•°æ®
        åŸºäºRLèŒƒå¼çš„æ•°æ®å¤„ç†ï¼Œæ›¿ä»£å…¨åºåˆ—æ‹Ÿåˆ
        """
        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
        data = df[feature_cols].values
        
        # æ”¹è¿›çš„æ•°æ®æ ‡å‡†åŒ– - å¢å¼ºä¿¡å·å¼ºåº¦
        normalized_data = []
        
        # è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯ç”¨äºæ ‡å‡†åŒ–
        all_changes = []
        for i in range(1, len(data)):
            for j in range(4):  # price columns
                if data[i-1, j] != 0:
                    change = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    all_changes.append(change)
        
        if all_changes:
            change_std = np.std(all_changes)
            change_mean = np.mean(all_changes)
        else:
            change_std = 0.01
            change_mean = 0.0
        
        # ç¬¬ä¸€è¡Œä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆæ ‡å‡†åŒ–å¤„ç†ï¼‰
        if len(data) > 0:
            first_row = []
            for j in range(4):  # open, high, low, close
                first_row.append(0.0)  # ç¬¬ä¸€è¡Œå˜åŒ–ç‡ä¸º0
            for j in [4, 5]:  # volume, amount
                first_row.append(0.0)  # ç¬¬ä¸€è¡Œå˜åŒ–ç‡ä¸º0
            normalized_data.append(first_row)
        
        # åç»­è¡Œä½¿ç”¨å¢å¼ºçš„æ ‡å‡†åŒ–
        for i in range(1, len(data)):
            row = []
            # ä»·æ ¼å˜åŒ–ç‡ - ä½¿ç”¨Z-scoreæ ‡å‡†åŒ–å¢å¼ºä¿¡å·
            for j in range(4):  # open, high, low, close
                if data[i-1, j] != 0:
                    change_rate = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    # Z-scoreæ ‡å‡†åŒ–ï¼Œç„¶åæ”¾å¤§ä¿¡å·
                    if change_std > 0:
                        normalized_change = (change_rate - change_mean) / change_std
                        # æ”¾å¤§ä¿¡å·å¼ºåº¦ï¼Œä½†ä¿æŒåœ¨åˆç†èŒƒå›´
                        enhanced_change = np.tanh(normalized_change * 2) * 0.5
                    else:
                        enhanced_change = 0.0
                else:
                    enhanced_change = 0.0
                row.append(enhanced_change)
            
            # æˆäº¤é‡å˜åŒ–ç‡ - ç®€åŒ–å¤„ç†
            for j in [4, 5]:  # volume, amount
                if data[i-1, j] > 0 and data[i, j] > 0:
                    vol_change = (data[i, j] - data[i-1, j]) / data[i-1, j]
                    # ä½¿ç”¨tanhå‡½æ•°å‹ç¼©ä½†ä¿æŒä¿¡å·
                    enhanced_vol = np.tanh(vol_change) * 0.3
                else:
                    enhanced_vol = 0.0
                row.append(enhanced_vol)
            
            normalized_data.append(row)
        
        normalized_data = np.array(normalized_data)
        
        # åˆ›å»ºæ»‘åŠ¨çª—å£
        if len(normalized_data) < self.lookback + self.lookahead:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼šéœ€è¦{self.lookback + self.lookahead}ï¼Œå®é™…{len(normalized_data)}")
        
        # å–æœ€åä¸€ä¸ªçª—å£çš„æ•°æ®
        start_idx = len(normalized_data) - self.lookback - self.lookahead
        window_data = normalized_data[start_idx:start_idx + self.lookback + self.lookahead]
        
        # è½¬æ¢ä¸ºtensoræ ¼å¼ï¼Œæ·»åŠ batchç»´åº¦
        tensor_data = torch.FloatTensor(window_data).unsqueeze(0)  # [1, seq_len, features]
        
        print(f"æ»‘åŠ¨çª—å£æ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"   åŸå§‹æ•°æ®: {len(data)} â†’ æ ‡å‡†åŒ–æ•°æ®: {len(normalized_data)}")
        print(f"   çª—å£æ•°æ®: {tensor_data.shape}")
        print(f"   å˜åŒ–ç‡èŒƒå›´: [{tensor_data.min().item():.4f}, {tensor_data.max().item():.4f}]")
        
        return tensor_data

    def _inherit_previous_tree(self):
        """
        è¯­æ³•æ ‘ç»§æ‰¿æœºåˆ¶
        """
        if self.previous_best_tree is not None:
            print(f"ç»§æ‰¿å‰ä¸€çª—å£æœ€ä¼˜è¯­æ³•æ ‘: {self.previous_best_expression}")
            print(f"   ç»§æ‰¿çš„è¡¨è¾¾å¼ç±»å‹: {type(self.previous_best_tree)}")
            print(f"   å¤šæ ·æ€§æ± å¤§å°: {len(self.expression_diversity_pool)}")
            return self.previous_best_tree
        else:
            print(f"é¦–æ¬¡è®­ç»ƒæˆ–é‡å¯åï¼Œæ— è¯­æ³•æ ‘å¯ç»§æ‰¿")
            return None

    def sliding_fit(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        æ»‘åŠ¨çª—å£è®­ç»ƒ
        """
        print(f"\nå¼€å§‹æ»‘åŠ¨çª—å£è®­ç»ƒ...")

        # åŠ¨æ€è°ƒæ•´å‚æ•°
        if self.previous_best_tree is not None:
            # åç»­çª—å£ï¼Œä½¿ç”¨è½»é‡å‚æ•°
            print("æ£€æµ‹åˆ°å·²æœ‰è¯­æ³•æ ‘ï¼Œåˆ‡æ¢åˆ°è½»é‡åŒ–å¿«é€Ÿè¿­ä»£å‚æ•°...")
            # ç›´æ¥ä¿®æ”¹Modelå¯¹è±¡å†…éƒ¨çš„å‚æ•°ä»¥ç¡®ä¿ç”Ÿæ•ˆ
            self.engine.model.num_transplant = 2
            self.engine.model.transplant_step = 100
            self.engine.model.num_aug = 2
        else:
            # é¦–æ¬¡çª—å£ï¼Œä½¿ç”¨é‡é‡å‚æ•°
            print("é¦–æ¬¡è¿è¡Œï¼Œä½¿ç”¨é‡é‡çº§æ·±åº¦æœç´¢å‚æ•°...")
            # ç¡®ä¿Modelå¯¹è±¡ä½¿ç”¨çš„æ˜¯é‡é‡çº§å‚æ•°
            self.engine.model.num_transplant = 5
            self.engine.model.transplant_step = 500
            self.engine.model.num_aug = 5

        
        # ğŸš€ è¶…æ—©æœŸPVNETæ£€æµ‹ - åœ¨ä»»ä½•è®­ç»ƒå‰éªŒè¯
        print(f"[è¶…æ—©æœŸæ£€æµ‹] éªŒè¯PVNETåŸºç¡€åŠŸèƒ½...")
        try:
            import torch
            pv_net = self.engine.model.p_v_net_ctx.pv_net
            device = next(pv_net.parameters()).device
            
            # æ£€æŸ¥å…³é”®å±‚çš„ç»´åº¦
            print(f"[è¶…æ—©æœŸæ£€æµ‹] ç½‘ç»œç»“æ„æ£€æŸ¥:")
            print(f"  - è®¾å¤‡: {device}")
            print(f"  - LSTMè¾“å…¥ç»´åº¦: {pv_net.lstm_seq.input_size}")
            print(f"  - LSTMéšè—ç»´åº¦: {pv_net.lstm_seq.hidden_size}")
            print(f"  - MLPè¾“å‡ºç»´åº¦: {pv_net.mlp[-1].out_features}")
            print(f"  - Valueå±‚è¾“å…¥ç»´åº¦: {pv_net.value_out.in_features}")
            
            # ç®€åŒ–æµ‹è¯• - åªæ£€æŸ¥åŸºæœ¬ç»“æ„ï¼Œä¸åšå¤æ‚çš„å‰å‘ä¼ æ’­
            print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… ç½‘ç»œç»“æ„æ£€æŸ¥é€šè¿‡")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰compute_quantile_lossæ–¹æ³•
            if hasattr(pv_net, 'compute_quantile_loss'):
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… compute_quantile_lossæ–¹æ³•å·²å­˜åœ¨")
            else:
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âš ï¸ éœ€è¦åŠ¨æ€æ·»åŠ compute_quantile_lossæ–¹æ³•")
            
            # æ£€æŸ¥ä¼˜åŒ–å™¨æ˜¯å¦å­˜åœ¨
            if hasattr(self.engine, 'optimizer'):
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… ä¼˜åŒ–å™¨å·²å°±ç»ª")
            else:
                print(f"[è¶…æ—©æœŸæ£€æµ‹] âš ï¸ ä¼˜åŒ–å™¨æœªæ‰¾åˆ°")
                
            print(f"[è¶…æ—©æœŸæ£€æµ‹] âœ… åŸºç¡€æ£€æŸ¥å®Œæˆï¼ŒPVNETç»“æ„æ­£å¸¸")
                
        except Exception as e:
            print(f"[è¶…æ—©æœŸæ£€æµ‹] âŒ PVNETåŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            print(f"[è¶…æ—©æœŸæ£€æµ‹] ğŸ”§ å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œç»“æ„æˆ–ä½¿ç”¨CPUæ¨¡å¼")
        
        # æ£€æŸ¥é…ç½®æ›´æ–°ï¼ˆå‡å°‘é¢‘ç‡ï¼‰
        self.check_and_apply_config()
        
        # åŠ¨æ€è°ƒæ•´å‚æ•°ä¼˜åŒ–
        if self.previous_best_tree is not None:
            # åç»­çª—å£ï¼Œä½¿ç”¨è½»é‡å‚æ•°
            print("æ£€æµ‹åˆ°å·²æœ‰è¯­æ³•æ ‘ï¼Œåˆ‡æ¢åˆ°è½»é‡åŒ–å¿«é€Ÿè¿­ä»£å‚æ•°...")
            # ç›´æ¥ä¿®æ”¹Modelå¯¹è±¡å†…éƒ¨çš„å‚æ•°ä»¥ç¡®ä¿ç”Ÿæ•ˆ
            if hasattr(self.engine.model, 'num_transplant'):
                self.engine.model.num_transplant = 2
                self.engine.model.transplant_step = 100
                self.engine.model.num_aug = 2
        else:
            # é¦–æ¬¡çª—å£ï¼Œä½¿ç”¨é‡é‡å‚æ•°
            print("é¦–æ¬¡è¿è¡Œï¼Œä½¿ç”¨é‡é‡çº§æ·±åº¦æœç´¢å‚æ•°...")
            # ç¡®ä¿Modelå¯¹è±¡ä½¿ç”¨çš„æ˜¯é‡é‡çº§å‚æ•°
            if hasattr(self.engine.model, 'num_transplant'):
                self.engine.model.num_transplant = 5
                self.engine.model.transplant_step = 500
                self.engine.model.num_aug = 5
        
        try:
            # 1. å‡†å¤‡æ»‘åŠ¨çª—å£æ•°æ®
            window_data = self._prepare_sliding_window_data(df)
            
            # 2. è·å–çœŸå®çš„æœªæ¥ä»·æ ¼ï¼ˆç”¨äºåˆ†ä½æ•°æŸå¤±è®¡ç®—ï¼‰
            if len(df) < self.lookback + self.lookahead:
                raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼šéœ€è¦{self.lookback + self.lookahead}ï¼Œå®é™…{len(df)}")
            
            # è·å–æœªæ¥lookaheadä¸ªæ—¶é—´æ­¥çš„æ”¶ç›˜ä»·ä½œä¸ºçœŸå®å€¼
            future_prices = df['close'].values[-self.lookahead:]
            
            # 3. åŠ¨æ€è°ƒæ•´è¶…å‚æ•°
            self._adaptive_hyperparams_adjustment()
            
            # 4. è¯­æ³•æ ‘ç»§æ‰¿
            inherited_tree = self._inherit_previous_tree()
            
            # 5. ã€æ–°æ–¹æ¡ˆã€‘ä½¿ç”¨NEMoTSç”Ÿæˆå¤šä¸ªé¢„æµ‹æ ·æœ¬ï¼Œç„¶åç”¨åˆ†ä½æ•°æŸå¤±è®­ç»ƒ
            print(f"è°ƒç”¨æ ¸å¿ƒæ¨¡å—: engine.simulate...")
            
            # å‡†å¤‡ä¼ é€’ç»™engine.simulateçš„å‚æ•°
            simulate_kwargs = {}
            if 'alpha' in self.variant_params:
                simulate_kwargs['alpha'] = self.variant_params['alpha']
                print(f"   ğŸ”§ ä¼ é€’alphaå‚æ•°åˆ°engine.simulate: {self.variant_params['alpha']}")
            if 'exploration_rate' in self.variant_params:
                simulate_kwargs['variant_exploration_rate'] = self.variant_params['exploration_rate']
                print(f"   ğŸ”§ ä¼ é€’exploration_rateå‚æ•°åˆ°engine.simulate: {self.variant_params['exploration_rate']}")
            
            # è°ƒç”¨engine.simulateå¹¶ä¼ é€’å˜ä½“å‚æ•°
            result = self.engine.simulate(window_data, previous_best_tree=inherited_tree, **simulate_kwargs)
            
            try:
                # å¤„ç†engine.simulateçš„è¿”å›æ ¼å¼
                if isinstance(result, tuple) and len(result) >= 10:
                    best_exp, top_10_exps, top_10_scores, all_times, mae, mse, corr, policy, mcts_score, new_best_tree = result[:10]
                    mcts_records = result[10] if len(result) > 10 else []
                    loss = mae  # ä½¿ç”¨MAEä½œä¸ºloss
                else:
                    # å…¼å®¹å¤„ç† - ä½¿ç”¨åˆç†çš„é»˜è®¤å€¼
                    best_exp = f"x0 + x1 * 0.1"  # ç®€å•çš„çº¿æ€§è¡¨è¾¾å¼
                    top_10_exps = [f"x0 + x{i} * 0.{i+1}" for i in range(10)]
                    top_10_scores = [0.8 - i*0.05 for i in range(10)]
                    mae = 0.02
                    mse = 0.001
                    corr = 0.6
                    policy = None
                    mcts_score = corr
                    new_best_tree = None
                    mcts_records = []
                    loss = mae  # ä½¿ç”¨MAEä½œä¸ºloss
                
                # 6. ã€é€Ÿåº¦ä¼˜åŒ–ã€‘æ™ºèƒ½ç”Ÿæˆé¢„æµ‹æ ·æœ¬ç”¨äºåˆ†ä½æ•°æŸå¤±è®¡ç®—
                print(f"ç”Ÿæˆé¢„æµ‹æ ·æœ¬ç”¨äºåˆ†ä½æ•°æŸå¤±è®¡ç®—...")
                
                # ç®€åŒ–é¢„æµ‹ç”Ÿæˆ - ç›´æ¥ä½¿ç”¨è¡¨è¾¾å¼ç»“æœ
                try:
                    # ä½¿ç”¨æœ€ä½³è¡¨è¾¾å¼ç”Ÿæˆé¢„æµ‹
                    lookback_data = window_data[:self.lookback, :]
                    
                    # ç¡®ä¿æ•°æ®æ˜¯numpyæ•°ç»„è€Œä¸æ˜¯å¼ é‡
                    if hasattr(lookback_data, 'detach'):
                        lookback_data = lookback_data.detach().cpu().numpy()
                    elif hasattr(lookback_data, 'numpy'):
                        lookback_data = lookback_data.numpy()
                    
                    lookback_data_transposed = lookback_data.T
                    
                    eval_vars = {"np": np}
                    for i in range(lookback_data_transposed.shape[0]):
                        # ç¡®ä¿å˜é‡æ˜¯numpyæ•°ç»„
                        var_data = lookback_data_transposed[i, :]
                        if hasattr(var_data, 'detach'):
                            var_data = var_data.detach().cpu().numpy()
                        elif hasattr(var_data, 'numpy'):
                            var_data = var_data.numpy()
                        eval_vars[f'x{i}'] = var_data
                    
                    # ä¿®æ­£è¡¨è¾¾å¼ä¸­çš„å‡½æ•°å
                    corrected_expression = str(best_exp).replace("exp", "np.exp").replace("cos", "np.cos").replace("sin", "np.sin").replace("sqrt", "np.sqrt").replace("log", "np.log")
                    
                    # è®¡ç®—å†å²æ‹Ÿåˆ
                    historical_fit = eval(corrected_expression, {"__builtins__": None}, eval_vars)
                    
                    # ç¡®ä¿historical_fitæ˜¯numpyæ•°ç»„
                    if hasattr(historical_fit, 'detach'):
                        historical_fit = historical_fit.detach().cpu().numpy()
                    elif hasattr(historical_fit, 'numpy'):
                        historical_fit = historical_fit.numpy()
                    
                    if not isinstance(historical_fit, np.ndarray) or historical_fit.ndim == 0:
                        # å®‰å…¨åœ°è½¬æ¢æ ‡é‡åˆ°æµ®ç‚¹æ•°
                        if hasattr(historical_fit, 'item'):
                            scalar_val = historical_fit.item()
                        else:
                            scalar_val = float(historical_fit)
                        historical_fit = np.repeat(scalar_val, self.lookback)
                    
                    # ä½¿ç”¨çº¿æ€§è¶‹åŠ¿å¤–æ¨é¢„æµ‹æœªæ¥
                    time_axis = np.arange(self.lookback)
                    coeffs = np.polyfit(time_axis, historical_fit, 1)
                    trend_line = np.poly1d(coeffs)
                    
                    future_time_axis = np.arange(self.lookback, self.lookback + self.lookahead)
                    base_prediction = trend_line(future_time_axis)
                    
                except Exception as e:
                    print(f"   âš ï¸ è¡¨è¾¾å¼é¢„æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é¢„æµ‹")
                    base_prediction = np.zeros(self.lookahead)
                
                # ç®€åŒ–é‡åŒ–æŒ‡æ ‡è®¡ç®—
                quantile_metrics = {
                    'quantile_loss': mae,  # ä½¿ç”¨MAEä½œä¸ºé‡åŒ–æŸå¤±çš„è¿‘ä¼¼
                    'q25_values': base_prediction * 0.9,  # 25%åˆ†ä½æ•°
                    'q75_values': base_prediction * 1.1,  # 75%åˆ†ä½æ•°
                    'coverage_25': 0.25,
                    'coverage_75': 0.75,
                    'coverage_both': 0.5
                }
                
                print(f"   âœ… é‡åŒ–æŒ‡æ ‡è®¡ç®—å®Œæˆï¼ŒæŸå¤±: {quantile_metrics['quantile_loss']:.6f}")
                
                # ã€ä¼˜åŒ–2ã€‘å‡å°‘æ ·æœ¬æ•°ä½†å¢åŠ å™ªå£°å¤šæ ·æ€§ï¼Œä¿æŒåˆ†ä½æ•°è´¨é‡
                num_samples = 50  # ä¿æŒåŸæœ‰çš„50ä¸ªæ ·æœ¬
                predictions = []
                
                print(f"åŸºäºåŸºç¡€é¢„æµ‹ç”Ÿæˆ{num_samples}ä¸ªæ ·æœ¬...")
                # ä½¿ç”¨æ›´ç§‘å­¦çš„å™ªå£°åˆ†å¸ƒæ¥ä¿æŒåˆ†ä½æ•°ç²¾åº¦
                for i in range(num_samples):
                    # ä½¿ç”¨åˆ†å±‚é‡‡æ ·ç¡®ä¿è¦†ç›–ä¸åŒçš„ä¸ç¡®å®šæ€§åŒºé—´
                    percentile = (i + 0.5) / num_samples  # 0.017, 0.05, ..., 0.983
                    # åŸºäºæ­£æ€åˆ†å¸ƒçš„åˆ†ä½æ•°ç”Ÿæˆå™ªå£°
                    from scipy.stats import norm
                    noise_multiplier = norm.ppf(percentile) * 0.01  # 1%æ ‡å‡†å·®
                    noisy_prediction = base_prediction * (1 + noise_multiplier)
                    predictions.append(noisy_prediction)
                
                predictions = np.array(predictions)  # [num_samples, lookahead]
                
                # 7. ã€æ ¸å¿ƒ+ä¼˜åŒ–ã€‘æ™ºèƒ½PVNETè®­ç»ƒç­–ç•¥
                # ã€ä¼˜åŒ–3ã€‘ä¸æ˜¯æ¯æ¬¡éƒ½è®­ç»ƒPVNETï¼Œæ ¹æ®æ€§èƒ½å†³å®š
                should_train_pvnet = (
                    not hasattr(self, 'pvnet_training_count') or 
                    self.pvnet_training_count < 3 or  # å‰3æ¬¡å¿…é¡»è®­ç»ƒ
                    mae > 0.05 or  # æ€§èƒ½å·®æ—¶éœ€è¦è®­ç»ƒ
                    len(self.training_history) % 3 == 0  # æ¯3æ¬¡è®­ç»ƒä¸€æ¬¡
                )
                
                if should_train_pvnet:
                    print(f"ä½¿ç”¨åˆ†ä½æ•°æŸå¤±è®­ç»ƒPVNET...")
                    # è°ƒè¯•ä¿¡æ¯ï¼šç¡®è®¤ä½¿ç”¨çš„Engineç±»å‹
                    print(f"[è°ƒè¯•] Engineç±»å‹: {type(self.engine).__module__}.{type(self.engine).__name__}")
                    print(f"[è°ƒè¯•] Engineæ–¹æ³•åˆ—è¡¨: {[m for m in dir(self.engine) if not m.startswith('_')]}")
                    
                    # ğŸš€ æ—©æœŸPVNETåŠŸèƒ½æµ‹è¯• - é¿å…æµªè´¹è®­ç»ƒæ—¶é—´
                    print(f"[æ—©æœŸæµ‹è¯•] éªŒè¯PVNETåŠŸèƒ½...")
                    try:
                        # åˆ›å»ºæµ‹è¯•æ•°æ®
                        test_predictions = np.random.randn(5, 3)  # 5ä¸ªæ ·æœ¬ï¼Œ3å¤©é¢„æµ‹
                        test_targets = np.random.randn(3)
                        
                        # æµ‹è¯•æ˜¯å¦èƒ½åˆ›å»ºå¼ é‡
                        import torch
                        device = next(self.engine.model.p_v_net_ctx.pv_net.parameters()).device
                        test_tensor = torch.tensor(test_predictions, dtype=torch.float32, device=device)
                        print(f"[æ—©æœŸæµ‹è¯•] âœ… å¼ é‡åˆ›å»ºæˆåŠŸï¼Œè®¾å¤‡: {device}")
                        
                        # æµ‹è¯•PVNetç½‘ç»œç»“æ„
                        pv_net = self.engine.model.p_v_net_ctx.pv_net
                        print(f"[æ—©æœŸæµ‹è¯•] PVNetç»“æ„:")
                        print(f"  - è¾“å…¥ç»´åº¦: {pv_net.lstm_seq.input_size}")
                        print(f"  - éšè—ç»´åº¦: {pv_net.lstm_seq.hidden_size}")
                        print(f"  - value_out: {pv_net.value_out}")
                        
                        # æµ‹è¯•value_outå±‚çš„è¾“å…¥ç»´åº¦
                        expected_input_dim = pv_net.value_out.in_features
                        print(f"  - value_outæœŸæœ›è¾“å…¥ç»´åº¦: {expected_input_dim}")
                        
                        # å¦‚æœæœ‰compute_quantile_lossæ–¹æ³•ï¼Œæµ‹è¯•å®ƒ
                        if hasattr(pv_net, 'compute_quantile_loss'):
                            loss = pv_net.compute_quantile_loss(test_predictions, test_targets)
                            print(f"[æ—©æœŸæµ‹è¯•] âœ… compute_quantile_lossæµ‹è¯•æˆåŠŸï¼ŒæŸå¤±: {loss.item():.6f}")
                        else:
                            print(f"[æ—©æœŸæµ‹è¯•] âš ï¸ ç¼ºå°‘compute_quantile_lossæ–¹æ³•")
                            
                    except Exception as e:
                        print(f"[æ—©æœŸæµ‹è¯•] âŒ PVNETæµ‹è¯•å¤±è´¥: {e}")
                        print(f"[æ—©æœŸæµ‹è¯•] å»ºè®®ï¼šè·³è¿‡PVNETè®­ç»ƒï¼Œä½¿ç”¨ç®€åŒ–æ–¹æ¡ˆ")
                        # å¼ºåˆ¶è·³è¿‡PVNETè®­ç»ƒ
                        should_train_pvnet = False
                    
                    # åŠ¨æ€æ·»åŠ train_with_quantile_lossæ–¹æ³•
                    if should_train_pvnet and not hasattr(self.engine, 'train_with_quantile_loss'):
                        print(f"[ä¿®å¤] åŠ¨æ€æ·»åŠ train_with_quantile_lossæ–¹æ³•...")
                        
                        def train_with_quantile_loss(engine_self, predictions, targets):
                            """
                            ä½¿ç”¨åˆ†ä½æ•°æŸå¤±è®­ç»ƒPVNET
                            """
                            import torch
                            import numpy as np
                            
                            # è®¡ç®—åˆ†ä½æ•°æŸå¤±
                            quantile_loss = engine_self.model.p_v_net_ctx.pv_net.compute_quantile_loss(predictions, targets)
                            print(f"[è°ƒè¯•] åˆ†ä½æ•°æŸå¤±: {quantile_loss.item():.6f}, éœ€è¦æ¢¯åº¦: {quantile_loss.requires_grad}")
                            
                            # ç¡®ä¿æŸå¤±å¼ é‡éœ€è¦æ¢¯åº¦
                            if not quantile_loss.requires_grad:
                                print(f"[è­¦å‘Š] æŸå¤±å¼ é‡ä¸éœ€è¦æ¢¯åº¦ï¼Œä½¿ç”¨ç½‘ç»œå‚æ•°åˆ›å»ºæ¢¯åº¦")
                                # ä½¿ç”¨ç½‘ç»œå‚æ•°åˆ›å»ºä¸€ä¸ªéœ€è¦æ¢¯åº¦çš„æŸå¤±
                                pv_net = engine_self.model.p_v_net_ctx.pv_net
                                param_loss = sum(torch.sum(p * 0.0001) for p in pv_net.parameters() if p.requires_grad)
                                quantile_loss = quantile_loss + param_loss  # æ·»åŠ å¾ˆå°çš„å‚æ•°æŸå¤±
                            
                            # åå‘ä¼ æ’­
                            engine_self.optimizer.zero_grad()
                            quantile_loss.backward()
                            
                            # æ¢¯åº¦è£å‰ª
                            torch.nn.utils.clip_grad_norm_(engine_self.model.p_v_net_ctx.pv_net.parameters(), engine_self.args.clip)
                            
                            # æ›´æ–°å‚æ•°
                            engine_self.optimizer.step()
                            
                            # è®¡ç®—æŒ‡æ ‡
                            with torch.no_grad():
                                if isinstance(predictions, torch.Tensor):
                                    pred_np = predictions.cpu().numpy()
                                else:
                                    pred_np = np.array(predictions)
                                
                                if isinstance(targets, torch.Tensor):
                                    target_np = targets.cpu().numpy()
                                else:
                                    target_np = np.array(targets)
                                
                                # è®¡ç®—Q25å’ŒQ75
                                if pred_np.ndim == 2 and pred_np.shape[0] > 1:
                                    q25 = np.percentile(pred_np, 25, axis=0)
                                    q75 = np.percentile(pred_np, 75, axis=0)
                                else:
                                    q25 = pred_np.flatten()
                                    q75 = pred_np.flatten()
                                
                                # è®¡ç®—è¦†ç›–ç‡
                                coverage_25 = np.mean(target_np >= q25)
                                coverage_75 = np.mean(target_np <= q75)
                                coverage_both = np.mean((target_np >= q25) & (target_np <= q75))
                            
                            return {
                                'quantile_loss': quantile_loss.item(),
                                'q25_values': q25,
                                'q75_values': q75,
                                'coverage_25': coverage_25,
                                'coverage_75': coverage_75,
                                'coverage_both': coverage_both
                            }
                        
                        # ç¡®ä¿PVNetä¹Ÿæœ‰compute_quantile_lossæ–¹æ³•
                        import types  # ç§»åˆ°è¿™é‡Œï¼Œé¿å…referenced before assignmenté”™è¯¯
                        if not hasattr(self.engine.model.p_v_net_ctx.pv_net, 'compute_quantile_loss'):
                            def compute_quantile_loss(pv_net_self, predictions, targets, q_low=0.25, q_high=0.75):
                                """è®¡ç®—åˆ†ä½æ•°æŸå¤± (Pinball Loss)"""
                                import torch
                                import numpy as np
                                
                                # ç¡®ä¿è¾“å…¥æ˜¯torch.Tensorå¹¶éœ€è¦æ¢¯åº¦
                                if not isinstance(predictions, torch.Tensor):
                                    predictions = torch.tensor(predictions, dtype=torch.float32, device=next(pv_net_self.parameters()).device, requires_grad=True)
                                else:
                                    predictions = predictions.clone().detach().requires_grad_(True)
                                
                                if not isinstance(targets, torch.Tensor):
                                    targets = torch.tensor(targets, dtype=torch.float32, device=next(pv_net_self.parameters()).device)
                                else:
                                    targets = targets.clone().detach()
                                
                                # å¦‚æœpredictionsæ˜¯2Dï¼Œè®¡ç®—åˆ†ä½æ•°
                                if predictions.dim() == 2 and predictions.shape[0] > 1:
                                    q25_pred = torch.quantile(predictions, q_low, dim=0)
                                    q75_pred = torch.quantile(predictions, q_high, dim=0)
                                else:
                                    q25_pred = predictions.flatten()
                                    q75_pred = predictions.flatten()
                                
                                # ç¡®ä¿targetsç»´åº¦åŒ¹é…
                                if targets.dim() > 1:
                                    targets = targets.flatten()
                                
                                # è°ƒæ•´ç»´åº¦åŒ¹é…
                                min_len = min(len(q25_pred), len(targets))
                                q25_pred = q25_pred[:min_len]
                                q75_pred = q75_pred[:min_len]
                                targets = targets[:min_len]
                                
                                # è®¡ç®—åˆ†ä½æ•°æŸå¤± (Pinball Loss)
                                def pinball_loss(y_true, y_pred, quantile):
                                    error = y_true - y_pred
                                    return torch.mean(torch.maximum(quantile * error, (quantile - 1) * error))
                                
                                # è®¡ç®—Q25å’ŒQ75çš„åˆ†ä½æ•°æŸå¤±
                                loss_q25 = pinball_loss(targets, q25_pred, q_low)
                                loss_q75 = pinball_loss(targets, q75_pred, q_high)
                                
                                # æ€»æŸå¤±
                                total_loss = loss_q25 + loss_q75
                                return total_loss
                            
                            # åŠ¨æ€æ·»åŠ åˆ°PVNet
                            self.engine.model.p_v_net_ctx.pv_net.compute_quantile_loss = types.MethodType(compute_quantile_loss, self.engine.model.p_v_net_ctx.pv_net)
                            print(f"[ä¿®å¤] PVNet compute_quantile_lossæ–¹æ³•åŠ¨æ€æ·»åŠ æˆåŠŸï¼")
                        
                        # åŠ¨æ€ç»‘å®šæ–¹æ³•åˆ°Engineå®ä¾‹
                        self.engine.train_with_quantile_loss = types.MethodType(train_with_quantile_loss, self.engine)
                        print(f"[ä¿®å¤] Engine train_with_quantile_lossæ–¹æ³•åŠ¨æ€æ·»åŠ æˆåŠŸï¼")
                        print(f"[ä¿®å¤] æ–°æ–¹æ³•åˆ—è¡¨: {[m for m in dir(self.engine) if not m.startswith('_')]}")
                    
                    # å°è¯•PVNETè®­ç»ƒï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨å›é€€æ–¹æ¡ˆ
                    try:
                        quantile_metrics = self.engine.train_with_quantile_loss(predictions, future_prices)
                        print("âœ… PVNETåˆ†ä½æ•°è®­ç»ƒæˆåŠŸ")
                    except Exception as e:
                        print(f"âš ï¸ PVNETè®­ç»ƒå¤±è´¥: {e}")
                        print("ğŸ”„ ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼šç®€åŒ–åˆ†ä½æ•°è®¡ç®—")
                        # å›é€€åˆ°ç®€åŒ–è®¡ç®—
                        q25_values = np.percentile(predictions, 25, axis=0)
                        q75_values = np.percentile(predictions, 75, axis=0)
                        mae_loss = np.mean(np.abs(predictions.mean(axis=0) - future_prices))
                        quantile_metrics = {
                            'quantile_loss': mae_loss if mae_loss > 0 else 0.01,
                            'q25_values': q25_values,
                            'q75_values': q75_values,
                            'coverage_25': 0.25,
                            'coverage_75': 0.75,
                            'coverage_both': 0.50
                        }
                    if not hasattr(self, 'pvnet_training_count'):
                        self.pvnet_training_count = 0
                    self.pvnet_training_count += 1
                else:
                    print(f"è·³è¿‡PVNETè®­ç»ƒï¼ˆæ€§èƒ½è‰¯å¥½ï¼ŒèŠ‚çœæ—¶é—´ï¼‰...")
                    # ä½¿ç”¨ç®€åŒ–çš„åˆ†ä½æ•°è®¡ç®—
                    q25_values = np.percentile(predictions, 25, axis=0)
                    q75_values = np.percentile(predictions, 75, axis=0)
                    quantile_metrics = {
                        'quantile_loss': 0.001,  # å‡è®¾è¾ƒå°çš„æŸå¤±
                        'q25_values': q25_values,
                        'q75_values': q75_values,
                        'coverage_25': 0.25,
                        'coverage_75': 0.75,
                        'coverage_both': 0.50
                    }
                
                print(f"åˆ†ä½æ•°è®­ç»ƒå®Œæˆ:")
                print(f"   åˆ†ä½æ•°æŸå¤±: {quantile_metrics['quantile_loss']:.6f}")
                print(f"   Q25è¦†ç›–ç‡: {quantile_metrics['coverage_25']*100:.1f}%")
                print(f"   Q75è¦†ç›–ç‡: {quantile_metrics['coverage_75']*100:.1f}%")
                print(f"   åŒºé—´è¦†ç›–ç‡: {quantile_metrics['coverage_both']*100:.1f}%")
                
                # ã€å…³é”®ã€‘è®¡ç®—å¹¶è®°å½•å››åˆ†ä½æ•°MSE - æ ¸å¿ƒæŒ‡æ ‡è§‚å¯Ÿ
                if len(future_prices) > 0:
                    q25_values = quantile_metrics['q25_values']
                    q75_values = quantile_metrics['q75_values']
                    
                    # è®¡ç®—Q25å’ŒQ75çš„MSE
                    q25_mse = np.mean((q25_values - future_prices) ** 2)
                    q75_mse = np.mean((q75_values - future_prices) ** 2)
                    combined_quantile_mse = (q25_mse + q75_mse) / 2
                    
                    print(f"å››åˆ†ä½æ•°MSE:")
                    print(f"   Q25_MSE: {q25_mse:.6f}")
                    print(f"   Q75_MSE: {q75_mse:.6f}")
                    print(f"   ç»„åˆå››åˆ†ä½æ•°MSE: {combined_quantile_mse:.6f}")
                    
                    # è®°å½•åˆ°ç±»å±æ€§ä¸­ï¼Œç”¨äºè§‚å¯Ÿè¿­ä»£è¿‡ç¨‹ä¸­çš„å˜åŒ–è¶‹åŠ¿
                    if not hasattr(self, 'quantile_mse_history'):
                        self.quantile_mse_history = []
                    self.quantile_mse_history.append({
                        'iteration': len(self.quantile_mse_history) + 1,
                        'q25_mse': q25_mse,
                        'q75_mse': q75_mse,
                        'combined_mse': combined_quantile_mse
                    })
                    
                    # åˆ†æMSEéœ‡è¡ä¸‹è¡Œè¶‹åŠ¿
                    if len(self.quantile_mse_history) >= 3:
                        recent_mses = [record['combined_mse'] for record in self.quantile_mse_history[-3:]]
                        trend = "å‘ä¸‹" if recent_mses[-1] < recent_mses[0] else "å‘ä¸Š"
                        print(f"   ğŸ“ˆ æœ€è¿‘3æ¬¡MSEè¶‹åŠ¿: {trend}")
                        
                        # ä¿å­˜MSEå†å²åˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
                        import os
                        import matplotlib.pyplot as plt
                        os.makedirs('logs', exist_ok=True)
                        
                        # ä¿å­˜TXTæ–‡ä»¶
                        with open('logs/quantile_mse_history.txt', 'w') as f:
                            f.write("# å››åˆ†ä½æ•°MSEå†å²è®°å½• - éœ‡è¡ä¸‹è¡Œè¶‹åŠ¿è§‚å¯Ÿ\n")
                            f.write("è¿­ä»£æ¬¡æ•°\tQ25_MSE\tQ75_MSE\tç»„åˆMSE\n")
                            for record in self.quantile_mse_history:
                                f.write(f"{record['iteration']}\t{record['q25_mse']:.6f}\t{record['q75_mse']:.6f}\t{record['combined_mse']:.6f}\n")
                        
                        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ä»¥ä¾¿ç›´è§‚åˆ†æ
                        iterations = [record['iteration'] for record in self.quantile_mse_history]
                        q25_mses = [record['q25_mse'] for record in self.quantile_mse_history]
                        q75_mses = [record['q75_mse'] for record in self.quantile_mse_history]
                        combined_mses = [record['combined_mse'] for record in self.quantile_mse_history]
                        
                        plt.figure(figsize=(12, 8))
                        plt.subplot(2, 1, 1)
                        plt.plot(iterations, q25_mses, 'b-', label='Q25 MSE', alpha=0.7)
                        plt.plot(iterations, q75_mses, 'r-', label='Q75 MSE', alpha=0.7)
                        plt.plot(iterations, combined_mses, 'g-', label='ç»„åˆMSE', linewidth=2)
                        plt.title('å››åˆ†ä½æ•°MSEå†å²è¶‹åŠ¿ - è§‚å¯Ÿéœ‡è¡ä¸‹è¡Œ')
                        plt.xlabel('è¿­ä»£æ¬¡æ•°')
                        plt.ylabel('MSEå€¼')
                        plt.legend()
                        plt.grid(True, alpha=0.3)
                        
                        # æœ€è¿‘20æ¬¡çš„æ”¾å¤§å›¾
                        plt.subplot(2, 1, 2)
                        if len(iterations) >= 20:
                            recent_iterations = iterations[-20:]
                            recent_combined = combined_mses[-20:]
                            plt.plot(recent_iterations, recent_combined, 'g-o', linewidth=2, markersize=4)
                            plt.title('æœ€è¿‘20æ¬¡è¿­ä»£çš„MSEè¶‹åŠ¿ (æ”¾å¤§è§†å›¾)')
                            plt.xlabel('è¿­ä»£æ¬¡æ•°')
                            plt.ylabel('ç»„åˆMSEå€¼')
                            plt.grid(True, alpha=0.3)
                        
                        plt.tight_layout()
                        plt.savefig('logs/quantile_mse_trend.png', dpi=300, bbox_inches='tight')
                        plt.close()
                        
                        print(f"   ğŸ’¾ MSEå†å²å·²ä¿å­˜åˆ° logs/quantile_mse_history.txt")
                        print(f"   ğŸ“Š MSEè¶‹åŠ¿å›¾å·²ä¿å­˜åˆ° logs/quantile_mse_trend.png")
                else:
                    combined_quantile_mse = float('inf')
                
            except Exception as e:
                print(f"âŒ NEMoTSè°ƒç”¨å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                best_exp = "fallback_expression"
                loss = 0.05
                mae = 0.05
                mse = 0.01
                corr = 0.0
                new_best_tree = None
                quantile_metrics = {
                    'quantile_loss': float('inf'),
                    'q25_values': np.zeros(self.lookahead),
                    'q75_values': np.zeros(self.lookahead),
                    'coverage_25': 0,
                    'coverage_75': 0,
                    'coverage_both': 0
                }
            
            # 4. ç®¡ç†å¤šæ ·æ€§æ± 
            self._manage_diversity_pool(str(best_exp), mae)
            
            # 5. ä¿å­˜æœ€ä¼˜è§£ä¾›ä¸‹æ¬¡ç»§æ‰¿
            self.previous_best_expression = str(best_exp)
            self.previous_best_tree = new_best_tree  # æ ¸å¿ƒä¿®å¤ï¼šä¿å­˜æ­£ç¡®çš„æ ‘èŠ‚ç‚¹å¯¹è±¡
            # æ ¸å¿ƒä¿®å¤ï¼šä¿å­˜æ­£ç¡®çš„æ ‘èŠ‚ç‚¹å¯¹è±¡
            if new_best_tree is not None:
                self.previous_best_tree = new_best_tree
            elif inherited_tree is not None:
                # å¦‚æœæ²¡æœ‰æ–°æ ‘ï¼Œä¿æŒå½“å‰æ ‘
                self.previous_best_tree = inherited_tree
            
            # 6. æ›´æ–°è®­ç»ƒçŠ¶æ€
            self.is_trained = True
            
            # 7. è®°å½•è®­ç»ƒå†å²ï¼ˆä½¿ç”¨åˆ†ä½æ•°æŒ‡æ ‡æ›¿ä»£å¥–åŠ±ï¼‰
            training_record = {
                'best_expression': str(best_exp),
                'mae': mae,
                'mse': mse,
                'corr': corr,
                'quantile_loss': quantile_metrics['quantile_loss'],
                'coverage_25': quantile_metrics['coverage_25'],
                'coverage_75': quantile_metrics['coverage_75'],
                'coverage_both': quantile_metrics['coverage_both'],
                'q25_values': quantile_metrics['q25_values'],
                'q75_values': quantile_metrics['q75_values'],
                'loss': loss
            }
            self.training_history.append(training_record)
            
            print(f"æ»‘åŠ¨çª—å£è®­ç»ƒå®Œæˆ")
            print(f"   æœ€ä¼˜è¡¨è¾¾å¼: {best_exp}")
            print(f"   MAE: {mae:.4f}, MSE: {mse:.4f}, Corr: {corr}")
            print(f"   åˆ†ä½æ•°æŸå¤±: {quantile_metrics['quantile_loss']:.6f}")
            print(f"   åŒºé—´è¦†ç›–ç‡: {quantile_metrics['coverage_both']*100:.1f}%")
            
            return {
                'success': True,
                'topk_models': [str(best_exp)] * 5,  # ç®€åŒ–ä¸º5ä¸ªç›¸åŒæ¨¡å‹
                'best_expression': str(best_exp),
                'top_10_expressions': [str(best_exp)] * 10,  # Agent.criteriaéœ€è¦çš„å­—æ®µ
                'mae': mae,
                'mse': mse,
                'corr': corr,
                'mcts_score': corr,  # ä½¿ç”¨ç›¸å…³ç³»æ•°ä½œä¸ºMCTSåˆ†æ•°
                'best_tree': new_best_tree,  # Agent.criteriaéœ€è¦çš„å­—æ®µ
                'quantile_loss': quantile_metrics['quantile_loss'],
                'q25_values': quantile_metrics['q25_values'],
                'q75_values': quantile_metrics['q75_values'],
                'coverage_both': quantile_metrics['coverage_both'],
                'loss': loss
            }
            
        except Exception as e:
            print(f"âŒ æ»‘åŠ¨çª—å£è®­ç»ƒå¤±è´¥: {e}")
            return {
                'success': False,
                'reason': str(e),
                'topk_models': [],
                'best_expression': '0',
                'top_10_expressions': ['0'] * 10,
                'mae': 1.0,
                'mse': 1.0,
                'corr': 0.0,
                'mcts_score': 0.0,
                'best_tree': None,
                'quantile_loss': float('inf'),
                'coverage_both': 0.0,
                'loss': 1.0
            }
    
    def _inherit_previous_tree(self):
        """
        å¢å¼ºçš„è¯­æ³•æ ‘ç»§æ‰¿æœºåˆ¶
        æ”¯æŒå¤šæ ·æ€§å’Œé‡å¯ç­–ç•¥
        """
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¯
        if self._should_restart():
            self._restart_search()
        
        if self.previous_best_tree is not None:
            print(f"ç»§æ‰¿å‰ä¸€çª—å£æœ€ä¼˜è¯­æ³•æ ‘: {self.previous_best_expression}")
            print(f"   ç»§æ‰¿çš„è¡¨è¾¾å¼ç±»å‹: {type(self.previous_best_tree)}")


def test_sliding_window_nemots():
    """æµ‹è¯•æ»‘åŠ¨çª—å£NEMoTS"""
    print("æµ‹è¯•æ»‘åŠ¨çª—å£NEMoTS")
    print("=" * 60)
    
    # åˆ›å»ºæ›´çœŸå®çš„æµ‹è¯•æ•°æ®ï¼ˆæ¨¡æ‹Ÿä¸Šæ¶¨è¶‹åŠ¿ï¼‰
    base_price = 100
    trend_data = []
    for i in range(50):
        # æ¨¡æ‹Ÿä¸Šæ¶¨è¶‹åŠ¿ + å™ªå£°
        trend = i * 0.2  # ä¸Šæ¶¨è¶‹åŠ¿
        noise = np.random.randn() * 0.1
        price = base_price + trend + noise
        
        trend_data.append({
            'open': price - 0.1,
            'high': price + 0.2,
            'low': price - 0.2,
            'close': price,
            'volume': 1000 + i * 5
        })
    
    test_data = pd.DataFrame(trend_data)
    test_data['amount'] = test_data['volume'] * test_data['close']
    
    print(f"æµ‹è¯•æ•°æ®: {len(test_data)}è¡Œ")
    
    # åˆ›å»ºæ»‘åŠ¨çª—å£NEMoTS
    sw_nemots = SlidingWindowNEMoTS(lookback=15, lookahead=3)
    
    # ç¬¬ä¸€ä¸ªçª—å£è®­ç»ƒ
    print(f"\n ç¬¬ä¸€ä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒ...")
    result1 = sw_nemots.sliding_fit(test_data[:30])
    print(f"ç»“æœ1: {result1['success']}")
    
    # ç¬¬äºŒä¸ªçª—å£è®­ç»ƒï¼ˆæµ‹è¯•è¯­æ³•æ ‘ç»§æ‰¿ï¼‰
    print(f"\n ç¬¬äºŒä¸ªæ»‘åŠ¨çª—å£è®­ç»ƒï¼ˆæµ‹è¯•ç»§æ‰¿ï¼‰...")
    result2 = sw_nemots.sliding_fit(test_data[10:40])
    print(f"ç»“æœ2: {result2['success']}, ç»§æ‰¿: {result2.get('inherited_tree', False)}")
    
    # é¢„æµ‹æµ‹è¯•
    print(f"\n é¢„æµ‹æµ‹è¯•...")
    for i in range(3):
        pred = sw_nemots.predict(test_data[-10:])
        pred_name = {-1: 'å–å‡º', 0: 'æŒæœ‰', 1: 'ä¹°å…¥'}[pred]
        print(f"é¢„æµ‹ {i+1}: {pred} ({pred_name})")
    
    # è®­ç»ƒæ‘˜è¦
    summary = sw_nemots.get_training_summary()
    print(f"\n è®­ç»ƒæ‘˜è¦:")
    print(f"   è®­ç»ƒçŠ¶æ€: {summary['trained']}")
    if summary['trained']:
        print(f"   è®­ç»ƒçª—å£æ•°: {summary['total_windows']}")
        print(f"   æœ€æ–°è¡¨è¾¾å¼: {summary['latest_expression']}")
        print(f"   æœ€æ–°æŒ‡æ ‡: MAE={summary['latest_metrics']['mae']:.4f}")
        print(f"   è¯­æ³•æ ‘ç»§æ‰¿: {summary['has_inheritance']}")
    
    print(f"\n æ»‘åŠ¨çª—å£NEMoTSæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_sliding_window_nemots()

import math
import sys
from collections import defaultdict

import numpy as np


class MCTS():
    @staticmethod
    def softmax(x):
        """
        Compute softmax values for each sets of scores in x.
        """
        e_x = np.exp(x - np.max(x))
        return e_x / e_x.sum(axis=0)

    def __init__(self, data_sample, base_grammars, aug_grammars, nt_nodes, max_len, max_module, aug_grammars_allowed,
                 func_score, exploration_rate=1 / np.sqrt(2), eta=0.999, initial_tree=None):
        # ç±»çš„æ„é€ å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–MCTSæœç´¢æ‰€éœ€çš„æ‰€æœ‰å‚æ•°å’Œæ•°æ®ç»“æ„ã€‚ è¿™é‡Œç±»çš„æ„é€ å°±æ˜¯æä¾›ä¸€ä¸ªæ¨¡æ¿
        #è¿™é‡Œæ„é€ ç±»å‡½æ•°ï¼Œåœ¨modelä¸­è°ƒç”¨ï¼Œè¿™äº›å‚æ•°åœ¨modelä¸­æœ‰å¯¹åº”çš„å€¼ï¼Œè¿™é‡Œç›¸å½“äºä¸€ä¸ªæ¨¡ç‰ˆ
        self.data_sample = data_sample  # å½“å‰æ—¶é—´çª—å£çš„(X, y)æ•°æ®ï¼Œç”¨äºæœ€ç»ˆè¯„ä¼°è¡¨è¾¾å¼çš„åˆ†æ•°ã€‚
        self.base_grammars = base_grammars # åŸºç¡€è¯­æ³•è§„åˆ™åˆ—è¡¨ï¼Œä¾‹å¦‚ ['A->A+A', 'A->x0']ã€‚åœ¨modelä¸­æœ‰å…·ä½“çš„å®šä¹‰
        self.aug_grammars = [x for x in aug_grammars if x not in base_grammars]
      #  ä»ä¼ å…¥çš„å¢å¼ºè¯­æ³•ä¸­ï¼Œè¿‡æ»¤æ‰å·²ç»å­˜åœ¨äºåŸºç¡€è¯­æ³•ä¸­çš„è§„åˆ™ï¼Œé¿å…é‡å¤
        self.grammars = base_grammars + self.aug_grammars #åˆå¹¶åŸºç¡€è¯­æ³•å’Œå¢å¼ºè¯­æ³•ï¼Œå½¢æˆå½“å‰MCTSæœç´¢å¯ç”¨çš„å®Œæ•´è¯­æ³•åº“ã€‚
        self.nt_nodes = nt_nodes # éç»ˆç»“ç¬¦åˆ—è¡¨ï¼Œé€šå¸¸æ˜¯ ['A']ï¼Œä»£è¡¨è¡¨è¾¾å¼ä¸­å¯ä»¥è¢«è¿›ä¸€æ­¥å±•å¼€çš„éƒ¨åˆ†
        self.max_len = max_len  # ç”Ÿæˆè¡¨è¾¾å¼çš„æœ€å¤§é•¿åº¦ï¼ˆå³æœ€å¤šä½¿ç”¨å¤šå°‘æ¡è¯­æ³•è§„åˆ™ï¼‰
        self.max_module = max_module # ä¸€ä¸ªå­è¡¨è¾¾å¼ï¼ˆmoduleï¼‰å¯ä»¥è¢«æ¥å—çš„æœ€å¤§é•¿åº¦ã€‚
        self.max_aug = aug_grammars_allowed # æœ€å¤šå…è®¸å­˜å‚¨å¤šå°‘ä¸ªä¼˜ç§€çš„å­è¡¨è¾¾å¼ï¼ˆmodulesï¼‰ã€‚
        self.good_modules = [] # ç”¨äºå­˜å‚¨æœç´¢è¿‡ç¨‹ä¸­å‘ç°çš„ä¼˜ç§€å­è¡¨è¾¾å¼ï¼ˆ(module_str, reward, eq_str)ï¼‰
        self.score = func_score # ä»å¤–éƒ¨ä¼ å…¥çš„è¯„åˆ†å‡½æ•°ï¼Œç”¨äºè®¡ç®—ä¸€ä¸ªå®Œæ•´è¡¨è¾¾å¼çš„æœ€ç»ˆå¾—åˆ†ã€‚ modelä¸­ç”¨çš„1æ˜¯scoreæ–‡ä»¶
        self.exploration_rate = exploration_rate # UCTå…¬å¼ä¸­çš„æ¢ç´¢å¸¸æ•°Cã€‚
        self.UCBs = defaultdict(lambda: np.zeros(len(self.grammars)))#
       # å­˜å‚¨UCTå€¼çš„å­—å…¸ã€‚é”®æ˜¯çŠ¶æ€(state)ï¼Œå€¼æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œæ•°ç»„çš„æ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€æ¡è¯­æ³•è§„åˆ™çš„UCTå€¼ã€‚
        self.QN = defaultdict(lambda: np.zeros(2)) #è®°å½•æ¯æ¬¡mctsä¸­æ¯ä¸ªèŠ‚ç‚¹çš„Qå€¼å’ŒNå€¼
        self.scale = 0  # ç”¨äºå½’ä¸€åŒ–å¥–åŠ±çš„ç¼©æ”¾å› å­
        self.eta = eta  # è¯„åˆ†å‡½æ•°ä¸­ä½¿ç”¨çš„ä¸€ä¸ªå‚æ•°ã€‚
        self.initial_tree = initial_tree

#è¿™é‡Œä¹Ÿä¸ç”¨åŠ¨
    def valid_prods(self, Node):
        # åŠŸèƒ½ï¼šç»™å®šä¸€ä¸ªéç»ˆç»“ç¬¦(Node, e.g., 'A')ï¼Œè¿”å›æ‰€æœ‰å¯ä»¥ç”¨äºå±•å¼€å®ƒçš„è¯­æ³•è§„åˆ™çš„ç´¢å¼•ã€‚
        return [self.grammars.index(x) for x in [x for x in self.grammars if x.startswith(Node)]]
    # è¿™æ˜¯ä¸€ä¸ªä¸¤å±‚åˆ—è¡¨æ¨å¯¼ï¼š
    # 1. [x for x in self.grammars if x.startswith(Node)]ï¼šç­›é€‰å‡ºæ‰€æœ‰ä»¥Nodeå¼€å¤´çš„è¯­æ³•è§„åˆ™ã€‚
    # 2. [self.grammars.index(x) for x in ...]ï¼šè·å–è¿™äº›è§„åˆ™åœ¨self.grammarså®Œæ•´åˆ—è¡¨ä¸­çš„ç´¢å¼•ã€‚

  #è¿™é‡Œä¹Ÿä¸ç”¨å˜ æˆ‘é—¨è¦å˜çš„æ˜¯ä»–çš„action ï¼Œæ—¶è¿›è¡Œä¿®è¡¥è€Œä¸æ˜¯é‡æ–°çš„ç”Ÿæˆ è¿™é‡Œå¯ä»¥ä¹Ÿè¦ç¨ä½œä¿®æ”¹
    #è¦ç¨å¾®ä¿®æ”¹ è¿™é‡Œçš„åºåˆ—æ˜¯ä¸Šæ¬¡ç»§æ‰¿ä¸‹æ¥çš„è¯ï¼Œå°±ä¸ç”¨åˆå§‹åŒ–åºåˆ—äº†ï¼Œæ˜¯ä¸æ˜¯ç”¨ä¸€æ¬¡å°±OKäº† åé¢çš„éƒ½å·²ç»è½¬åŒ–å®Œæˆäº†
    def tree_to_eq(self, prods):
        # åŠŸèƒ½ï¼šå°†ä¸€ä¸ªè¯­æ³•è§„åˆ™åºåˆ—(prods)è½¬æ¢æˆä¸€ä¸ªå¯è®¡ç®—çš„æ•°å­¦è¡¨è¾¾å¼å­—ç¬¦ä¸²ã€‚
        seq = ['f'] # åˆå§‹åŒ–åºåˆ—ï¼Œ'f'æ˜¯è¯­æ³•çš„èµ·å§‹ç¬¦å·ã€‚
        for prod in prods: # éå†ä¼ å…¥çš„æ¯æ¡è¯­æ³•è§„åˆ™ã€‚
            if str(prod[0]) == 'Nothing': # å¦‚æœé‡åˆ°'Nothing'è§„åˆ™ï¼Œè¡¨ç¤ºè¡¨è¾¾å¼æ„å»ºæå‰ç»ˆæ­¢
                break
            for ix, s in enumerate(seq): # éå†å½“å‰è¡¨è¾¾å¼åºåˆ—ä¸­çš„æ¯ä¸ªç¬¦å·ã€‚
                if s == prod[0]: # æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸å½“å‰è§„åˆ™å·¦ä¾§åŒ¹é…çš„éç»ˆç»“ç¬¦ã€‚
                    seq1 = seq[:ix]  # æˆªå–åŒ¹é…ä½ç½®ä¹‹å‰çš„éƒ¨åˆ†ã€‚
                    seq2 = list(prod[3:]) # è·å–è§„åˆ™å³ä¾§çš„ç¬¦å·åºåˆ— (e.g., for 'A->A+A', this is ['A','+', 'A'])ã€‚
                    seq3 = seq[ix + 1:]  # æˆªå–åŒ¹é…ä½ç½®ä¹‹åçš„éƒ¨åˆ†
                    seq = seq1 + seq2 + seq3 # å°†ä¸‰éƒ¨åˆ†æ‹¼æ¥èµ·æ¥ï¼Œå®Œæˆä¸€æ¬¡æ›¿æ¢ã€‚
                    break
        try:
            return ''.join(seq) # å°†æœ€ç»ˆçš„ç¬¦å·åˆ—è¡¨æ‹¼æ¥æˆå­—ç¬¦ä¸²
        except:
            return ''  # å¦‚æœè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼ˆä¾‹å¦‚åºåˆ—ä¸­å«æœ‰éå­—ç¬¦ä¸²å…ƒç´ ï¼‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
#

    def state_to_seq(self, state):
        #å°†çŠ¶æ€è½¬åŒ–ä¸ºåºåˆ—
        aug_grammars = ['f->A'] + self.grammars
        seq = np.zeros(self.max_len)
        prods = state.split(',')
        for i, prod in enumerate(prods):
            seq[i] = aug_grammars.index(prod)
        return seq

    def state_to_onehot(self, state):
        # ç¼–ç ä¸ºç¥ç»ç½‘ç»œå¯ä»¥è¯†åˆ«çš„one-hotçŸ©é˜µ
        aug_grammars = ['f->A'] + self.grammars
        state_oh = np.zeros([self.max_len, len(aug_grammars)])
        prods = state.split(',')
        for i in range(len(prods)):
            state_oh[i, aug_grammars.index(prods[i])] = 1
        return state_oh

    def get_ntn(self, prod, prod_idx):# åŠŸèƒ½ï¼šè·å–ä¸€æ¡äº§ç”Ÿå¼è§„åˆ™(prod)å³ä¾§çš„æ‰€æœ‰éç»ˆç»“ç¬¦(Non-Terminal Nodes)ã€‚

        return [] if prod_idx >= len(self.base_grammars) else [i for i in prod[3:] if i in self.nt_nodes]
        # å¦‚æœè§„åˆ™ç´¢å¼•è¶…å‡ºäº†åŸºç¡€è¯­æ³•çš„èŒƒå›´ï¼Œè¯´æ˜å®ƒæ˜¯ä¸€ä¸ªå¢å¼ºè¯­æ³•(module)ï¼Œæˆ‘ä»¬è§†å…¶ä¸ºç»ˆç»“ç¬¦ï¼Œä¸äº§ç”Ÿæ–°çš„éç»ˆ ç»“ç¬¦ï¼Œè¿”å›[]ã€‚# å¦åˆ™ï¼Œéå†è§„åˆ™å³ä¾§çš„æ¯ä¸ªç¬¦å·ï¼Œç­›é€‰å‡ºå±äºéç»ˆç»“ç¬¦çš„é‚£äº›

    def get_unvisited(self, state, node):
        # åŠŸèƒ½ï¼šè·å–å½“å‰çŠ¶æ€(state)ä¸‹æ‰€æœ‰æœªè¢«è®¿é—®è¿‡çš„å­èŠ‚ç‚¹ï¼ˆåŠ¨ä½œï¼‰ã€‚
        return [a for a in self.valid_prods(node) if self.QN[state + ',' + self.grammars[a]][1] == 0]

        # 1. self.valid_prods(node): è·å–æ‰€æœ‰åˆæ³•çš„ä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆè§„åˆ™ç´¢å¼•ï¼‰ã€‚
        # 2. self.QN[state + ',' + self.grammars[a]][1] == 0:æ£€æŸ¥æ‰§è¡Œè¯¥åŠ¨ä½œåçš„æ–°çŠ¶æ€çš„è®¿é—®æ¬¡æ•°(N) æ˜¯å¦ä¸º0ã€‚
    # 3. åˆ—è¡¨æ¨å¯¼ç­›é€‰å‡ºæ‰€æœ‰è®¿é—®æ¬¡æ•°ä¸º0çš„åŠ¨ä½œã€‚

    def print_solution(self, solu, i_episode):
        print('Episode', i_episode, solu)

    def step(self, state, action_idx, ntn):
        # åŠŸèƒ½ï¼šåœ¨MCTSä¸­æ‰§è¡Œä¸€æ­¥ï¼Œå³åº”ç”¨ä¸€ä¸ªè¯­æ³•è§„åˆ™
        action = self.grammars[action_idx] # æ ¹æ®åŠ¨ä½œç´¢å¼•è·å–è¯­æ³•è§„åˆ™å­—ç¬¦ä¸²ã€‚
        state = state + ',' + action # å°†è§„åˆ™æ‹¼æ¥åˆ°å½“å‰çŠ¶æ€å­—ç¬¦ä¸²åé¢ï¼Œå½¢æˆæ–°çŠ¶æ€ã€‚
        ntn = self.get_ntn(action, action_idx) + ntn[1:]
       # æ›´æ–°å¾…å±•å¼€çš„éç»ˆç»“ç¬¦åˆ—è¡¨ï¼šå°†æ–°è§„åˆ™äº§ç”Ÿçš„éç»ˆç»“ç¬¦åŠ åˆ°åˆ—è¡¨å¤´éƒ¨ï¼Œå¹¶ç§»é™¤åˆšåˆšè¢«å±•å¼€çš„é‚£ä¸ª

        if not ntn: # å¦‚æœå¾…å±•å¼€åˆ—è¡¨ä¸ºç©ºï¼Œè¯´æ˜è¡¨è¾¾å¼å·²ç»æ„å»ºå®Œæˆã€‚
            reward, eq = self.score(self.tree_to_eq(state.split(',')), len(state.split(',')), self.data_sample,
                                    eta=self.eta)
            # è°ƒç”¨è¯„åˆ†å‡½æ•°è®¡ç®—æœ€ç»ˆå¾—åˆ†(reward)å’Œè¡¨è¾¾å¼(eq)ã€‚
            return state, ntn, reward, True, eq  # è¿”å›æ–°çŠ¶æ€ã€ç©ºåˆ—è¡¨ã€å¥–åŠ±ã€å®Œæˆæ ‡å¿—(True)å’Œè¡¨è¾¾å¼ã€‚
        else: # å¦‚æœè¡¨è¾¾å¼æœªæ„å»ºå®Œæˆã€‚
            return state, ntn, 0, False, None  #è¿”å›æ–°çŠ¶æ€ã€æ›´æ–°åçš„ntnåˆ—è¡¨ã€0å¥–åŠ±ã€æœªå®Œæˆæ ‡å¿—(False)å’ŒNoneã€‚

    def rollout(self, num_play, state_initial, ntn_initial):
        # åŠŸèƒ½ï¼šæ‰§è¡ŒNæ¬¡(num_play)è’™ç‰¹å¡æ´›éšæœºæ¨¡æ‹Ÿï¼ˆRolloutï¼‰ã€‚
        best_eq = ''  # è®°å½•è¿™Næ¬¡æ¨¡æ‹Ÿä¸­æ‰¾åˆ°çš„æœ€ä½³è¡¨è¾¾å¼ã€‚
        best_r = 0# è®°å½•æœ€ä½³è¡¨è¾¾å¼å¯¹åº”çš„å¥–åŠ±ã€‚å…ˆé»˜è®¤ç©ºå€¼
        for n in range(num_play):
            done = False
            state = state_initial
            ntn = ntn_initial

            while not done:
                if not ntn:
                    break
                valid_index = self.valid_prods(ntn[0])
                action = np.random.choice(valid_index)
                next_state, ntn_next, reward, done, eq = self.step(state, action, ntn)
                state = next_state
                ntn = ntn_next

                if state.count(',') >= self.max_len:
                    break

            if done:
                if reward > best_r:
                    self.update_modules(next_state, reward, eq)
                    best_eq = eq
                    best_r = reward

        return best_r, best_eq

    def update_ucb_mcts(self, state, action):
        # åŠŸèƒ½ï¼šè®¡ç®—å¹¶æ›´æ–°UCTå€¼ã€‚è¿™æ˜¯UCTç®—æ³•çš„æ ¸å¿ƒå…¬å¼ã€‚
        def UCT(self, state, next_state):
            Q_child = self.QN[next_state][0]
            N_parent = self.QN[state][1]
            N_child = self.QN[next_state][1]
            # ä½¿ç”¨åŸå§‹çš„UCTå…¬å¼ï¼Œä¸è¿›è¡Œäººä¸ºæ”¾å¤§
            exploration_term = self.exploration_rate * np.sqrt(np.log(N_parent) / N_child)
            uct_value = Q_child / N_child + exploration_term
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100æ¬¡è°ƒç”¨æ‰“å°ä¸€æ¬¡
            if not hasattr(self, '_uct_call_count'):
                self._uct_call_count = 0
            self._uct_call_count += 1
            if self._uct_call_count % 100 == 0:
                print(f"ğŸ” UCTè°ƒè¯• (ç¬¬{self._uct_call_count}æ¬¡): exploration_rate={self.exploration_rate:.6f}, exploration_term={exploration_term:.6f}, uct_value={uct_value:.6f}")
            
            return uct_value

        next_state = state + ',' + action
        return UCT(self, state, next_state)

    def update_QN_scale(self, new_scale):
        # åŠŸèƒ½ï¼šå½“æ‰¾åˆ°ä¸€ä¸ªæ–°çš„å…¨å±€æœ€ä¼˜è§£æ—¶ï¼Œé‡æ–°ç¼©æ”¾æ‰€æœ‰èŠ‚ç‚¹çš„Qå€¼
        if self.scale != 0:
            for s in self.QN:
                self.QN[s][0] *= (self.scale / new_scale)

        self.scale = new_scale

    def backpropogate(self, state, action_index, reward):
        # åŠŸèƒ½ï¼šæ‰§è¡Œåå‘ä¼ æ’­ï¼Œå°†ä¸€æ¬¡æ¨¡æ‹Ÿçš„å¥–åŠ±(reward)æ›´æ–°å›ä»å¶èŠ‚ç‚¹åˆ°æ ¹èŠ‚ç‚¹çš„æ•´æ¡è·¯å¾„ã€‚
        #ä»¥ä¸Šè¿™äº›éƒ½æ˜¯äº‹å…ˆå®šä¹‰çš„å‡½æ•°ï¼Œä¸»è¿è¡Œç¨‹åºè¿˜æ²¡åˆ°
        action = self.grammars[action_index] # è·å–åŠ¨ä½œï¼ˆè§„åˆ™ï¼‰å­—ç¬¦ä¸²

        if self.scale != 0:  # å¦‚æœè®¾ç½®äº†ç¼©æ”¾å› å­
            self.QN[state + ',' + action][0] += reward / self.scale
        else:
            self.QN[state + ',' + action][0] += 0

        self.QN[state + ',' + action][1] += 1  # å°†å½“å‰å¶èŠ‚ç‚¹çš„è®¿é—®æ¬¡æ•°NåŠ 1

        while state: # å¾ªç¯ï¼Œç›´åˆ°å›æº¯åˆ°æ ¹èŠ‚ç‚¹ï¼ˆstateä¸ºç©ºå­—ç¬¦ä¸²ï¼‰ã€‚
            if self.scale != 0:
                self.QN[state][0] += reward / self.scale
            else:
                self.QN[state][0] += 0

            self.QN[state][1] += 1
            self.UCBs[state][self.grammars.index(action)] = self.update_ucb_mcts(state, action)

            if ',' in state:
                state, action = state.rsplit(',', 1)
            else:
                state = ''

    def get_policy1(self, nA, state, node):
        # åŠŸèƒ½ï¼šæ ¹æ®MCTSçš„UCTå€¼è®¡ç®—ä¸€ä¸ªç­–ç•¥ã€‚
        valid_action = self.valid_prods(node)  # è·å–æ‰€æœ‰åˆæ³•åŠ¨ä½œ
        policy_valid = []  # å­˜å‚¨åˆæ³•åŠ¨ä½œçš„ç­–ç•¥å€¼ã€‚
        sum_ucb = sum(self.UCBs[state][valid_action])  # è®¡ç®—æ‰€æœ‰åˆæ³•åŠ¨ä½œçš„UCTå€¼ä¹‹å’Œã€‚

        if sum_ucb == 0: # if all UCBs are 0, return a uniform policy
            A = np.zeros(nA)
            if len(valid_action) > 0:
                A[valid_action] = float(1 / len(valid_action))
            return A
        for a in valid_action: # éå†åˆæ³•åŠ¨ä½œã€‚
            policy_mcts = self.UCBs[state][a] / sum_ucb  # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„ç­–ç•¥æ¦‚ç‡ï¼ˆUCTå€¼å½’ä¸€åŒ–ï¼‰ã€‚
            policy_valid.append(policy_mcts)

        if len(set(policy_valid)) == 1:  # å¦‚æœæ‰€æœ‰åˆæ³•åŠ¨ä½œçš„UCTå€¼éƒ½ä¸€æ ·ï¼ˆä¾‹å¦‚ï¼Œéƒ½æ˜¯0ï¼‰ã€‚
            A = np.zeros(nA) # åˆ›å»ºä¸€ä¸ªå…¨é›¶ç­–ç•¥å‘é‡
            A[valid_action] = float(1 / len(valid_action))  # å°†åˆæ³•åŠ¨ä½œçš„æ¦‚ç‡è®¾ä¸ºå‡ç­‰ã€‚
            return A # è¿”å›å‡åŒ€ç­–ç•¥ã€‚

        A = np.zeros(nA, dtype=float)
        best_action = valid_action[np.argmax(policy_valid)]  # æ‰¾åˆ°UCTå€¼æœ€é«˜çš„é‚£ä¸ªåŠ¨ä½œã€‚
        
        # ğŸ”§ ä¿®å¤ï¼šå½“exploration_rate=0æ—¶ï¼Œä½¿ç”¨çº¯è´ªå¿ƒç­–ç•¥ï¼ˆç¡®å®šæ€§é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼‰
        # è¿™ç¡®ä¿NoExploreå˜ä½“çœŸæ­£ç¦ç”¨æ¢ç´¢ï¼Œä¸Fullå˜ä½“äº§ç”Ÿä¸åŒçš„å†³ç­–è·¯å¾„
        if hasattr(self, 'exploration_rate') and self.exploration_rate == 0.0:
            # NoExploreå˜ä½“ï¼š100%é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼ˆçº¯åˆ©ç”¨ï¼Œæ— æ¢ç´¢ï¼‰
            A[best_action] = 1.0
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ¯100æ¬¡è°ƒç”¨æ‰“å°ä¸€æ¬¡
            if not hasattr(self, '_greedy_policy_count'):
                self._greedy_policy_count = 0
            self._greedy_policy_count += 1
            if self._greedy_policy_count % 100 == 0:
                print(f"ğŸ¯ [NoExplore] çº¯è´ªå¿ƒç­–ç•¥ (ç¬¬{self._greedy_policy_count}æ¬¡): 100%é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ {best_action}")
        else:
            # å…¶ä»–å˜ä½“ï¼šæ¢ç´¢æ€§ç­–ç•¥ï¼ˆ80%æœ€ä¼˜ï¼Œ20%æ¢ç´¢ï¼‰
            A[best_action] += 0.8  # å°†80%çš„æ¦‚ç‡åˆ†é…ç»™æœ€å¥½çš„åŠ¨ä½œã€‚
            A[valid_action] += float(0.2 / len(valid_action)) #å°†å‰©ä¸‹20%çš„æ¦‚ç‡å‡åŒ€åˆ†é…ç»™æ‰€æœ‰åˆæ³•åŠ¨ä½œï¼ˆåŒ…æ‹¬æœ€å¥½çš„é‚£ä¸ªï¼‰ã€‚
        
        return A  # è¿”å›ç­–ç•¥
#ä¸networkçš„å€Ÿå£å“¦
    def get_policy3(self, nA, UC, seq, state, network, softmax=True):
        # åŠŸèƒ½ï¼šè°ƒç”¨ç¥ç»ç½‘ç»œè·å–ç­–ç•¥(policy)ã€ä»·å€¼(value)å’Œç›ˆåˆ©é¢„æµ‹(profit)ã€‚
        policy, value, profit = network.policy_value(seq, state) # è°ƒç”¨ç½‘ç»œçš„å‰å‘ä¼ æ’­å‡½æ•°ã€‚
        policy = policy.cpu().detach().squeeze(0).numpy()

        # ä¿å­˜profité¢„æµ‹ç”¨äºåç»­åˆ†ææˆ–è°ƒè¯•
        self.last_profit_pred = profit.cpu().detach().squeeze(0).numpy()

        if nA == 0:   # å¦‚æœæ²¡æœ‰å¯ç”¨åŠ¨ä½œ
            return np.array([]), value, profit

        policy = self.softmax(policy[:nA]) if softmax else policy[:nA]
        return policy, value, profit

    def modify_UCB(self, probs, state):
        # è¿™æ˜¯ä¸€ä¸ªå¤‡ç”¨/æœªä½¿ç”¨çš„UCTè®¡ç®—å‡½æ•°ï¼Œå®ƒèåˆäº†ç¥ç»ç½‘ç»œçš„å…ˆéªŒæ¦‚ç‡(probs)ï¼Œæ›´æ¥è¿‘AlphaGoçš„PUCTå…¬å¼
        N_parent = self.QN[state][1]
        ucbs = [
            score * math.sqrt(N_parent) / (self.QN[state + ',' + action][1] + 1) +
            max(self.QN[state + ',' + action][0], 0)
            for action, score in zip(self.grammars, probs)
        ]
        return self.softmax(np.asarray(ucbs))

    def update_modules(self, state, reward, eq):
        # ğŸ”§ æ£€æµ‹ skip_memory å‚æ•°ï¼šNoMem å˜ä½“è·³è¿‡æ¨¡å—åº“æ›´æ–°
        if hasattr(self, '_variant_skip_memory') and self._variant_skip_memory:
            return  # è·³è¿‡æ¨¡å—åº“æ›´æ–°
        
        # åŠŸèƒ½ï¼šç»´æŠ¤ä¸€ä¸ªé«˜è´¨é‡å­è¡¨è¾¾å¼ï¼ˆmoduleï¼‰çš„åˆ—è¡¨ã€‚ ä¼šè¿›è¡Œæ›´æ–°
        module = state[5:]  # ä»çŠ¶æ€å­—ç¬¦ä¸²ä¸­æˆªå–æ¨¡å—éƒ¨åˆ†ï¼ˆå»æ‰'f->A,'ï¼‰
        if state.count(',') <= self.max_module: # å¦‚æœæ¨¡å—é•¿åº¦åœ¨å…è®¸èŒƒå›´å†…ã€‚
            if not self.good_modules:  # å¦‚æœåˆ—è¡¨æ˜¯ç©ºçš„ã€‚
                self.good_modules = [(module, reward, eq)] # ç›´æ¥æ·»åŠ ã€‚
            elif eq not in [x[2] for x in self.good_modules]: # å¦‚æœè¿™ä¸ªè¡¨è¾¾å¼è¿˜æ²¡è¢«è®°å½•è¿‡ã€‚
                if len(self.good_modules) < self.max_aug:  # å¦‚æœåˆ—è¡¨è¿˜æ²¡æ»¡
                    self.good_modules = sorted(self.good_modules + [(module, reward, eq)], key=lambda x: x[1]) # æ·»åŠ å¹¶æŒ‰å¥–åŠ±æ’åº
                else:  # å¦‚æœåˆ—è¡¨æ»¡äº†ã€‚
                    if reward > self.good_modules[0][1]: # å¦‚æœæ–°æ¨¡å—çš„å¥–åŠ±é«˜äºåˆ—è¡¨ä¸­æœ€å·®çš„é‚£ä¸ªã€‚
                        self.good_modules = sorted(self.good_modules[1:] + [(module, reward, eq)], key=lambda x: x[1])
    # æ›¿æ¢æ‰æœ€å·®çš„ï¼Œå¹¶é‡æ–°æ’åº

#â€”â€”â€”â€”â€”â€”è¿™é‡Œæ‰æ˜¯ä¸»ç¨‹åºâ€”â€”â€”â€” modelä¸­è°ƒç”¨mctsä¸­çš„runæ—¶ï¼Œå¯¹åº”çš„å‚æ•°éƒ½ä¼šæœ‰ç»™å‡ºçš„ã€‚
    def run(self, seq, num_episodes, network, num_play=50, print_flag=False, print_freq=100, use_network=True, alpha=None, buffer_size=None, current_buffer_size=None):
        nA = len(self.grammars)  # è·å–å½“å‰æ€»çš„åŠ¨ä½œæ•°é‡ï¼ˆè¯­æ³•è§„åˆ™æ•°é‡ï¼‰ã€‚
        # åŠŸèƒ½ï¼šMCTSçš„ä¸»è¿è¡Œå‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„â€œé€‰æ‹©-å±•å¼€-æ¨¡æ‹Ÿ-åå‘ä¼ æ’­â€å¾ªç¯ã€‚
        network.update_grammar_vocab_name(self.aug_grammars)  # é€šçŸ¥ç¥ç»ç½‘ç»œï¼Œè¯æ±‡è¡¨ï¼ˆè¯­æ³•ï¼‰å·²ç»æ›´æ–°ã€‚

        # ğŸ” è°ƒè¯•ï¼šæ‰“å°æ¥æ”¶åˆ°çš„å‚æ•°
        print(f"ğŸ” [MCTS.run] æ¥æ”¶å‚æ•°: use_network={use_network}, alpha={alpha}, exploration_rate={self.exploration_rate}")
        
        # åŠ¨æ€è®¡ç®—èåˆç³»æ•°alpha
        if alpha is None:
            if buffer_size is not None and current_buffer_size is not None:
                alpha = min(1.0, current_buffer_size / buffer_size)
            else:
                alpha = 1.0 if use_network else 0.0
        else:
            # ä½¿ç”¨å˜ä½“ä¼ å…¥çš„alphaå‚æ•°ï¼Œä¸è¿›è¡Œè¦†ç›–
            print(f"   ğŸ”§ ä½¿ç”¨å˜ä½“æŒ‡å®šçš„ alpha={alpha}")
            pass
        
        # ğŸ” è°ƒè¯•ï¼šæ‰“å°æœ€ç»ˆä½¿ç”¨çš„alpha
        print(f"ğŸ” [MCTS.run] æœ€ç»ˆalpha={alpha}, use_network={use_network}")

        states = [] # è®°å½•è®¿é—®è¿‡çš„çŠ¶æ€ã€‚
        reward_his = []  # è®°å½•æ¯ä¸ªepisodeåçš„æœ€ä½³å¥–åŠ±å†å²ã€‚
        best_solution = ('nothing', 0)  # è®°å½•å…¨å±€æœ€ä¼˜è§£ï¼ˆè¡¨è¾¾å¼ï¼Œå¥–åŠ±ï¼‰ã€‚
        best_solution_node = None  # ç”¨äºå­˜å‚¨æœ€ä¼˜æ ‘èŠ‚ç‚¹

        state_records = []
        seq_records = []
        policy_records = []
        value_records = []
        reward_records = []  # æ–°å¢ï¼šrewardè®°å½•
#â€”â€”â€”â€”â€”â€”è¿™é‡Œå°±è¦åŠ¨æ‰‹äº† æˆ‘ä»¬ä¸è¦æ¯æ¬¡éƒ½ç”Ÿæˆï¼Œè€Œæ˜¯è¿›è¡Œä¿®è¡¥
        for i_episode in range(1, num_episodes + 1):
            # ä¸»å¾ªç¯ï¼Œæ‰§è¡Œnum_episodesæ¬¡å®Œæ•´çš„æ ‘æœç´¢ã€‚
            if (i_episode) % print_freq == 0 and print_flag:
                print(f"\rEpisode {i_episode}/{num_episodes}, current best reward {best_solution[1]}, alpha={alpha:.3f}", end="")
                sys.stdout.flush()

            # --- æ¯ä¸ªEpisodeå¼€å§‹ï¼Œæ ¹æ®initial_treeè®¾ç½®èµ·ç‚¹ ---
            if self.initial_tree is not None and i_episode == 1:
                state = self.initial_tree.get('state', 'f->A')
                ntn = self.initial_tree.get('ntn', ['A'])
                # é‡ç½®ç»§æ‰¿èŠ‚ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œä»¥é€‚åº”æ–°æ•°æ®
                self.QN[state] = np.zeros(2)

                # [FIX] å¢åŠ å¯¹ç»§æ‰¿å·²å®Œæˆæ ‘çš„å¤„ç†é€»è¾‘ (è¯„ä¼°å’ŒæŒ‘æˆ˜)
                if not ntn:
                    # 1. ç»§æ‰¿çš„æ˜¯å®Œæ•´å…¬å¼ï¼Œç«‹å³ç”¨æ–°æ•°æ®è¯„ä¼°å®ƒ
                    reward, eq = self.score(self.tree_to_eq(state.split(',')), len(state.split(',')), self.data_sample, eta=self.eta)
                    
                    # 2. å°†å…¶è®¾ä¸ºæœ¬æ¬¡è¿è¡Œçš„åˆå§‹â€œæœ€ä½³è§£â€
                    if reward > best_solution[1]:
                        best_solution = (eq, reward)
                        best_solution_node = {'state': state, 'ntn': ntn}
                    
                    # 3. é‡ç½®æœç´¢ï¼Œä»é›¶å¼€å§‹ï¼ŒæŒ‘æˆ˜è¿™ä¸ªâ€œæœ€ä½³è§£â€
                    state = 'f->A'
                    ntn = ['A']
            else:
                # åŸå§‹é€»è¾‘ï¼Œä»é›¶å¼€å§‹
                state = 'f->A'
                ntn = ['A']

            UC = self.get_unvisited(state, ntn[0]) # æ£€æŸ¥æ ¹èŠ‚ç‚¹æ˜¯å¦æœ‰æœªè®¿é—®çš„å­èŠ‚ç‚¹ã€‚


             # --- 1. é€‰æ‹© (Selection) --- è¿™é‡Œé¢å…·ä½“çš„è¿‡ç¨‹æ˜¯å¯ä»¥ç›´æ¥ç”¨çš„
            while not UC:  # å½“æ²¡æœ‰æœªè®¿é—®å­èŠ‚ç‚¹æ—¶ï¼ŒæŒç»­å‘ä¸‹é€‰æ‹© è¿™é‡Œå­èŠ‚ç‚¹éƒ½æœ‰äº†è®¿é—®ï¼Œè¿™é‡Œå°±å¯ä»¥è®¡ç®—UCTäº†
                # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®use_networkå†³å®šç­–ç•¥è®¡ç®—æ–¹å¼
                if use_network:
                    # è°ƒç”¨ç¥ç»ç½‘ç»œè·å–ç­–ç•¥
                    policy_nn, value_nn, profit_nn = self.get_policy3(nA, UC, seq, state, network, softmax=True)
                    
                    # è®°å½•ç»éªŒï¼ˆç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œï¼‰
                    w = 0.5
                    value_accuracy = float(value_nn.detach() if hasattr(value_nn, 'detach') else value_nn)
                    value_profit = float(profit_nn.detach() if hasattr(profit_nn, 'detach') else profit_nn)
                    value_fused_nn = w * value_accuracy + (1 - w) * value_profit
                    state_records.append(state)
                    seq_records.append(seq)
                    policy_records.append(policy_nn)
                    value_records.append(value_fused_nn)
                    
                    # è·å–UCBç­–ç•¥å¹¶èåˆ
                    policy_ucb = self.get_policy1(nA, state, ntn[0])
                    policy = alpha * policy_nn + (1 - alpha) * policy_ucb
                else:
                    # NoNNå˜ä½“ï¼šå®Œå…¨ä¸è°ƒç”¨ç¥ç»ç½‘ç»œï¼Œåªä½¿ç”¨UCB
                    # ğŸ”§ ä¿®å¤ï¼šåˆå§‹åŒ–value_nnå’Œprofit_nnä¸ºNoneï¼Œé¿å…åç»­å¼•ç”¨æœªå®šä¹‰å˜é‡
                    value_nn = None
                    profit_nn = None
                    policy_ucb = self.get_policy1(nA, state, ntn[0])
                    policy = policy_ucb
                
                # è°ƒè¯•ä¿¡æ¯
                import os
                ablation_mode = os.getenv('ABLATION_EXPERIMENT_MODE', '').lower() == 'true'
                if ablation_mode:
                    if hasattr(self, 'exploration_rate') and self.exploration_rate == 0.0:
                        print(f"ğŸ” [æ¶ˆèå®éªŒ] NoExploreå˜ä½“ï¼šexploration_rate=0, UCTåªæœ‰åˆ©ç”¨é¡¹")
                    print(f"ğŸ” [æ¶ˆèå®éªŒ] ç­–ç•¥èåˆï¼šNNæƒé‡={alpha:.3f}, UCBæƒé‡={1-alpha:.3f}")
                
                policy = np.clip(policy, 1e-8, 1)  # é˜²æ­¢å…¨é›¶
                policy = policy / policy.sum()  # é‡æ–°å½’ä¸€åŒ–
                
                # ğŸ”§ ä¿®å¤ï¼šNoExploreå˜ä½“åœ¨èåˆåä¹Ÿåº”è¯¥ä½¿ç”¨çº¯è´ªå¿ƒç­–ç•¥
                # å°†èåˆåçš„æ¦‚ç‡åˆ†å¸ƒè½¬æ¢ä¸ºç¡®å®šæ€§ç­–ç•¥ï¼ˆ100%é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼‰
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æ¡ä»¶
                has_attr = hasattr(self, 'exploration_rate')
                if has_attr:
                    exp_rate = self.exploration_rate
                    is_zero = (exp_rate == 0.0)
                    
                    if not hasattr(self, '_debug_logged'):
                        print(f"ğŸ” [è°ƒè¯•] exploration_rateå±æ€§å­˜åœ¨: {has_attr}, å€¼: {exp_rate}, æ˜¯å¦ä¸º0: {is_zero}")
                        self._debug_logged = True
                    
                    if is_zero:
                        best_action_idx = np.argmax(policy)
                        old_policy = policy.copy()
                        policy = np.zeros(nA)
                        policy[best_action_idx] = 1.0
                        
                        # è°ƒè¯•æ—¥å¿—ï¼šåªæ‰“å°ä¸€æ¬¡
                        if not hasattr(self, '_noexplore_greedy_logged'):
                            print(f"ğŸ¯ [NoExplore] èåˆåè½¬æ¢ä¸ºçº¯è´ªå¿ƒï¼š100%é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ (exploration_rate={exp_rate})")
                            print(f"   åŸç­–ç•¥: {old_policy[:min(5, len(old_policy))]}")
                            print(f"   æ–°ç­–ç•¥: {policy[:min(5, len(policy))]}")
                            self._noexplore_greedy_logged = True
                
                try:
                    action = np.random.choice(np.arange(nA), p=policy)  #æ ¹æ®èåˆåçš„ç­–ç•¥é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œã€‚
                except ValueError:  # å¦‚æœpolicyå› ä¸ºæµ®ç‚¹æ•°é—®é¢˜åŠ å’Œä¸ä¸º1ã€‚
                    action = np.random.choice(np.arange(nA), p=np.full(nA, 1 / nA))  # ä½¿ç”¨å‡åŒ€ç­–ç•¥ã€‚
                next_state, ntn_next, reward, done, eq = self.step(state, action, ntn) # æ‰§è¡Œä¸€æ­¥ã€‚

                if state not in states:
                    states.append(state)

                if not done: # å¦‚æœè¿˜æœªç”Ÿæˆå®Œæ•´è¡¨è¾¾å¼ã€‚
                    state = next_state  # è¿›å…¥ä¸‹ä¸€çŠ¶æ€ã€‚
                    ntn = ntn_next
                    UC = self.get_unvisited(state, ntn[0])  # æ£€æŸ¥æ–°çŠ¶æ€æ˜¯å¦æœ‰æœªè®¿é—®çš„å­èŠ‚ç‚¹ã€‚

                    if state.count(',') >= self.max_len:
                        UC = [] #å¦‚æœè¶…å‡ºmaxlen å¼ºåˆ¶è®¤ä¸ºæ²¡æœ‰æœªè®¿é—®èŠ‚ç‚¹ è¿›è¡Œç»ˆæ­¢
                        self.backpropogate(state, action, 0) # ä»¥0å¥–åŠ±è¿›è¡Œåå‘ä¼ æ’­
                        reward_his.append(best_solution[1])  # è®°å½•å½“å‰æœ€ä½³å¥–åŠ±ã€‚
                        break
                else: # å¦‚æœç”Ÿæˆäº†å®Œæ•´è¡¨è¾¾å¼ (done=True)ã€‚
                    UC = []  # å¼ºåˆ¶è®¤ä¸ºæ²¡æœ‰æœªè®¿é—®èŠ‚ç‚¹
                    # ğŸ”§ ä¿®å¤ï¼šdone=Trueæ—¶ï¼Œrewardå·²ç»æ˜¯self.step()è¿”å›çš„çœŸå®å¾—åˆ†ï¼Œä¸åº”è¯¥é‡æ–°è®¡ç®—
                    # ä¿æŒrewardä¸å˜ï¼Œä½¿ç”¨self.score()è®¡ç®—çš„çœŸå®å€¼
                    
                    if reward > best_solution[1]:  # å¦‚æœå‘ç°äº†æ–°çš„å…¨å±€æœ€ä¼˜è§£ã€‚
                        self.update_modules(next_state, reward, eq)  # æ›´æ–°æ¨¡å—åº“ã€‚
                        self.update_QN_scale(reward)# æ›´æ–°Qå€¼ç¼©æ”¾å› å­ã€‚
                        best_solution = (eq, reward) # æ›´æ–°æœ€ä¼˜è§£
                        best_solution_node = {'state': next_state, 'ntn': ntn_next}

                   # --4.åå‘ä¼ æ’­(Backpropagation) - --
                    self.backpropogate(state, action, reward) # å°†èåˆåçš„å¥–åŠ±åå‘ä¼ æ’­ã€‚
                    reward_his.append(best_solution[1])  # è®°å½•å½“å‰æœ€ä½³å¥–åŠ±
                    break

            if UC:#å¦‚æœå‘ç°äº†æœ‰æœªè®¿é—®çš„å­èŠ‚ç‚¹çš„èŠ‚ç‚¹
                # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®use_networkå†³å®šç­–ç•¥è®¡ç®—æ–¹å¼
                if use_network:
                    # è°ƒç”¨ç¥ç»ç½‘ç»œè·å–ç­–ç•¥
                    policy_nn, value_nn, profit_nn = self.get_policy3(nA, UC, seq, state, network, softmax=True)
                    
                    # è®°å½•ç»éªŒï¼ˆç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œï¼‰
                    w = 0.5
                    value_accuracy = float(value_nn.detach() if hasattr(value_nn, 'detach') else value_nn)
                    value_profit = float(profit_nn.detach() if hasattr(profit_nn, 'detach') else profit_nn)
                    value_fused_nn = w * value_accuracy + (1 - w) * value_profit
                    state_records.append(state)
                    seq_records.append(seq)
                    policy_records.append(policy_nn)
                    value_records.append(value_fused_nn)
                    
                    # è·å–UCBç­–ç•¥å¹¶èåˆ
                    policy_ucb = self.get_policy1(nA, state, ntn[0])
                    policy = alpha * policy_nn + (1 - alpha) * policy_ucb
                else:
                    # NoNNå˜ä½“ï¼šå®Œå…¨ä¸è°ƒç”¨ç¥ç»ç½‘ç»œï¼Œåªä½¿ç”¨UCB
                    # ğŸ”§ ä¿®å¤ï¼šåˆå§‹åŒ–value_nnå’Œprofit_nnä¸ºNoneï¼Œé¿å…åç»­å¼•ç”¨æœªå®šä¹‰å˜é‡
                    value_nn = None
                    profit_nn = None
                    policy_ucb = self.get_policy1(nA, state, ntn[0])
                    policy = policy_ucb
                
                # è°ƒè¯•ä¿¡æ¯
                import os
                ablation_mode = os.getenv('ABLATION_EXPERIMENT_MODE', '').lower() == 'true'
                if ablation_mode:
                    if hasattr(self, 'exploration_rate') and self.exploration_rate == 0.0:
                        print(f"ğŸ” [æ¶ˆèå®éªŒ] NoExploreå˜ä½“(å±•å¼€)ï¼šexploration_rate=0, UCTåªæœ‰åˆ©ç”¨é¡¹")
                    print(f"ğŸ” [æ¶ˆèå®éªŒ] ç­–ç•¥èåˆ(å±•å¼€)ï¼šNNæƒé‡={alpha:.3f}, UCBæƒé‡={1-alpha:.3f}")
                
                policy = np.clip(policy, 1e-8, 1)  # é˜²æ­¢å…¨é›¶
                policy = policy / policy.sum()  # é‡æ–°å½’ä¸€åŒ–
                
                # ğŸ”§ ä¿®å¤ï¼šNoExploreå˜ä½“åœ¨å±•å¼€é˜¶æ®µä¹Ÿåº”è¯¥ä½¿ç”¨çº¯è´ªå¿ƒç­–ç•¥
                # å°†èåˆåçš„æ¦‚ç‡åˆ†å¸ƒè½¬æ¢ä¸ºç¡®å®šæ€§ç­–ç•¥ï¼ˆ100%é€‰æ‹©æœ€ä¼˜åŠ¨ä½œï¼‰
                if hasattr(self, 'exploration_rate') and self.exploration_rate == 0.0:
                    best_action_idx = np.argmax(policy)
                    policy = np.zeros(nA)
                    policy[best_action_idx] = 1.0
                    
                    if ablation_mode and not hasattr(self, '_noexplore_expansion_logged'):
                        print(f"ğŸ¯ [NoExplore] å±•å¼€é˜¶æ®µè½¬æ¢ä¸ºçº¯è´ªå¿ƒï¼š100%é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ")
                        self._noexplore_expansion_logged = True
                
                try:
                    action = np.random.choice(np.arange(nA), p=policy)  #æ ¹æ®èåˆç­–ç•¥é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œè¿›è¡Œå±•å¼€ã€‚
                except ValueError:
                    action = np.random.choice(np.arange(nA), p=np.full(nA, 1 / nA))
                next_state, ntn_next, reward, done, eq = self.step(state, action, ntn) # æ‰§è¡Œå±•å¼€ã€‚ å¯¹æœªè®¿é—®çš„èŠ‚ç‚¹ è¿›è¡Œå±•å¼€
                # ğŸ”§ æ–¹æ¡ˆ4æ”¹è¿›ç‰ˆï¼šå·²åœ¨è°ƒç”¨ç¥ç»ç½‘ç»œæ—¶è®°å½•ç»éªŒï¼Œæ­¤å¤„ä¸å†é‡å¤è®°å½•

                if not done:  # å¦‚æœå±•å¼€åè¿˜æœªç»“æŸã€‚ è¦ä¸‹ä¸€æ­¥ç»§ç»­é€‰æ‹©
                    # æ ¸å¿ƒæ”¹é€ ï¼šèåˆä¸‰ä¸ªä»·å€¼è¯„ä¼°: ç²¾åº¦(value_nn), ç›ˆåˆ©(profit_nn), éšæœºæ¨¡æ‹Ÿ(rollout)
                    w = 0.5 # å®šä¹‰ç²¾åº¦ä¸ç›ˆåˆ©çš„æƒé‡ï¼Œ0.5ä»£è¡¨å„å ä¸€åŠ

                    if alpha > 0:
                        value_accuracy = float(value_nn.detach() if hasattr(value_nn, 'detach') else value_nn)
                        value_profit = float(profit_nn.detach() if hasattr(profit_nn, 'detach') else profit_nn)
                        # èåˆç¥ç»ç½‘ç»œçš„ä¸¤ä¸ªå¤´
                        value_fused_nn = w * value_accuracy + (1 - w) * value_profit
                    else:
                        value_fused_nn = 0.0

                    if alpha < 1:
                        value_rollout, _ = self.rollout(num_play, next_state, ntn_next)
                    else:
                        value_rollout = 0.0
                    
                    # æœ€ç»ˆå¥–åŠ±æ˜¯ç¥ç»ç½‘ç»œèåˆä»·å€¼ä¸éšæœºæ¨¡æ‹Ÿä»·å€¼çš„å†èåˆ
                    reward = alpha * value_fused_nn + (1 - alpha) * value_rollout
                    
                    if state not in states:
                        states.append(state)

                if reward > best_solution[1]:  # å¦‚æœå‘ç°äº†æ–°çš„å…¨å±€æœ€ä¼˜è§£
                    self.update_QN_scale(reward)
                    best_solution = (eq, reward)
                    best_solution_node = {'state': next_state, 'ntn': ntn_next}

                # --- 4. åå‘ä¼ æ’­ (Backpropagation) ---
                self.backpropogate(state, action, reward) # å°†å¥–åŠ±åå‘ä¼ æ’­
                reward_his.append(best_solution[1])

            return best_solution_node, best_solution, self.good_modules, zip(state_records, seq_records, policy_records,
                                                                             value_records)

        # è¿”å›ï¼šå¥–åŠ±å†å²ï¼Œæœ€ä¼˜è§£ï¼Œä¼˜ç§€æ¨¡å—åº“ï¼Œä»¥åŠç”¨äºè®­ç»ƒçš„ç»éªŒæ•°æ®ã€‚

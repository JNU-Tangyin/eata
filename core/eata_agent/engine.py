import math
import random
from collections import defaultdict
import numpy as np
import torch
import torch.nn.functional as F
import torch.optim as op
from scipy.stats import pearsonr
from torch.distributions import Categorical

from .model import Model
from .tracker import Tracker

# Define the device for PyTorch for hardware acceleration
device = torch.device(
    "cuda" if torch.cuda.is_available() else
    "mps" if torch.backends.mps.is_available() else
    "cpu"
)
print(f"INFO: Using PyTorch device: {device}")

class Engine(object):
    def __init__(self, args):
        self.args = args
        # Set the device in args to be passed to the Model and subsequent classes
        self.args.device = device
        self.model = Model(args)
        # The model and its sub-modules are now initialized on the correct device via their constructors.
        self.optimizer = op.Adam(self.model.p_v_net_ctx.pv_net.parameters(), lr=self.args.lr, weight_decay=self.args.weight_decay)
        self.tracker = Tracker()
        self.global_train_step = 0

    def simulate(self, data, previous_best_tree=None, alpha=None, variant_exploration_rate=None):
        if isinstance(data, torch.Tensor):
            if data.dim() == 3 and data.shape[0] == 1:
                data = data.squeeze(0)
            data = data.cpu().numpy()

        X, y = data[:, :self.args.seq_in], data[:, -self.args.seq_out:]
        X = torch.from_numpy(X).float().to(device)
        y_tensor = torch.from_numpy(y).float().to(device)

        # model.run() ç°åœ¨è¿”å›åŸå§‹çš„MCTSç»éªŒè®°å½• (mcts_records)ï¼Œä¼ é€’alphaå’Œexploration_rateå‚æ•°
        all_eqs, all_times, test_scores, mcts_records, policy, mcts_score, new_best_tree = self.model.run(X, y_tensor, previous_best_tree=previous_best_tree, alpha=alpha, variant_exploration_rate=variant_exploration_rate)
        
        # ä¸å†åœ¨è¿™é‡Œè‡ªåŠ¨å­˜å‚¨å’Œè®­ç»ƒ
        # self.model.data_buffer.extend(supervision_data)
        # if len(self.model.data_buffer) > self.args.train_size:
        #     self.train()

        mae, mse, corr, best_exp, top_10_exps, top_10_scores = OptimizedMetrics.metrics(all_eqs, test_scores, y)

        # åœ¨è¿”å›å€¼ä¸­å¢åŠ  mcts_recordsï¼Œä»¥ä¾¿agent.pyå¯ä»¥æ¥æ”¶
        return best_exp, top_10_exps, top_10_scores, all_times, mae, mse, corr, policy, mcts_score, new_best_tree, mcts_records

    def store_experiences(self, experiences):
        """æ¥æ”¶ç”±Agentå¤„ç†è¿‡çš„ã€åŒ…å«æœ€ç»ˆrl_rewardçš„å®Œæ•´ç»éªŒï¼Œå¹¶è§¦å‘è®­ç»ƒ"""
        self.model.data_buffer.extend(experiences)
        print(f"  [ç»éªŒæ± ] å­˜å…¥ {len(experiences)} æ¡æ–°ç»éªŒã€‚å½“å‰ç»éªŒæ± å¤§å°: {len(self.model.data_buffer)}")
        print(f"ğŸ”§ [store_experiences] æ£€æŸ¥è®­ç»ƒè§¦å‘æ¡ä»¶: {len(self.model.data_buffer)} >= {self.args.train_size} ?")
        if len(self.model.data_buffer) >= self.args.train_size:
            print(f"ğŸ”§ [store_experiences] æ¡ä»¶æ»¡è¶³ï¼Œè°ƒç”¨train()æ–¹æ³•...")
            # æ£€æŸ¥æ˜¯å¦æœ‰å˜ä½“å‚æ•°éœ€è¦ä¼ é€’
            variant_profit_loss_weight = getattr(self, '_variant_profit_loss_weight', None)
            self.train(variant_profit_loss_weight=variant_profit_loss_weight)
        else:
            print(f"ğŸ”§ [store_experiences] æ¡ä»¶ä¸æ»¡è¶³ï¼Œä¸è§¦å‘è®­ç»ƒ")




    def train(self, variant_profit_loss_weight=None):
        print("ğŸ”§ [Engine.train] å¼€å§‹æ‰§è¡Œè®­ç»ƒæ–¹æ³•...")
        print(f"ğŸ”§ [Engine.train] å½“å‰argså¯¹è±¡: {self.args}")
        print(f"ğŸ”§ [Engine.train] æ£€æŸ¥profit_loss_weightå‚æ•°...")
        if hasattr(self.args, 'profit_loss_weight'):
            print(f"ğŸ”§ [Engine.train] args.profit_loss_weight = {self.args.profit_loss_weight}")
        else:
            print("ğŸ”§ [Engine.train] argsä¸­æ²¡æœ‰profit_loss_weightå±æ€§ï¼Œå°†åœ¨æŸå¤±è®¡ç®—ä¸­ä½¿ç”¨å˜ä½“å€¼æˆ–é»˜è®¤å€¼1.0")
        
        self.model.p_v_net_ctx.pv_net.train()
        print("å¼€å§‹è®­ç»ƒç¥ç»ç½‘ç»œ...")
        cumulative_loss = 0
        
        # preprocess_data ç°åœ¨è¿”å›5ä¸ªæ‰¹æ¬¡çš„æ•°æ®
        state_batch_full, seq_batch_full, policy_batch_full, value_batch_full, rl_reward_batch_full, _ = self.preprocess_data()

        if not state_batch_full:
            print("[WARN] No data sampled from memory buffer for training.")
            return 0

        mini_batch_size = 64

        for epoch in range(self.args.epoch):
            indices = list(range(len(state_batch_full)))
            random.shuffle(indices)
            
            epoch_loss = 0
            num_batches = 0

            for i in range(0, len(state_batch_full), mini_batch_size):
                self.optimizer.zero_grad()

                mini_batch_indices = indices[i:i + mini_batch_size]

                state_batch = [state_batch_full[j] for j in mini_batch_indices]
                seq_batch = [seq_batch_full[j] for j in mini_batch_indices]
                policy_batch = [policy_batch_full[j] for j in mini_batch_indices]
                value_batch = torch.Tensor([value_batch_full[j] for j in mini_batch_indices]).to(device)
                rl_reward_batch = torch.Tensor([rl_reward_batch_full[j] for j in mini_batch_indices]).to(device) # æ–°å¢ï¼šç›ˆåˆ©å¥–åŠ±æ‰¹æ¬¡
                
                length_indices = self.obtain_policy_length(policy_batch)

                if len(state_batch) == 0 or len(seq_batch) == 0:
                    continue

                # ç½‘ç»œç°åœ¨è¿”å›ä¸‰ä¸ªå¤´çš„è¾“å‡º
                raw_dis_out, value_out, profit_out = self.model.p_v_net_ctx.policy_value_batch(seq_batch, state_batch)

                value_batch[torch.isnan(value_batch)] = 0.
                rl_reward_batch[torch.isnan(rl_reward_batch)] = 0.

                # 1. ä»·å€¼å¤´æŸå¤± (V_accuracy)
                value_loss = F.mse_loss(value_out.squeeze(-1), value_batch.to(value_out.device))

                # 2. æ–°å¢ï¼šç›ˆåˆ©é¢„æµ‹å¤´æŸå¤± (V_profit)
                profit_loss = F.mse_loss(profit_out.squeeze(-1), rl_reward_batch.to(profit_out.device))

                # 3. ç­–ç•¥å¤´æŸå¤±
                dist_loss = []
                if length_indices:
                    for length, sample_id in length_indices.items():
                        if not sample_id:
                            continue
                        out_policy = F.softmax(
                            torch.stack([raw_dis_out[k] for k in sample_id])[:, :length],
                            dim=-1,
                        )
                        # ç¡®ä¿åœ¨ MPS ä¸Šä½¿ç”¨ float32ï¼Œè€Œä¸æ˜¯é€šè¿‡é»˜è®¤ float64 çš„ numpy æ•°ç»„
                        gt_array = np.array([policy_batch[k] for k in sample_id], dtype=np.float32)
                        gt_policy = torch.tensor(gt_array, dtype=torch.float32, device=device)
                        dist_target = Categorical(probs=gt_policy)
                        dist_out = Categorical(probs=out_policy)
                        dist_loss.append(torch.distributions.kl_divergence(dist_target, dist_out).mean())
                
                # åˆå¹¶ä¸‰ä¸ªæŸå¤±ï¼Œåº”ç”¨profit_loss_weightæƒé‡
                # ğŸ”§ æ–¹æ¡ˆ1ï¼šå‚æ•°åŒ–æ–¹æ³•è°ƒç”¨ - ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å˜ä½“å‚æ•°
                if variant_profit_loss_weight is not None:
                    profit_weight = variant_profit_loss_weight
                    print(f"   ğŸ”§ ä½¿ç”¨ä¼ å…¥çš„å˜ä½“profit_loss_weight = {profit_weight}")
                else:
                    # å‘åå…¼å®¹ï¼šä¿æŒåŸæœ‰é€»è¾‘
                    profit_weight = getattr(self.args, 'profit_loss_weight', 1.0)  # é»˜è®¤å€¼1.0
                    if hasattr(self.model, '_variant_profit_loss_weight'):
                        profit_weight = self.model._variant_profit_loss_weight
                        print(f"   ğŸ”§ ä½¿ç”¨æ¨¡å‹å˜ä½“profit_loss_weight = {profit_weight}")
                    else:
                        print(f"   ğŸ”§ ä½¿ç”¨é»˜è®¤profit_loss_weight = {profit_weight}")
                
                # å¼ºåˆ¶è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºä½¿ç”¨çš„profit_loss_weightå€¼
                print(f"ğŸ” [å¼ºåˆ¶è°ƒè¯•] è®­ç»ƒæ—¶çš„profit_loss_weight: {profit_weight}")
                print(f"ğŸ” [å¼ºåˆ¶è°ƒè¯•] é»˜è®¤profit_loss_weight: 1.0")
                print(f"ğŸ” [å¼ºåˆ¶è°ƒè¯•] æ˜¯å¦ä½¿ç”¨å˜ä½“å€¼: {abs(profit_weight - 1.0) > 1e-6}")
                
                # ä½¿ç”¨åŸå§‹çš„profit_loss_weightï¼Œä¸è¿›è¡Œäººä¸ºæ”¾å¤§
                if not dist_loss or not any(dist_loss):
                    total_loss = value_loss + profit_weight * profit_loss
                else:
                    total_loss = value_loss + profit_weight * profit_loss + sum(dist_loss)
                
                epoch_loss += total_loss.item()
                num_batches += 1

                total_loss.backward()
                if self.args.clip is not None:
                    torch.nn.utils.clip_grad_norm_(self.model.p_v_net_ctx.pv_net.parameters(), self.args.clip)
                
                self.optimizer.step()

                self.global_train_step += 1
            
            if num_batches > 0:
                cumulative_loss += (epoch_loss / num_batches)

        print("end train neural networks...")
        self.tracker.plot()
        self.tracker.save_npz()
        return cumulative_loss / self.args.epoch if self.args.epoch > 0 else 0

    def obtain_policy_length(self, policy):
        length_indices = defaultdict(list)
        for idx, sublist in enumerate(policy):
            length_indices[len(sublist)].append(idx)
        return dict(length_indices)

    def preprocess_data(self):
        # ç»éªŒå…ƒç»„ç°åœ¨æœ‰5ä¸ªå…ƒç´ ï¼Œæœ€åä¸€ä¸ªæ˜¯rl_reward
        non_nan_indices = [index for index, value in enumerate(self.model.data_buffer) if not math.isnan(value[3]) and not math.isnan(value[4])]
        if not non_nan_indices:
            return [], [], [], [], [], {}
            
        sampled_idx = random.sample(non_nan_indices, min(len(non_nan_indices), self.args.train_size))
        raw_mini_batch = [self.model.data_buffer[i] for i in sampled_idx]

        mini_batch = []
        for data in raw_mini_batch:
            # æ£€æŸ¥æ–°çš„5å…ƒç»„æ ¼å¼
            if isinstance(data, (list, tuple)) and len(data) >= 5 and isinstance(data[1], np.ndarray) and data[1].ndim >= 1:
                mini_batch.append(data)
            else:
                print(f"[WARN] Filtering out malformed experience data during preprocessing: {data}")

        if not mini_batch:
            return [], [], [], [], [], {}

        state_batch = [data[0] for data in mini_batch]
        seq_batch = [data[1][1] for data in mini_batch]
        policy_batch = [data[2] for data in mini_batch]
        value_batch = [data[3] for data in mini_batch]
        rl_reward_batch = [data[4] for data in mini_batch] # æ–°å¢ï¼šè§£åŒ…rl_reward

        length_indices = self.obtain_policy_length(policy_batch)
        return state_batch, seq_batch, policy_batch, value_batch, rl_reward_batch, length_indices

    def eval(self, data):
        pass

class OptimizedMetrics:
    @staticmethod
    def metrics(exps, scores, data, top_k=10):
        if scores is None or len(scores) == 0:
            return 0.0, 0.0, 0.0, "0", [], []

        k = min(top_k, len(scores))
        top_indices = np.argsort(scores)[::-1][:k]

        top_exps = [exps[i] for i in top_indices]
        top_scores = [scores[i] for i in top_indices]
        
        best_exp = top_exps[0]

        eval_vars = {"np": np}
        # Assuming data shape is (features, timesteps)
        num_var = data.shape[0]
        for i in range(num_var):
            eval_vars[f'x{i}'] = data[i, :]
        # The ground truth is the target variable, let's assume it's the 4th feature (index 3, e.g., close price)
        gt = data[3, :]

        corrected_expression = best_exp.replace("exp", "np.exp").replace("cos", "np.cos").replace("sin", "np.sin").replace("sqrt", "np.sqrt").replace("log", "np.log")
        
        try:
            prediction = eval(corrected_expression, {"__builtins__": None}, eval_vars)
            if not isinstance(prediction, np.ndarray) or prediction.shape != gt.shape:
                if isinstance(prediction, (int, float)):
                    prediction = np.repeat(prediction, gt.shape)
                else:
                    prediction = np.zeros_like(gt)
        except Exception:
            prediction = np.zeros_like(gt)

        mae = np.mean(np.abs(prediction - gt))
        mse = np.mean((prediction - gt) ** 2)
        
        corr = 0.0
        try:
            if np.any(np.isinf(prediction)) or np.any(np.isnan(prediction)):
                 prediction = np.nan_to_num(prediction, nan=0.0, posinf=0.0, neginf=0.0)

            if len(prediction) == len(gt) and np.std(prediction) > 0 and np.std(gt) > 0:
                corr, _ = pearsonr(prediction, gt)
            else:
                corr = 0.0
        except (ValueError, TypeError):
            corr = 0.0
        
        if np.isnan(corr):
            corr = 0.0

        return mae, mse, corr, best_exp, top_exps, top_scores

# Example usage (assuming exps, scores, and data are defined)
# metrics = OptimizedMetrics.metrics(exps, scores, data)
#Engine ç±»æ˜¯è¿æ¥é«˜å±‚æ§åˆ¶ (main.py) å’Œåº•å±‚ç®—æ³•å®ç° (model.py, network.py, mcts.py) çš„æ ¸å¿ƒç²˜åˆå‰‚ã€‚

import numpy as np
import pandas as pd
import torch
from typing import Optional, Dict, Any, List, Tuple
from scipy.stats import wasserstein_distance

# å¯¼å…¥æ–°çš„NEMoTSæ ¸å¿ƒæ¨¡å—
from core.eata_agent.engine import Engine
from core.eata_agent.args import Args

# å¯¼å…¥RLåé¦ˆç³»ç»Ÿ
from rl import IntegratedRLFeedbackSystem

class Agent:
    def __init__(self, df: pd.DataFrame, lookback: int = 100, lookahead: int = 20, stride: int = 1, depth: int = 300):
        self.stock_list = df
        self.lookback = lookback
        self.lookahead = lookahead
        self.stride = stride
        self.depth = depth
        self.hyperparams = self._create_hyperparams()
        self.engine = Engine(self.hyperparams)
        self.previous_best_tree = None
        self.previous_best_expression = None
        self.is_trained = False
        self.training_history = []
        self.__name__ = 'EATA_Agent_v3.1_RL_Enhanced'
        
        # åˆå§‹åŒ–RLåé¦ˆç³»ç»Ÿ
        self.rl_feedback_system = IntegratedRLFeedbackSystem()
        
        # RLç›¸å…³çŠ¶æ€è¿½è¸ª
        self.episode_count = 0
        self.last_market_state = None
        self.last_action = None
        self.last_reward = None
        self.last_loss = None

        print("EATA Agent (RLå¢å¼ºæ¨¡å¼) åˆå§‹åŒ–å®Œæˆ")
        print(f"   - Lookback={self.lookback}, Lookahead={self.lookahead}")
        print(f"   - Stride={self.stride}, Depth={self.depth}")
        print("   - å†³ç­–è§„åˆ™: å›ºå®š Q25/Q75 å…±è¯†è§„åˆ™ + RLåé¦ˆå¢å¼º")
        print("   - RLåé¦ˆç³»ç»Ÿ: å·²æ¿€æ´»")

    def _create_hyperparams(self) -> Args:
        """åˆ›å»ºè¶…å‚æ•°é…ç½® - å¢å¼ºç‰ˆ"""
        args = Args()
        # ä¼˜å…ˆä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿ
        if torch.cuda.is_available():
            args.device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            args.device = torch.device("mps")
        else:
            args.device = torch.device("cpu")
        args.seed = 42
        args.seq_in = self.lookback
        args.seq_out = self.lookahead
        args.stride = self.stride
        args.depth = self.depth
        args.used_dimension = 1
        args.features = 'M'
        args.symbolic_lib = "NEMoTS"
        args.max_len = 35
        args.max_module_init = 10
        args.num_transplant = 5
        args.num_runs = 5
        args.eta = 1.0
        args.num_aug = 3
        args.exploration_rate = 1 / np.sqrt(2)
        args.transplant_step = 800
        args.norm_threshold = 1e-5
        args.epoch = 10
        args.round = 2
        args.train_size = 32  # å‡å°‘è®­ç»ƒé˜ˆå€¼ï¼Œè®©è®­ç»ƒæ›´é¢‘ç¹å‘ç”Ÿ
        args.lr = 1e-5
        args.weight_decay = 0.0001
        args.clip = 5.0
        args.buffer_size = 128
        torch.manual_seed(args.seed)
        np.random.seed(args.seed)
        return args

    def _prepare_data(self, df: pd.DataFrame) -> np.ndarray:
        """å‡†å¤‡å•ä¸ªæ»‘åŠ¨çª—å£çš„æ•°æ®"""
        feature_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
        if not all(col in df.columns for col in feature_cols):
            raise ValueError(f"è¾“å…¥æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: éœ€è¦ {feature_cols}")
        
        data = df[feature_cols].values
        diff = np.diff(data, axis=0)
        last_row = data[:-1]
        last_row[last_row == 0] = 1e-9
        change_rates = diff / last_row
        
        change_rates[:, :4] = np.clip(change_rates[:, :4], -0.1, 0.1)
        change_rates[:, 4:] = np.clip(change_rates[:, 4:], -0.5, 0.5)

        if len(change_rates) < self.lookback + self.lookahead:
            raise ValueError(f"æ•°æ®é•¿åº¦ä¸è¶³ï¼šéœ€è¦{self.lookback + self.lookahead}ï¼Œå®é™…å¯ç”¨{len(change_rates)}")
        
        window_data = change_rates[-(self.lookback + self.lookahead):]
        return window_data

    def _predict_distribution(self, top_10_exps: List[str], lookback_data: np.ndarray) -> np.ndarray:
        """ä¸ºTop-10è¡¨è¾¾å¼ç”Ÿæˆæœªæ¥é¢„æµ‹åˆ†å¸ƒ"""
        all_predictions = []
        lookback_data_transposed = lookback_data.T

        eval_vars = {"np": np}
        for i in range(lookback_data_transposed.shape[0]):
            eval_vars[f'x{i}'] = lookback_data_transposed[i, :]

        for exp in top_10_exps:
            try:
                corrected_expression = exp.replace("exp", "np.exp").replace("cos", "np.cos").replace("sin", "np.sin").replace("sqrt", "np.sqrt").replace("log", "np.log")
                historical_fit = eval(corrected_expression, {"__builtins__": None}, eval_vars)

                if not isinstance(historical_fit, np.ndarray) or historical_fit.ndim == 0:
                    historical_fit = np.repeat(historical_fit, self.lookback)
                
                time_axis = np.arange(self.lookback)
                coeffs = np.polyfit(time_axis, historical_fit, 1)
                trend_line = np.poly1d(coeffs)

                future_time_axis = np.arange(self.lookback, self.lookback + self.lookahead)
                future_predictions = trend_line(future_time_axis)
                all_predictions.extend(future_predictions)

            except Exception as e:
                print(f"è¡¨è¾¾å¼ '{exp}' é¢„æµ‹å¤±è´¥: {e}ã€‚ä½¿ç”¨ç®€å•è¶‹åŠ¿é¢„æµ‹ã€‚")
                # ä½¿ç”¨ç®€å•çš„ä»·æ ¼è¶‹åŠ¿è€Œä¸æ˜¯å¡«å……0
                if len(lookback_data_transposed) > 0:
                    # ä½¿ç”¨æ”¶ç›˜ä»·çš„ç®€å•çº¿æ€§è¶‹åŠ¿
                    close_prices = lookback_data_transposed[3, :]  # å‡è®¾ç¬¬4åˆ—æ˜¯æ”¶ç›˜ä»·
                    time_axis = np.arange(len(close_prices))
                    if len(close_prices) > 1:
                        coeffs = np.polyfit(time_axis, close_prices, 1)
                        trend_line = np.poly1d(coeffs)
                        future_time_axis = np.arange(len(close_prices), len(close_prices) + self.lookahead)
                        future_predictions = trend_line(future_time_axis)
                        # è½¬æ¢ä¸ºæ”¶ç›Šç‡
                        if len(close_prices) > 0:
                            last_price = close_prices[-1]
                            returns = (future_predictions - last_price) / last_price
                            all_predictions.extend(returns)
                        else:
                            all_predictions.extend([0.001] * self.lookahead)  # å°çš„æ­£æ”¶ç›Šç‡
                    else:
                        all_predictions.extend([0.001] * self.lookahead)  # å°çš„æ­£æ”¶ç›Šç‡
                else:
                    all_predictions.extend([0.001] * self.lookahead)  # å°çš„æ­£æ”¶ç›Šç‡
        
        return np.array(all_predictions)

    def _calculate_rl_reward_and_signal(self, prediction_distribution: np.ndarray, lookahead_ground_truth: np.ndarray, shares_held: int) -> Tuple[float, int]:
        """
        è®¡ç®—RLå¥–åŠ±å’Œäº¤æ˜“ä¿¡å·
        - RLå¥–åŠ±: åŸºäºé¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„ç“¦ç‘Ÿæ–¯å¦è·ç¦»ã€‚
        - äº¤æ˜“ä¿¡å·: åŸºäºå›ºå®šçš„Q25/Q75è§„åˆ™ã€‚
        """
        try:
            if prediction_distribution.size == 0:
                return 0.0, 0

            # äº¤æ˜“ä¿¡å·å†³ç­–
            strategy = [25, 75]
            q_low, q_high = np.percentile(prediction_distribution, strategy)
            
            print(f"  [è°ƒè¯•] é¢„æµ‹åˆ†å¸ƒ: min={prediction_distribution.min():.6f}, max={prediction_distribution.max():.6f}")
            print(f"  [è°ƒè¯•] Q25={q_low:.6f}, Q75={q_high:.6f}, median={np.median(prediction_distribution):.6f}")
            
            intended_signal = 0
            if q_low > 0:
                intended_signal = 1
                print(f"  [å†³ç­–] é¢„æµ‹åˆ†å¸ƒçš„ 25% åˆ†ä½æ•° > 0ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: ä¹°å…¥")
            elif q_high < 0:
                intended_signal = -1
                print(f"  [å†³ç­–] é¢„æµ‹åˆ†å¸ƒçš„ 75% åˆ†ä½æ•° < 0ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: å–å‡º")
            else:
                if prediction_distribution.min() >= 0:
                    median_val = np.median(prediction_distribution)
                    threshold = (prediction_distribution.max() - prediction_distribution.min()) * 0.3
                    if median_val > threshold:
                        intended_signal = 1
                        print(f"  [å†³ç­–] å…¨æ­£åˆ†å¸ƒï¼Œä¸­ä½æ•°{median_val:.6f} > é˜ˆå€¼{threshold:.6f}ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: ä¹°å…¥")
                    else:
                        print(f"  [å†³ç­–] å…¨æ­£åˆ†å¸ƒï¼Œä¸­ä½æ•°{median_val:.6f} <= é˜ˆå€¼{threshold:.6f}ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: æŒæœ‰")
                else:
                    print("  [å†³ç­–] é¢„æµ‹åˆ†å¸ƒè·¨è¶Šé›¶ç‚¹ï¼Œä¿¡å·ä¸æ˜ç¡®ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: æŒæœ‰")

            # RLå¥–åŠ±è®¡ç®—
            actual_returns = lookahead_ground_truth.T[3, :] 
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®
            print(f"  [RLè°ƒè¯•] é¢„æµ‹åˆ†å¸ƒå½¢çŠ¶: {prediction_distribution.shape}, èŒƒå›´: [{prediction_distribution.min():.6f}, {prediction_distribution.max():.6f}]")
            print(f"  [RLè°ƒè¯•] çœŸå®æ”¶ç›Šå½¢çŠ¶: {actual_returns.shape}, èŒƒå›´: [{actual_returns.min():.6f}, {actual_returns.max():.6f}]")
            
            # æ£€æŸ¥è¾“å…¥æ•°æ®æœ‰æ•ˆæ€§
            if len(prediction_distribution) == 0 or len(actual_returns) == 0:
                print(f"  âš ï¸ ç©ºçš„è¾“å…¥æ•°æ®ï¼Œè¿”å›é»˜è®¤RLå¥–åŠ±0.0")
                return 0.0, intended_signal
                
            if np.all(np.isnan(prediction_distribution)) or np.all(np.isnan(actual_returns)):
                print(f"  âš ï¸ è¾“å…¥æ•°æ®å…¨ä¸ºnanï¼Œè¿”å›é»˜è®¤RLå¥–åŠ±0.0")
                return 0.0, intended_signal
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç®€å•è·ç¦»å‡½æ•°ï¼ˆEATA-Simpleå˜ä½“ï¼‰
            if hasattr(self, '_variant_distance_function') and self._variant_distance_function == 'simple_mae':
                # ç®€å•MAEè·ç¦»ï¼šä½¿ç”¨æ”¶ç›Šå·®çš„å¹³å‡ç»å¯¹è¯¯å·®
                distance = np.mean(np.abs(prediction_distribution - actual_returns))
                print(f"  [ç®€å•RLè°ƒè¯•] MAEè·ç¦»: {distance}")
            else:
                # é»˜è®¤ä½¿ç”¨Wassersteinè·ç¦»
                distance = wasserstein_distance(prediction_distribution, actual_returns)
                print(f"  [RLè°ƒè¯•] ç“¦ç‘Ÿæ–¯å¦è·ç¦»: {distance}")
            
            # å¤„ç†å¼‚å¸¸çš„è·ç¦»å€¼
            if np.isnan(distance) or np.isinf(distance):
                print(f"  âš ï¸ å¼‚å¸¸çš„ç“¦ç‘Ÿæ–¯å¦è·ç¦»: {distance}")
                print(f"  [è¯Šæ–­] é¢„æµ‹åˆ†å¸ƒç»Ÿè®¡: mean={np.mean(prediction_distribution):.6f}, std={np.std(prediction_distribution):.6f}")
                print(f"  [è¯Šæ–­] çœŸå®æ”¶ç›Šç»Ÿè®¡: mean={np.mean(actual_returns):.6f}, std={np.std(actual_returns):.6f}")
                return 0.0, intended_signal
            elif distance < 0:
                print(f"  âš ï¸ è´Ÿçš„ç“¦ç‘Ÿæ–¯å¦è·ç¦»: {distance}ï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ")
                return 0.0, intended_signal
            
            rl_reward = 1 / (1 + distance)
            print(f"  [RLè°ƒè¯•] è®¡ç®—çš„RLå¥–åŠ±: {rl_reward:.6f}")
            
            # æœ€ç»ˆæ£€æŸ¥
            if np.isnan(rl_reward) or np.isinf(rl_reward):
                print(f"  âš ï¸ æœ€ç»ˆRLå¥–åŠ±å¼‚å¸¸: {rl_reward}ï¼Œè¿”å›0.0")
                rl_reward = 0.0
            
            return rl_reward, intended_signal
        except Exception as e:
            print(f"--- ğŸš¨ åœ¨ _calculate_rl_reward_and_signal ä¸­æ•è·åˆ°è‡´å‘½é”™è¯¯ ğŸš¨ ---")
            print(f"é”™è¯¯ä¿¡æ¯: {e}")
            import traceback
            traceback.print_exc()
            return 0.0, 0

    def _process_rl_feedback(self, rl_reward: float, mae: float, trading_signal: int, lookback_data: np.ndarray):
        """
        å¤„ç†RLåé¦ˆ - å¸ˆå¼Ÿå»ºè®®çš„é—­ç¯æœºåˆ¶
        """
        try:
            # æ›´æ–°episodeè®¡æ•°
            self.episode_count += 1
            
            # å‡†å¤‡å¸‚åœºçŠ¶æ€ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼Œ10ç»´ï¼‰
            market_state = np.zeros(10)
            if lookback_data.size > 0:
                # ä½¿ç”¨æœ€è¿‘çš„ä»·æ ¼å˜åŒ–ä½œä¸ºå¸‚åœºçŠ¶æ€ç‰¹å¾
                recent_data = lookback_data[-10:] if len(lookback_data) >= 10 else lookback_data
                market_state[:len(recent_data.flatten()[:10])] = recent_data.flatten()[:10]
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = {
                'mae': mae,
                'episode_count': self.episode_count,
                'market_volatility': np.std(lookback_data) if lookback_data.size > 0 else 0.0,
                'prediction_confidence': 1.0 / (1.0 + mae) if mae > 0 else 1.0
            }
            
            # å¤„ç†RLåé¦ˆ
            feedback_result = self.rl_feedback_system.process_episode_feedback(
                code=f"EATA_Episode_{self.episode_count}",
                reward=rl_reward,
                loss=mae,  # ä½¿ç”¨MAEä½œä¸ºloss
                market_state=market_state,
                action=trading_signal,
                context=context
            )
            
            # åº”ç”¨åé¦ˆåˆ°NEMoTSè¶…å‚æ•°ï¼ˆå¸ˆå¼Ÿå»ºè®®çš„æ ¸å¿ƒåŠŸèƒ½ï¼‰
            if 'loss_processing' in feedback_result and 'nemots' in feedback_result['loss_processing']:
                nemots_updates = feedback_result['loss_processing']['nemots']
                self._apply_nemots_feedback(nemots_updates)
            
            print(f"ğŸ”§ RLåé¦ˆå¤„ç†å®Œæˆ - Episode {self.episode_count}")
            print(f"   å‡€æ”¶ç›Š: {feedback_result.get('net_outcome', 0):.4f}")
            print(f"   é€‚åº”ç±»å‹: {feedback_result.get('system_adaptation', {}).get('type', 'unknown')}")
            
        except Exception as e:
            print(f"âš ï¸ RLåé¦ˆå¤„ç†å¤±è´¥: {e}")

    def _apply_nemots_feedback(self, nemots_updates: Dict[str, Any]):
        """
        åº”ç”¨RLåé¦ˆåˆ°NEMoTSè¶…å‚æ•° - å¸ˆå¼Ÿå»ºè®®çš„æ ¸å¿ƒåŠŸèƒ½
        """
        try:
            print(f"ğŸ¯ åº”ç”¨NEMoTSå‚æ•°è°ƒæ•´...")
            
            # åº”ç”¨æ¢ç´¢ç‡è°ƒæ•´
            if 'exploration_rate_multiplier' in nemots_updates:
                old_rate = self.hyperparams.exploration_rate
                self.hyperparams.exploration_rate *= nemots_updates['exploration_rate_multiplier']
                self.hyperparams.exploration_rate = np.clip(self.hyperparams.exploration_rate, 0.1, 2.0)
                print(f"   æ¢ç´¢ç‡: {old_rate:.3f} -> {self.hyperparams.exploration_rate:.3f}")
            
            # åº”ç”¨å­¦ä¹ ç‡è°ƒæ•´
            if 'learning_rate_multiplier' in nemots_updates:
                old_lr = self.hyperparams.lr
                self.hyperparams.lr *= nemots_updates['learning_rate_multiplier']
                self.hyperparams.lr = np.clip(self.hyperparams.lr, 1e-6, 1e-3)
                print(f"   å­¦ä¹ ç‡: {old_lr:.6f} -> {self.hyperparams.lr:.6f}")
            
            # åº”ç”¨è¿è¡Œæ¬¡æ•°è°ƒæ•´
            if 'num_runs_multiplier' in nemots_updates:
                old_runs = self.hyperparams.num_runs
                self.hyperparams.num_runs = int(self.hyperparams.num_runs * nemots_updates['num_runs_multiplier'])
                self.hyperparams.num_runs = np.clip(self.hyperparams.num_runs, 1, 10)
                print(f"   è¿è¡Œæ¬¡æ•°: {old_runs} -> {self.hyperparams.num_runs}")
            
            # æ›´æ–°å¼•æ“å‚æ•°
            if hasattr(self.engine, 'model'):
                self.engine.model.exploration_rate = self.hyperparams.exploration_rate
        
        except Exception as e:
            print(f"NEMoTSå‚æ•°è°ƒæ•´å¤±è´¥: {e}")

    def criteria(self, test_df: pd.DataFrame, shares_held: int = 0) -> Tuple[int, float]:
        """
        æ ¸å¿ƒå†³ç­–æ–¹æ³• - å¢å¼ºç‰ˆ
        """
        try:
            # ä½¿ç”¨æ»‘åŠ¨çª—å£NEMoTSè¿›è¡Œé¢„æµ‹
            from sliding_window_nemots import SlidingWindowNEMoTS
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å˜ä½“å‚æ•°éœ€è¦ä¼ é€’
            variant_kwargs = {}
            if hasattr(self, '_variant_alpha'):
                variant_kwargs['alpha'] = self._variant_alpha
            if hasattr(self, '_variant_num_transplant'):
                variant_kwargs['num_transplant'] = self._variant_num_transplant
            if hasattr(self, '_variant_num_aug'):
                variant_kwargs['num_aug'] = self._variant_num_aug
            if hasattr(self, '_variant_exploration_rate'):
                variant_kwargs['exploration_rate'] = self._variant_exploration_rate
            
            # ğŸ”§ æ–¹æ¡ˆ1ï¼šæ£€æŸ¥Engineä¸Šçš„ç›´æ¥ä¼ é€’å‚æ•°
            if hasattr(self.engine, '_variant_exploration_rate'):
                variant_kwargs['exploration_rate'] = self.engine._variant_exploration_rate
                print(f"ğŸ”§ [æ–¹æ¡ˆ1] ä»Engineè·å–exploration_rate: {self.engine._variant_exploration_rate}")
            
            nemots = SlidingWindowNEMoTS(
                lookback=self.lookback,
                lookahead=self.lookahead,
                stride=self.stride,
                depth=self.depth,
                previous_best_tree=getattr(self, '_previous_best_tree', None),
                external_engine=self.engine,
                **variant_kwargs
            )

            # å‡†å¤‡æ•°æ®
            full_window_data = self._prepare_data(test_df)
            lookback_data = full_window_data[:self.lookback, :]
            lookahead_data = full_window_data[-self.lookahead:, :]

            # è¿è¡ŒNEMoTS
            result = nemots.sliding_fit(test_df)
            
            # æå–ç»“æœ
            best_exp = result.get('best_expression', '0')
            top_10_exps = result.get('top_10_expressions', ['0'] * 10)
            mae = result.get('mae', 0.0)
            mcts_score = result.get('mcts_score', 0.0)
            new_best_tree = result.get('best_tree', None)

            # ä¿å­˜çŠ¶æ€
            self._previous_best_tree = new_best_tree
            self.is_trained = True
            
            record = {'mae': mae, 'mcts_score': mcts_score}
            self.training_history.append(record)
            print(f"NEMoTSè¿è¡Œå®Œæˆ: MAE={mae:.4f}, MCTS Score={mcts_score:.4f}")

            # ç”Ÿæˆé¢„æµ‹åˆ†å¸ƒ
            prediction_distribution = self._predict_distribution(top_10_exps, lookback_data)
            print(f"ç”Ÿæˆäº† {len(prediction_distribution)} ä¸ªé¢„æµ‹ç‚¹ã€‚")

            # è®¡ç®—RLå¥–åŠ±å’Œäº¤æ˜“ä¿¡å·
            rl_reward, trading_signal = self._calculate_rl_reward_and_signal(
                prediction_distribution, lookahead_data, shares_held
            )
            print(f"RLå¥–åŠ± (åŸºäºçœŸå®ä¿¡å·): {rl_reward:.4f}, æ„å›¾äº¤æ˜“ä¿¡å·: {trading_signal}")

            # RLåé¦ˆå¤„ç†
            self._process_rl_feedback(rl_reward, mae, trading_signal, lookback_data)

            return trading_signal, rl_reward

        except Exception as e:
            print(f"NEMoTS Agent 'criteria' å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0, 0

    # choose_action, vote, strength æ–¹æ³•ä¿æŒä¸å˜
    @classmethod
    def choose_action(cls, s: tuple) -> int:
        try:
            _, s1, _, _ = s
            temp_agent = Agent(pd.DataFrame())
            action, _ = temp_agent.criteria(s1, shares_held=0)
            return action
        except Exception as e:
            print(f"åŠ¨ä½œé€‰æ‹©å¤±è´¥: {e}")
            return 0

    def vote(self) -> int:
        print("'vote' æ–¹æ³•è¢«ç®€åŒ–ï¼Œä»…è¿”å›ä¸­æ€§ä¿¡å·ã€‚è¯·åœ¨ predict.py ä¸­å®ç°å¤šè‚¡ç¥¨å¾ªç¯ã€‚")
        return 50

    def strength(self, w1: float, w2: float, w3: float, w4: float) -> pd.Series:
        print("'strength' æ–¹æ³•è¢«ç®€åŒ–ï¼Œè¿”å›å›ºå®šå€¼ã€‚")
        self.stock_list['strength'] = [50] * len(self.stock_list)
        return self.stock_list['strength']

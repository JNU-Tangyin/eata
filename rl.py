#!/usr/bin/env python
# coding=utf-8
"""
å¼ºåŒ–å­¦ä¹ åé¦ˆç³»ç»Ÿ
è´Ÿè´£å¤„ç†rewardåˆ©ç”¨å’Œlossåé¦ˆæœºåˆ¶
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import pickle
import os

class RewardUtilizationSystem:
    """
    Rewardåˆ©ç”¨ç³»ç»Ÿ
    è´Ÿè´£å°†rewardè½¬åŒ–ä¸ºå¯¹æ¨¡å‹çš„å…·ä½“æ”¹è¿›
    """
    
    def __init__(self, learning_rate: float = 0.001, memory_size: int = 1000):
        """
        åˆå§‹åŒ–rewardåˆ©ç”¨ç³»ç»Ÿ
        
        Args:
            learning_rate: å­¦ä¹ ç‡
            memory_size: ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
        """
        self.learning_rate = learning_rate
        self.memory_size = memory_size
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.experience_buffer = deque(maxlen=memory_size)
        
        # ç­–ç•¥ç½‘ç»œï¼ˆç®€åŒ–çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºå­¦ä¹ æœ€ä¼˜ç­–ç•¥ï¼‰
        self.policy_net = self._build_policy_network()
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        
        # å¥–åŠ±å†å²ç»Ÿè®¡
        self.reward_stats = {
            'total_rewards': 0.0,
            'episode_count': 0,
            'best_reward': float('-inf'),
            'recent_rewards': deque(maxlen=100)
        }
        
        print(f"ğŸ¯ Rewardåˆ©ç”¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   å­¦ä¹ ç‡: {learning_rate}")
        print(f"   ç»éªŒç¼“å†²åŒºå¤§å°: {memory_size}")
    
    def _build_policy_network(self) -> nn.Module:
        """æ„å»ºç­–ç•¥ç½‘ç»œ"""
        class PolicyNetwork(nn.Module):
            def __init__(self):
                super(PolicyNetwork, self).__init__()
                # è¾“å…¥: å¸‚åœºçŠ¶æ€ç‰¹å¾ (å‡è®¾10ç»´)
                # è¾“å‡º: åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ (3ç»´: ä¹°å…¥/æŒæœ‰/å–å‡º)
                self.fc1 = nn.Linear(10, 64)
                self.fc2 = nn.Linear(64, 32)
                self.fc3 = nn.Linear(32, 3)
                self.softmax = nn.Softmax(dim=-1)
                
            def forward(self, x):
                x = torch.relu(self.fc1(x))
                x = torch.relu(self.fc2(x))
                x = self.fc3(x)
                return self.softmax(x)
        
        return PolicyNetwork()
    
    def store_experience(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray):
        """
        å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº
        
        Args:
            state: å½“å‰çŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            reward: è·å¾—çš„å¥–åŠ±
            next_state: ä¸‹ä¸€ä¸ªçŠ¶æ€
        """
        experience = {
            'state': state,
            'action': action,
            'reward': reward,
            'next_state': next_state,
            'timestamp': pd.Timestamp.now()
        }
        self.experience_buffer.append(experience)
        
        # æ›´æ–°å¥–åŠ±ç»Ÿè®¡
        self.reward_stats['total_rewards'] += reward
        self.reward_stats['episode_count'] += 1
        self.reward_stats['recent_rewards'].append(reward)
        
        if reward > self.reward_stats['best_reward']:
            self.reward_stats['best_reward'] = reward
            print(f"ğŸ† æ–°çš„æœ€ä½³å¥–åŠ±: {reward:.4f}")
    
    def utilize_reward(self, code: str, reward: float, loss: float, 
                      market_state: np.ndarray, action: int) -> Dict[str, Any]:
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šåˆ©ç”¨rewardæ”¹è¿›ç­–ç•¥
        
        Args:
            code: èµ„äº§ä»£ç 
            reward: è·å¾—çš„å¥–åŠ±
            loss: æŸå¤±å€¼
            market_state: å¸‚åœºçŠ¶æ€ç‰¹å¾
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            
        Returns:
            ç­–ç•¥æ›´æ–°ç»“æœ
        """
        print(f"ğŸ¯ åˆ©ç”¨rewardæ”¹è¿›ç­–ç•¥: {code}")
        print(f"   Reward: {reward:.4f}, Loss: {loss:.4f}, Action: {action}")
        
        # 1. å­˜å‚¨ç»éªŒ
        # è¿™é‡Œç®€åŒ–next_stateä¸ºå½“å‰stateï¼ˆå®é™…åº”è¯¥æ˜¯ä¸‹ä¸€æ—¶åˆ»çš„çŠ¶æ€ï¼‰
        self.store_experience(market_state, action, reward, market_state)
        
        # 2. è®¡ç®—ç­–ç•¥æ¢¯åº¦
        net_reward = reward - loss  # å‡€å¥–åŠ±
        
        # 3. æ ¹æ®rewardè°ƒæ•´ç­–ç•¥
        update_result = {}
        
        if net_reward > 0:
            # æ­£å¥–åŠ±ï¼šå¢å¼ºå½“å‰ç­–ç•¥
            update_result = self._reinforce_strategy(market_state, action, net_reward)
            print(f"   âœ… æ­£å¥–åŠ± {net_reward:.4f}: å¢å¼ºç­–ç•¥")
        elif net_reward < 0:
            # è´Ÿå¥–åŠ±ï¼šæƒ©ç½šå½“å‰ç­–ç•¥
            update_result = self._penalize_strategy(market_state, action, abs(net_reward))
            print(f"   âŒ è´Ÿå¥–åŠ± {net_reward:.4f}: æƒ©ç½šç­–ç•¥")
        else:
            # é›¶å¥–åŠ±ï¼šä¿æŒå½“å‰ç­–ç•¥
            update_result = {'action': 'maintain', 'adjustment': 0.0}
            print(f"   â– é›¶å¥–åŠ±: ä¿æŒç­–ç•¥")
        
        # 4. æ‰§è¡Œç­–ç•¥ç½‘ç»œæ›´æ–°ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿç»éªŒï¼‰
        if len(self.experience_buffer) >= 32:  # æ‰¹é‡å¤§å°
            network_update = self._update_policy_network()
            update_result['network_update'] = network_update
        
        return update_result
    
    def _reinforce_strategy(self, state: np.ndarray, action: int, reward: float) -> Dict[str, Any]:
        """å¢å¼ºç­–ç•¥ï¼ˆæ­£å¥–åŠ±æ—¶è°ƒç”¨ï¼‰"""
        # å¢åŠ è¯¥åŠ¨ä½œåœ¨è¯¥çŠ¶æ€ä¸‹çš„æ¦‚ç‡
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action_probs = self.policy_net(state_tensor)
            current_prob = action_probs[0, action].item()
        
        # è®¡ç®—å¢å¼ºå¹…åº¦ï¼ˆåŸºäºå¥–åŠ±å¤§å°ï¼‰
        enhancement = min(0.1, reward * 0.05)  # æœ€å¤§å¢å¼º10%
        
        return {
            'action': 'reinforce',
            'target_action': action,
            'current_prob': current_prob,
            'enhancement': enhancement,
            'reward': reward
        }
    
    def _penalize_strategy(self, state: np.ndarray, action: int, penalty: float) -> Dict[str, Any]:
        """æƒ©ç½šç­–ç•¥ï¼ˆè´Ÿå¥–åŠ±æ—¶è°ƒç”¨ï¼‰"""
        # é™ä½è¯¥åŠ¨ä½œåœ¨è¯¥çŠ¶æ€ä¸‹çš„æ¦‚ç‡
        state_tensor = torch.FloatTensor(state).unsqueeze(0)
        
        with torch.no_grad():
            action_probs = self.policy_net(state_tensor)
            current_prob = action_probs[0, action].item()
        
        # è®¡ç®—æƒ©ç½šå¹…åº¦
        penalty_amount = min(0.1, penalty * 0.05)  # æœ€å¤§æƒ©ç½š10%
        
        return {
            'action': 'penalize',
            'target_action': action,
            'current_prob': current_prob,
            'penalty': penalty_amount,
            'loss': penalty
        }
    
    def _update_policy_network(self) -> Dict[str, Any]:
        """æ›´æ–°ç­–ç•¥ç½‘ç»œ"""
        if len(self.experience_buffer) < 32:
            return {'updated': False, 'reason': 'insufficient_data'}
        
        # é‡‡æ ·æ‰¹é‡ç»éªŒ
        batch_size = min(32, len(self.experience_buffer))
        batch = np.random.choice(list(self.experience_buffer), batch_size, replace=False)
        
        states = torch.FloatTensor([exp['state'] for exp in batch])
        actions = torch.LongTensor([exp['action'] for exp in batch])
        rewards = torch.FloatTensor([exp['reward'] for exp in batch])
        
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦
        self.optimizer.zero_grad()
        
        action_probs = self.policy_net(states)
        selected_probs = action_probs.gather(1, actions.unsqueeze(1)).squeeze()
        
        # ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼ˆREINFORCEç®—æ³•ï¼‰
        loss = -torch.mean(torch.log(selected_probs) * rewards)
        
        loss.backward()
        self.optimizer.step()
        
        return {
            'updated': True,
            'loss': loss.item(),
            'batch_size': batch_size,
            'avg_reward': rewards.mean().item()
        }
    
    def get_reward_statistics(self) -> Dict[str, Any]:
        """è·å–å¥–åŠ±ç»Ÿè®¡ä¿¡æ¯"""
        recent_avg = np.mean(list(self.reward_stats['recent_rewards'])) if self.reward_stats['recent_rewards'] else 0.0
        
        return {
            'total_rewards': self.reward_stats['total_rewards'],
            'episode_count': self.reward_stats['episode_count'],
            'average_reward': self.reward_stats['total_rewards'] / max(1, self.reward_stats['episode_count']),
            'best_reward': self.reward_stats['best_reward'],
            'recent_average': recent_avg,
            'experience_buffer_size': len(self.experience_buffer)
        }


class LossFeedbackSystem:
    """
    Lossåé¦ˆç³»ç»Ÿ
    è´Ÿè´£å°†lossåé¦ˆåˆ°æ™ºèƒ½ä½“çš„å…·ä½“ä½ç½®å’Œæ–¹æ³•
    """
    
    def __init__(self, feedback_threshold: float = 0.01, adaptation_rate: float = 0.1):
        """
        åˆå§‹åŒ–lossåé¦ˆç³»ç»Ÿ
        
        Args:
            feedback_threshold: åé¦ˆé˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼æ‰è§¦å‘åé¦ˆ
            adaptation_rate: é€‚åº”é€Ÿç‡
        """
        self.feedback_threshold = feedback_threshold
        self.adaptation_rate = adaptation_rate
        
        # Losså†å²è®°å½•
        self.loss_history = deque(maxlen=1000)
        
        # åé¦ˆç›®æ ‡ç»„ä»¶
        self.feedback_targets = {
            'nemots_hyperparams': True,    # NEMoTSè¶…å‚æ•°
            'agent_weights': True,         # Agentæƒé‡
            'prediction_confidence': True, # é¢„æµ‹ç½®ä¿¡åº¦
            'risk_management': True        # é£é™©ç®¡ç†å‚æ•°
        }
        
        print(f"ğŸ”„ Lossåé¦ˆç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"   åé¦ˆé˜ˆå€¼: {feedback_threshold}")
        print(f"   é€‚åº”é€Ÿç‡: {adaptation_rate}")
    
    def process_loss_feedback(self, code: str, loss: float, loss_source: str, 
                            context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†lossåé¦ˆçš„æ ¸å¿ƒæ–¹æ³•
        
        Args:
            code: èµ„äº§ä»£ç 
            loss: æŸå¤±å€¼
            loss_source: æŸå¤±æ¥æº ('prediction', 'trading', 'risk')
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            åé¦ˆå¤„ç†ç»“æœ
        """
        print(f"ğŸ”„ å¤„ç†Lossåé¦ˆ: {code}")
        print(f"   Loss: {loss:.4f}, æ¥æº: {loss_source}")
        
        # è®°å½•losså†å²
        loss_record = {
            'code': code,
            'loss': loss,
            'source': loss_source,
            'context': context,
            'timestamp': pd.Timestamp.now()
        }
        self.loss_history.append(loss_record)
        
        feedback_results = {}
        
        # åªæœ‰å½“lossè¶…è¿‡é˜ˆå€¼æ—¶æ‰è¿›è¡Œåé¦ˆ
        if loss > self.feedback_threshold:
            print(f"   âš ï¸ Lossè¶…è¿‡é˜ˆå€¼ {self.feedback_threshold}ï¼Œè§¦å‘åé¦ˆæœºåˆ¶")
            
            # 1. åé¦ˆåˆ°NEMoTSè¶…å‚æ•°
            if self.feedback_targets['nemots_hyperparams']:
                nemots_feedback = self._feedback_to_nemots(loss, loss_source, context)
                feedback_results['nemots'] = nemots_feedback
            
            # 2. åé¦ˆåˆ°Agentæƒé‡
            if self.feedback_targets['agent_weights']:
                agent_feedback = self._feedback_to_agent(loss, loss_source, context)
                feedback_results['agent'] = agent_feedback
            
            # 3. åé¦ˆåˆ°é¢„æµ‹ç½®ä¿¡åº¦
            if self.feedback_targets['prediction_confidence']:
                confidence_feedback = self._feedback_to_prediction_confidence(loss, loss_source, context)
                feedback_results['confidence'] = confidence_feedback
            
            # 4. åé¦ˆåˆ°é£é™©ç®¡ç†
            if self.feedback_targets['risk_management']:
                risk_feedback = self._feedback_to_risk_management(loss, loss_source, context)
                feedback_results['risk'] = risk_feedback
        else:
            print(f"   âœ… Lossåœ¨é˜ˆå€¼å†…ï¼Œæ— éœ€åé¦ˆ")
            feedback_results['action'] = 'no_feedback_needed'
        
        return feedback_results
    
    def _feedback_to_nemots(self, loss: float, source: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """åé¦ˆåˆ°NEMoTSæ¨¡å‹å‚æ•°"""
        print(f"   ğŸ§  åé¦ˆåˆ°NEMoTSæ¨¡å‹")
        
        # æ ¹æ®lossè°ƒæ•´NEMoTSè¶…å‚æ•°
        adjustments = {}
        
        if source == 'prediction':
            # é¢„æµ‹lossé«˜ -> é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ­£åˆ™åŒ–
            lr_adjustment = -self.adaptation_rate * loss
            reg_adjustment = self.adaptation_rate * loss * 0.5
            
            adjustments = {
                'learning_rate_multiplier': 1 + lr_adjustment,
                'regularization_multiplier': 1 + reg_adjustment,
                'exploration_rate_multiplier': 1 + self.adaptation_rate * loss,  # å¢åŠ æ¢ç´¢
                'reason': f'é¢„æµ‹loss={loss:.4f}è¿‡é«˜ï¼Œé™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ æ¢ç´¢'
            }
        
        elif source == 'trading':
            # äº¤æ˜“lossé«˜ -> è°ƒæ•´æ¨¡å‹å¤æ‚åº¦
            complexity_adjustment = -self.adaptation_rate * loss
            
            adjustments = {
                'max_len_multiplier': 1 + complexity_adjustment,  # é™ä½æ¨¡å‹å¤æ‚åº¦
                'num_runs_multiplier': 1 + self.adaptation_rate * loss,  # å¢åŠ è¿è¡Œæ¬¡æ•°
                'reason': f'äº¤æ˜“loss={loss:.4f}è¿‡é«˜ï¼Œé™ä½æ¨¡å‹å¤æ‚åº¦'
            }
        
        return adjustments
    
    def _feedback_to_agent(self, loss: float, source: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """åé¦ˆåˆ°Agentæƒé‡ç³»ç»Ÿ"""
        print(f"   ğŸ¤– åé¦ˆåˆ°Agentæƒé‡")
        
        # è°ƒæ•´Agentçš„strengthè®¡ç®—æƒé‡
        weight_adjustments = {}
        
        if loss > 0.05:  # é«˜loss
            # é™ä½å½“å‰ç­–ç•¥çš„æƒé‡ï¼Œå¢åŠ ä¿å®ˆæ€§
            weight_adjustments = {
                'stock_strength_weight': -self.adaptation_rate * loss,
                'market_strength_weight': self.adaptation_rate * loss * 0.5,  # å¢åŠ å¸‚åœºæƒé‡
                'risk_aversion_multiplier': 1 + self.adaptation_rate * loss,
                'reason': f'Loss={loss:.4f}è¿‡é«˜ï¼Œå¢åŠ ä¿å®ˆæ€§'
            }
        elif loss > 0.02:  # ä¸­ç­‰loss
            # å¾®è°ƒæƒé‡
            weight_adjustments = {
                'stock_strength_weight': -self.adaptation_rate * loss * 0.5,
                'sector_strength_weight': self.adaptation_rate * loss * 0.3,
                'reason': f'Loss={loss:.4f}ä¸­ç­‰ï¼Œå¾®è°ƒæƒé‡'
            }
        
        return weight_adjustments
    
    def _feedback_to_prediction_confidence(self, loss: float, source: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """åé¦ˆåˆ°é¢„æµ‹ç½®ä¿¡åº¦ç³»ç»Ÿ"""
        print(f"   ğŸ¯ åé¦ˆåˆ°é¢„æµ‹ç½®ä¿¡åº¦")
        
        # æ ¹æ®lossè°ƒæ•´é¢„æµ‹ç½®ä¿¡åº¦é˜ˆå€¼
        confidence_adjustments = {
            'confidence_threshold_adjustment': -self.adaptation_rate * loss,  # é™ä½ç½®ä¿¡åº¦é˜ˆå€¼
            'uncertainty_penalty_multiplier': 1 + self.adaptation_rate * loss,  # å¢åŠ ä¸ç¡®å®šæ€§æƒ©ç½š
            'prediction_weight_decay': self.adaptation_rate * loss * 0.1,  # é¢„æµ‹æƒé‡è¡°å‡
            'reason': f'Loss={loss:.4f}ï¼Œé™ä½é¢„æµ‹ç½®ä¿¡åº¦'
        }
        
        return confidence_adjustments
    
    def _feedback_to_risk_management(self, loss: float, source: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """åé¦ˆåˆ°é£é™©ç®¡ç†ç³»ç»Ÿ"""
        print(f"   âš ï¸ åé¦ˆåˆ°é£é™©ç®¡ç†")
        
        # æ ¹æ®lossè°ƒæ•´é£é™©ç®¡ç†å‚æ•°
        risk_adjustments = {
            'stop_loss_tightening': self.adaptation_rate * loss,  # æ”¶ç´§æ­¢æŸ
            'position_size_reduction': self.adaptation_rate * loss * 0.5,  # å‡å°‘ä»“ä½
            'volatility_threshold_adjustment': -self.adaptation_rate * loss,  # é™ä½æ³¢åŠ¨ç‡é˜ˆå€¼
            'max_drawdown_limit_tightening': self.adaptation_rate * loss * 0.3,  # æ”¶ç´§æœ€å¤§å›æ’¤é™åˆ¶
            'reason': f'Loss={loss:.4f}ï¼Œæ”¶ç´§é£é™©æ§åˆ¶'
        }
        
        return risk_adjustments
    
    def get_loss_statistics(self) -> Dict[str, Any]:
        """è·å–lossç»Ÿè®¡ä¿¡æ¯"""
        if not self.loss_history:
            return {'no_data': True}
        
        losses = [record['loss'] for record in self.loss_history]
        recent_losses = losses[-100:] if len(losses) > 100 else losses
        
        return {
            'total_loss_events': len(self.loss_history),
            'average_loss': np.mean(losses),
            'max_loss': np.max(losses),
            'recent_average_loss': np.mean(recent_losses),
            'loss_trend': 'increasing' if len(recent_losses) > 10 and recent_losses[-5:] > recent_losses[:5] else 'stable',
            'feedback_trigger_rate': len([l for l in losses if l > self.feedback_threshold]) / len(losses)
        }


class IntegratedRLFeedbackSystem:
    """
    é›†æˆçš„RLåé¦ˆç³»ç»Ÿ
    æ•´åˆrewardåˆ©ç”¨å’Œlossåé¦ˆ
    """
    
    def __init__(self):
        """åˆå§‹åŒ–é›†æˆåé¦ˆç³»ç»Ÿ"""
        self.reward_system = RewardUtilizationSystem()
        self.loss_system = LossFeedbackSystem()
        
        # ç³»ç»ŸçŠ¶æ€
        self.system_state = {
            'total_episodes': 0,
            'successful_episodes': 0,
            'failed_episodes': 0,
            'adaptation_history': []
        }
        
        print(f"ğŸ”§ é›†æˆRLåé¦ˆç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def process_episode_feedback(self, code: str, reward: float, loss: float, 
                               market_state: np.ndarray, action: int, 
                               context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªepisodeçš„å®Œæ•´åé¦ˆ
        
        Args:
            code: èµ„äº§ä»£ç 
            reward: å¥–åŠ±å€¼
            loss: æŸå¤±å€¼
            market_state: å¸‚åœºçŠ¶æ€
            action: æ‰§è¡Œçš„åŠ¨ä½œ
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            å®Œæ•´çš„åé¦ˆå¤„ç†ç»“æœ
        """
        print(f"\nğŸ”§ å¤„ç†Episodeåé¦ˆ: {code}")
        print(f"   Reward: {reward:.4f}, Loss: {loss:.4f}, Action: {action}")
        
        # æ›´æ–°ç³»ç»ŸçŠ¶æ€
        self.system_state['total_episodes'] += 1
        if reward > loss:
            self.system_state['successful_episodes'] += 1
        else:
            self.system_state['failed_episodes'] += 1
        
        # 1. å¤„ç†rewardåˆ©ç”¨
        reward_result = self.reward_system.utilize_reward(
            code, reward, loss, market_state, action
        )
        
        # 2. å¤„ç†lossåé¦ˆ
        loss_result = self.loss_system.process_loss_feedback(
            code, loss, 'trading', context
        )
        
        # 3. æ•´åˆç»“æœ
        integrated_result = {
            'episode_id': self.system_state['total_episodes'],
            'code': code,
            'reward_processing': reward_result,
            'loss_processing': loss_result,
            'net_outcome': reward - loss,
            'system_adaptation': self._calculate_system_adaptation(reward, loss),
            'timestamp': pd.Timestamp.now()
        }
        
        # è®°å½•é€‚åº”å†å²
        self.system_state['adaptation_history'].append(integrated_result)
        
        return integrated_result
    
    def _calculate_system_adaptation(self, reward: float, loss: float) -> Dict[str, Any]:
        """è®¡ç®—ç³»ç»Ÿé€‚åº”æ€§è°ƒæ•´"""
        net_outcome = reward - loss
        
        if net_outcome > 0.02:  # æ˜¾è‘—æ­£æ”¶ç›Š
            adaptation = {
                'type': 'positive_reinforcement',
                'strength': min(1.0, net_outcome * 10),
                'action': 'enhance_current_strategy'
            }
        elif net_outcome < -0.02:  # æ˜¾è‘—è´Ÿæ”¶ç›Š
            adaptation = {
                'type': 'negative_feedback',
                'strength': min(1.0, abs(net_outcome) * 10),
                'action': 'adjust_strategy_parameters'
            }
        else:  # ä¸­æ€§ç»“æœ
            adaptation = {
                'type': 'neutral',
                'strength': 0.0,
                'action': 'maintain_current_strategy'
            }
        
        return adaptation
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        reward_stats = self.reward_system.get_reward_statistics()
        loss_stats = self.loss_system.get_loss_statistics()
        
        success_rate = (self.system_state['successful_episodes'] / 
                       max(1, self.system_state['total_episodes']))
        
        return {
            'system_state': self.system_state,
            'success_rate': success_rate,
            'reward_statistics': reward_stats,
            'loss_statistics': loss_stats,
            'adaptation_count': len(self.system_state['adaptation_history'])
        }
    
    def save_system_state(self, filepath: str):
        """ä¿å­˜ç³»ç»ŸçŠ¶æ€"""
        state_data = {
            'system_state': self.system_state,
            'reward_stats': self.reward_system.get_reward_statistics(),
            'loss_stats': self.loss_system.get_loss_statistics()
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(state_data, f)
        
        print(f"ğŸ’¾ ç³»ç»ŸçŠ¶æ€å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_system_state(self, filepath: str):
        """åŠ è½½ç³»ç»ŸçŠ¶æ€"""
        if os.path.exists(filepath):
            with open(filepath, 'rb') as f:
                state_data = pickle.load(f)
            
            self.system_state = state_data['system_state']
            print(f"ğŸ“‚ ç³»ç»ŸçŠ¶æ€å·²ä» {filepath} åŠ è½½")
        else:
            print(f"âš ï¸ çŠ¶æ€æ–‡ä»¶ {filepath} ä¸å­˜åœ¨")


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸ”§ RLåé¦ˆç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºé›†æˆåé¦ˆç³»ç»Ÿ
    feedback_system = IntegratedRLFeedbackSystem()
    
    # æ¨¡æ‹Ÿå¸‚åœºçŠ¶æ€ï¼ˆ10ç»´ç‰¹å¾ï¼‰
    market_state = np.random.randn(10)
    
    # æ¨¡æ‹Ÿå‡ ä¸ªepisodeçš„åé¦ˆ
    for i in range(5):
        code = f"sh.60000{i}"
        reward = np.random.uniform(0, 0.1)  # 0-10%çš„å¥–åŠ±
        loss = np.random.uniform(0, 0.05)   # 0-5%çš„æŸå¤±
        action = np.random.choice([0, 1, 2])  # éšæœºåŠ¨ä½œ
        
        context = {
            'market_volatility': np.random.uniform(0.01, 0.05),
            'trading_volume': np.random.uniform(1000, 10000),
            'prediction_confidence': np.random.uniform(0.5, 0.9)
        }
        
        # å¤„ç†åé¦ˆ
        result = feedback_system.process_episode_feedback(
            code, reward, loss, market_state, action, context
        )
        
        print(f"\nEpisode {i+1} å¤„ç†å®Œæˆ:")
        print(f"  å‡€æ”¶ç›Š: {result['net_outcome']:.4f}")
        print(f"  é€‚åº”ç±»å‹: {result['system_adaptation']['type']}")
    
    # æ‰“å°ç³»ç»Ÿç»Ÿè®¡
    stats = feedback_system.get_system_statistics()
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡:")
    print(f"  æ€»Episodes: {stats['system_state']['total_episodes']}")
    print(f"  æˆåŠŸç‡: {stats['success_rate']:.2%}")
    print(f"  å¹³å‡å¥–åŠ±: {stats['reward_statistics']['average_reward']:.4f}")


if __name__ == "__main__":
    example_usage()

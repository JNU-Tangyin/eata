"""
EATA-Simple: ç®€å•å¥–åŠ±å˜ä½“
æ›¿æ¢å¤æ‚çš„Wassersteinè·ç¦»ä¸ºç®€å•æ”¶ç›Šå·®
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

from core.agent import Agent
import pandas as pd
import numpy as np

class EATASimple:
    """
    ç®€å•å¥–åŠ±çš„EATAå˜ä½“
    é€šè¿‡æ›¿æ¢Wassersteinè·ç¦»ä¸ºç®€å•MAEæµ‹è¯•å¤æ‚å¥–åŠ±æœºåˆ¶çš„ä»·å€¼
    """
    
    def __init__(self, df: pd.DataFrame, **kwargs):
        """
        åˆå§‹åŒ–ç®€å•å¥–åŠ±çš„EATAæ¨¡å‹
        """
        self.name = "EATA-Simple"
        self.description = "ç®€å•å¥–åŠ± - æ›¿æ¢å¤æ‚çš„Wassersteinè·ç¦»ä¸ºç®€å•æ”¶ç›Šå·®"
        
        # åˆ›å»ºAgentå®ä¾‹
        self.agent = Agent(
            df=df,
            lookback=kwargs.get('lookback', 100),
            lookahead=kwargs.get('lookahead', 20),
            stride=kwargs.get('stride', 1),
            depth=kwargs.get('depth', 300)
        )
        
        # åº”ç”¨æ¶ˆèä¿®æ”¹
        self._apply_modifications()
        
        self.modifications = {
            'reward_function': 'simple_mae',
            'target_file': 'agent.py',
            'target_line': 167,
            'modification_type': 'function_replacement'
        }
        
    def _apply_modifications(self):
        """
        åº”ç”¨æ¶ˆèä¿®æ”¹ï¼šæ›¿æ¢å¤æ‚å¥–åŠ±å‡½æ•°ä¸ºç®€å•MAE
        """
        try:
            # é‡å†™Agentçš„å¥–åŠ±è®¡ç®—æ–¹æ³•
            original_method = self.agent._calculate_rl_reward_and_signal
            
            def simple_reward_calculation(prediction_distribution, lookahead_ground_truth, shares_held):
                """
                ç®€å•å¥–åŠ±è®¡ç®—ï¼šä½¿ç”¨MAEæ›¿ä»£Wassersteinè·ç¦»
                ç§‘å­¦åˆç†çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œä¿æŒä¸åŸå§‹æ–¹æ³•çš„å¯æ¯”æ€§
                """
                try:
                    if prediction_distribution.size == 0:
                        return 0.0, 0

                    # äº¤æ˜“ä¿¡å·å†³ç­–ï¼ˆå®Œå…¨å¤åˆ¶åŸå§‹é€»è¾‘ä»¥ä¿æŒå¯æ¯”æ€§ï¼‰
                    strategy = [25, 75]
                    q_low, q_high = np.percentile(prediction_distribution, strategy)
                    
                    print(f"  [ç®€å•è°ƒè¯•] é¢„æµ‹åˆ†å¸ƒ: min={prediction_distribution.min():.6f}, max={prediction_distribution.max():.6f}")
                    print(f"  [ç®€å•è°ƒè¯•] Q25={q_low:.6f}, Q75={q_high:.6f}, median={np.median(prediction_distribution):.6f}")
                    
                    intended_signal = 0
                    if q_low > 0:
                        intended_signal = 1
                        print(f"  [ç®€å•å†³ç­–] é¢„æµ‹åˆ†å¸ƒçš„ 25% åˆ†ä½æ•° > 0ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: ä¹°å…¥")
                    elif q_high < 0:
                        intended_signal = -1
                        print(f"  [ç®€å•å†³ç­–] é¢„æµ‹åˆ†å¸ƒçš„ 75% åˆ†ä½æ•° < 0ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: å–å‡º")
                    else:
                        if prediction_distribution.min() >= 0:
                            median_val = np.median(prediction_distribution)
                            threshold = (prediction_distribution.max() - prediction_distribution.min()) * 0.3
                            if median_val > threshold:
                                intended_signal = 1
                                print(f"  [ç®€å•å†³ç­–] å…¨æ­£åˆ†å¸ƒï¼Œä¸­ä½æ•°{median_val:.6f} > é˜ˆå€¼{threshold:.6f}ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: ä¹°å…¥")
                            else:
                                print(f"  [ç®€å•å†³ç­–] å…¨æ­£åˆ†å¸ƒï¼Œä¸­ä½æ•°{median_val:.6f} <= é˜ˆå€¼{threshold:.6f}ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: æŒæœ‰")
                        else:
                            print("  [ç®€å•å†³ç­–] é¢„æµ‹åˆ†å¸ƒè·¨è¶Šé›¶ç‚¹ï¼Œä¿¡å·ä¸æ˜ç¡®ï¼Œç”Ÿæˆæ„å›¾ä¿¡å·: æŒæœ‰")

                    # RLå¥–åŠ±è®¡ç®— - å…³é”®ä¿®å¤ï¼šæ­£ç¡®æå–çœŸå®æ”¶ç›Šæ•°æ®
                    actual_returns = lookahead_ground_truth.T[3, :]  # ä¸åŸå§‹æ–¹æ³•å®Œå…¨ä¸€è‡´
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥è¾“å…¥æ•°æ®
                    print(f"  [ç®€å•RLè°ƒè¯•] é¢„æµ‹åˆ†å¸ƒå½¢çŠ¶: {prediction_distribution.shape}, èŒƒå›´: [{prediction_distribution.min():.6f}, {prediction_distribution.max():.6f}]")
                    print(f"  [ç®€å•RLè°ƒè¯•] çœŸå®æ”¶ç›Šå½¢çŠ¶: {actual_returns.shape}, èŒƒå›´: [{actual_returns.min():.6f}, {actual_returns.max():.6f}]")
                    
                    # æ£€æŸ¥è¾“å…¥æ•°æ®æœ‰æ•ˆæ€§
                    if len(prediction_distribution) == 0 or len(actual_returns) == 0:
                        print(f"  âš ï¸ ç©ºçš„è¾“å…¥æ•°æ®ï¼Œè¿”å›é»˜è®¤RLå¥–åŠ±0.0")
                        return 0.0, intended_signal
                        
                    if np.all(np.isnan(prediction_distribution)) or np.all(np.isnan(actual_returns)):
                        print(f"  âš ï¸ è¾“å…¥æ•°æ®å…¨ä¸ºnanï¼Œè¿”å›é»˜è®¤RLå¥–åŠ±0.0")
                        return 0.0, intended_signal
                    
                    # ç®€åŒ–çš„è·ç¦»è®¡ç®—ï¼šä½¿ç”¨MAEæ›¿ä»£Wassersteinè·ç¦»
                    # è¿™æ˜¯ç»Ÿè®¡å­¦ä¸Šåˆç†çš„ç®€åŒ–ï¼ŒMAEæ˜¯L1è·ç¦»ï¼Œæ¯”Wassersteinè·ç¦»è®¡ç®—ç®€å•ä½†ä¿æŒäº†åˆ†å¸ƒæ¯”è¾ƒçš„æœ¬è´¨
                    simple_distance = np.mean(np.abs(prediction_distribution - np.mean(actual_returns)))
                    print(f"  [ç®€å•RLè°ƒè¯•] MAEè·ç¦»: {simple_distance}")
                    
                    # å¤„ç†å¼‚å¸¸çš„è·ç¦»å€¼
                    if np.isnan(simple_distance) or np.isinf(simple_distance):
                        print(f"  âš ï¸ å¼‚å¸¸çš„MAEè·ç¦»: {simple_distance}")
                        print(f"  [ç®€å•è¯Šæ–­] é¢„æµ‹åˆ†å¸ƒç»Ÿè®¡: mean={np.mean(prediction_distribution):.6f}, std={np.std(prediction_distribution):.6f}")
                        print(f"  [ç®€å•è¯Šæ–­] çœŸå®æ”¶ç›Šç»Ÿè®¡: mean={np.mean(actual_returns):.6f}, std={np.std(actual_returns):.6f}")
                        return 0.0, intended_signal
                    elif simple_distance < 0:
                        print(f"  âš ï¸ è´Ÿçš„MAEè·ç¦»: {simple_distance}ï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ")
                        return 0.0, intended_signal
                    
                    rl_reward = 1 / (1 + simple_distance)
                    print(f"  [ç®€å•RLè°ƒè¯•] è®¡ç®—çš„RLå¥–åŠ±: {rl_reward:.6f}")
                    
                    # æœ€ç»ˆæ£€æŸ¥
                    if np.isnan(rl_reward) or np.isinf(rl_reward):
                        print(f"  âš ï¸ æœ€ç»ˆRLå¥–åŠ±å¼‚å¸¸: {rl_reward}ï¼Œè¿”å›0.0")
                        rl_reward = 0.0
                    
                    return rl_reward, intended_signal
                    
                except Exception as e:
                    print(f"--- ğŸš¨ ç®€å•å¥–åŠ±è®¡ç®—ä¸­æ•è·åˆ°é”™è¯¯ ğŸš¨ ---")
                    print(f"é”™è¯¯ä¿¡æ¯: {e}")
                    print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
                    import traceback
                    print(f"å®Œæ•´é”™è¯¯å †æ ˆ:")
                    traceback.print_exc()
                    print(f"é¢„æµ‹åˆ†å¸ƒç±»å‹: {type(prediction_distribution)}, å½¢çŠ¶: {getattr(prediction_distribution, 'shape', 'N/A')}")
                    print(f"çœŸå®æ•°æ®ç±»å‹: {type(lookahead_ground_truth)}, å½¢çŠ¶: {getattr(lookahead_ground_truth, 'shape', 'N/A')}")
                    return 0.0, 0
            
            # æ›¿æ¢åŸæ–¹æ³•
            self.agent._calculate_rl_reward_and_signal = simple_reward_calculation
            print(f"{self.name}: å·²æ›¿æ¢ä¸ºç®€å•MAEå¥–åŠ±å‡½æ•°")
            
        except Exception as e:
            print(f"{self.name}: åº”ç”¨ä¿®æ”¹æ—¶å‡ºé”™: {e}")
    
    def run_backtest(self, train_df: pd.DataFrame, test_df: pd.DataFrame, ticker: str):
        """
        è¿è¡Œå›æµ‹ - ä½¿ç”¨ä¸å¯¹æ¯”å®éªŒç›¸åŒçš„æ ¸å¿ƒå›æµ‹é€»è¾‘
        """
        try:
            print(f"è¿è¡Œ{self.name}å›æµ‹ - {ticker}")
            print(f"   ä¿®æ”¹: reward_function='simple_mae' (ç®€å•å¥–åŠ±)")
            
            # å¯¼å…¥æ ¸å¿ƒå›æµ‹å‡½æ•°
            import sys
            import os
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            sys.path.insert(0, project_root)
            
            from predict import run_eata_core_backtest
            
            # åˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            combined_df = pd.concat([train_df, test_df]).reset_index(drop=True)
            
            # ä½¿ç”¨æ ¸å¿ƒå›æµ‹å‡½æ•°ï¼Œä¼ å…¥é¢„é…ç½®çš„Agentï¼ˆå·²ç»æ›¿æ¢äº†å¥–åŠ±å‡½æ•°ï¼‰
            core_metrics, portfolio_df = run_eata_core_backtest(
                stock_df=combined_df,
                ticker=ticker,
                lookback=50,
                lookahead=10,
                stride=1,
                depth=300,
                variant_params=None,  # Simpleå˜ä½“ä¸ä½¿ç”¨å‚æ•°ä¼ é€’
                pre_configured_agent=self.agent  # ä½¿ç”¨å·²ç»ä¿®æ”¹è¿‡å¥–åŠ±å‡½æ•°çš„Agent
            )
            print(f"  [è°ƒè¯•] æ ¸å¿ƒå›æµ‹å‡½æ•°æ‰§è¡Œå®Œæˆ")
            print(f"  [è°ƒè¯•] è¿”å›çš„æŒ‡æ ‡: {core_metrics}")
            
            # æå–æŒ‡æ ‡
            annual_return = core_metrics.get('Annual Return (AR)', 0.0)
            sharpe_ratio = core_metrics.get('Sharpe Ratio', 0.0)
            max_drawdown = core_metrics.get('Max Drawdown (MDD)', 0.0)
            win_rate = core_metrics.get('Win Rate', 0.0)
            volatility = core_metrics.get('Volatility (Annual)', 0.0)
            avg_rl_reward = core_metrics.get('Average RL Reward', 0.0)
            
            return {
                'variant': self.name,
                'ticker': ticker,
                'annual_return': annual_return,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'win_rate': win_rate,
                'volatility': volatility,
                'rl_reward': avg_rl_reward,
                'modifications': self.modifications
            }
            
            print(f"{self.name}å›æµ‹å®Œæˆ - å¹´åŒ–æ”¶ç›Š: {annual_return:.4f}, RLå¥–åŠ±: {avg_rl_reward:.6f}")
            
        except Exception as e:
            print(f"{self.name}å›æµ‹å¤±è´¥: {str(e)}")
            return {
                'variant': self.name,
                'ticker': ticker,
                'error': str(e),
                'modifications': self.modifications
            }
    
    def _calculate_returns(self, test_df: pd.DataFrame, trading_signal: int):
        """è®¡ç®—æ”¶ç›Šç‡åºåˆ—"""
        if len(test_df) < 2:
            return np.array([0.0])
            
        prices = test_df['close'].values
        price_returns = np.diff(prices) / prices[:-1]
        strategy_returns = price_returns * trading_signal
        
        return strategy_returns
    
    def _calculate_metrics(self, returns: np.ndarray):
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if len(returns) == 0:
            return {
                'annual_return': 0.0,
                'sharpe_ratio': 0.0,
                'max_drawdown': 0.0,
                'win_rate': 0.0,
                'volatility': 0.0
            }
        
        annual_return = np.mean(returns) * 252
        sharpe_ratio = annual_return / (np.std(returns) * np.sqrt(252)) if np.std(returns) > 0 else 0.0
        
        cumulative_returns = np.cumprod(1 + returns)
        running_max = np.maximum.accumulate(cumulative_returns)
        drawdowns = (cumulative_returns - running_max) / running_max
        max_drawdown = np.min(drawdowns)
        
        win_rate = np.sum(returns > 0) / len(returns) if len(returns) > 0 else 0.0
        volatility = np.std(returns) * np.sqrt(252)
        
        return {
            'annual_return': annual_return,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'volatility': volatility
        }
    
    def get_variant_info(self):
        """è·å–å˜ä½“ä¿¡æ¯"""
        return {
            'name': self.name,
            'description': self.description,
            'modifications': self.modifications,
            'hypothesis': 'å¯¹åˆ†å¸ƒçš„é²æ£’æ€§å˜å·®ï¼Œå®¹æ˜“å—åˆ°æç«¯è¡Œæƒ…å™ªå£°ç‚¹å½±å“',
            'expected_performance': {
                'distribution_robustness': '-60%',
                'extreme_event_handling': 'poor',
                'noise_sensitivity': 'high'
            }
        }

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
import warnings
import logging

# éšè—è­¦å‘Šå’Œæ—¥å¿—å™ªéŸ³
warnings.filterwarnings('ignore')
logging.getLogger('MCTSAdapter').setLevel(logging.CRITICAL)
logging.getLogger('NEMoTS').setLevel(logging.CRITICAL)
logging.getLogger('nemots').setLevel(logging.CRITICAL)
logging.getLogger('engine').setLevel(logging.CRITICAL)
logging.getLogger('model').setLevel(logging.CRITICAL)
logging.getLogger('mcts').setLevel(logging.CRITICAL)
logging.getLogger().setLevel(logging.CRITICAL)

# å®Œå…¨ç¦ç”¨æ‰€æœ‰æ—¥å¿—è¾“å‡º
import sys
class NullWriter:
    def write(self, txt): pass
    def flush(self): pass

# ä¸´æ—¶é‡å®šå‘stderræ¥éšè—æ—¥å¿—
original_stderr = sys.stderr

from data import BaostockDataWorker
from sliding_window_nemots import SlidingWindowNEMoTS
from agent import Agent
from env import StockmarketEnv
from rl import IntegratedRLFeedbackSystem
import torch

class BandwagonRL:
    """
    Bandwagonå¼ºåŒ–å­¦ä¹ ä¸»ç®—æ³•
    æ•´åˆæ»‘åŠ¨çª—å£ã€NEMoTSè®­ç»ƒã€é¢„æµ‹å’ŒRLåé¦ˆæœºåˆ¶
    """
    
    def __init__(self, asset_codes: List[str], window_size: int = 20, lookahead: int = 20, topk: int = 10):
        """
        åˆå§‹åŒ–Bandwagon RLç®—æ³•
        
        Args:
            asset_codes: èµ„äº§ä»£ç åˆ—è¡¨
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            lookahead: å‰ç»çª—å£å¤§å°
            topk: æœ€ä½³æ‹Ÿåˆæ•°é‡
        """
        self.asset_codes = asset_codes
        self.window_size = window_size
        self.lookahead = lookahead
        self.topk = topk
        
        # æ•°æ®å·¥ä½œå™¨
        try:
            self.dataworker = BaostockDataWorker()
            print(f"   âœ… æ•°æ®å·¥ä½œå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"   âš ï¸ æ•°æ®å·¥ä½œå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.dataworker = None
        
        # ä¸ºæ¯ä¸ªèµ„äº§åˆ›å»ºæ»‘åŠ¨çª—å£NEMoTS
        self.nemots_models = {}
        for code in asset_codes:
            try:
                # ä¸´æ—¶éšè—stderrè¾“å‡º
                sys.stderr = NullWriter()
                self.nemots_models[code] = SlidingWindowNEMoTS(
                    lookback=window_size, 
                    lookahead=lookahead
                )
                sys.stderr = original_stderr
                print(f"   âœ… NEMoTSæ¨¡å‹åˆ›å»ºæˆåŠŸ: {code}")
            except Exception as e:
                sys.stderr = original_stderr
                print(f"   âš ï¸ NEMoTSæ¨¡å‹åˆ›å»ºå¤±è´¥: {code}, {e}")
                self.nemots_models[code] = None
        
        # RLç›¸å…³ç»„ä»¶
        self.agents = {}  # æ¯ä¸ªèµ„äº§çš„æ™ºèƒ½ä½“
        self.envs = {}    # æ¯ä¸ªèµ„äº§çš„ç¯å¢ƒ
        
        # è®­ç»ƒå†å²å’Œåé¦ˆæœºåˆ¶
        self.training_history = []
        self.reward_history = []
        self.loss_history = []
        
        # é›†æˆRLåé¦ˆç³»ç»Ÿ
        self.feedback_system = IntegratedRLFeedbackSystem()
        
        print(f"ğŸš€ Bandwagon RLç®—æ³•åˆå§‹åŒ–å®Œæˆ")
        print(f"   èµ„äº§æ•°é‡: {len(asset_codes)}")
        print(f"   çª—å£å¤§å°: {window_size}, å‰ç»: {lookahead}, TopK: {topk}")
    
    def load_asset_data(self, code: str, days: int = 500) -> pd.DataFrame:
        """ä»æ–‡ä»¶ä¸­è¯»å–èµ„äº§ä»£ç æ•°æ®"""
        if self.dataworker is None:
            print(f"ğŸ“Š æ•°æ®å·¥ä½œå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {code}")
            return self._create_mock_data(code, days)
            
        try:
            data = self.dataworker.latest(code, ktype='d', days=days)
            print(f"ğŸ“Š åŠ è½½èµ„äº§ {code}: {len(data)} å¤©æ•°æ®")
            
            # ç¡®ä¿æ•°æ®åŒ…å«å¿…è¦çš„åˆ—
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            if not all(col in data.columns for col in required_cols):
                print(f"âš ï¸ æ•°æ®ç¼ºå°‘å¿…è¦åˆ—ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return self._create_mock_data(code, days)
            
            # ç¡®ä¿æœ‰amountåˆ—
            if 'amount' not in data.columns:
                data['amount'] = data['volume'] * data['close']
            
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½èµ„äº§ {code} å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return self._create_mock_data(code, days)
    
    def _create_mock_data(self, code: str, days: int) -> pd.DataFrame:
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        import datetime
        
        dates = pd.date_range(end=datetime.datetime.now(), periods=days, freq='D')
        
        # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
        base_price = 10.0
        prices = []
        current_price = base_price
        
        for i in range(days):
            # éšæœºæ¸¸èµ° + å°å¹…è¶‹åŠ¿
            change = np.random.normal(0.001, 0.02)  # 0.1%å‡å€¼ï¼Œ2%æ ‡å‡†å·®
            current_price = current_price * (1 + change)
            current_price = max(current_price, 1.0)  # ç¡®ä¿ä»·æ ¼ä¸ºæ­£
            prices.append(current_price)
        
        # ç”ŸæˆOHLCVæ•°æ®
        data = []
        for i, price in enumerate(prices):
            high = price * (1 + np.random.uniform(0, 0.02))
            low = price * (1 - np.random.uniform(0, 0.02))
            open_price = prices[i-1] if i > 0 else price
            volume = np.random.uniform(1000, 10000)
            
            data.append({
                'date': dates[i],
                'open': open_price,
                'high': high,
                'low': low,
                'close': price,
                'volume': volume,
                'amount': volume * price
            })
        
        df = pd.DataFrame(data)
        print(f"   ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {len(df)} å¤©")
        return df
    
    def sliding_window_training(self, code: str, data: pd.DataFrame, current_day: int) -> Dict[str, Any]:
        """
        æ»‘åŠ¨çª—å£è®­ç»ƒ - è·å¾—topkä¸ªæœ€ä½³æ‹Ÿåˆ
        
        Args:
            code: èµ„äº§ä»£ç 
            data: å†å²æ•°æ®
            current_day: å½“å‰è®­ç»ƒæ—¥ç´¢å¼•
            
        Returns:
            è®­ç»ƒç»“æœåŒ…å«topkä¸ªæœ€ä½³æ‹Ÿåˆ
        """
        print(f"\nğŸ§  å¼€å§‹æ»‘åŠ¨çª—å£è®­ç»ƒ: {code}, ç¬¬{current_day}å¤©")
        
        # è·å–è®­ç»ƒçª—å£æ•°æ® - éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®ç”¨äºè®­ç»ƒ
        # è‡³å°‘éœ€è¦ window_size + lookahead çš„æ•°æ®
        required_length = self.window_size + self.lookahead
        start_idx = max(0, current_day - required_length)
        end_idx = current_day
        window_data = data.iloc[start_idx:end_idx].copy()
        
        print(f"   æ•°æ®èŒƒå›´: {start_idx} -> {end_idx}, é•¿åº¦: {len(window_data)}, éœ€è¦: {required_length}")
        
        if len(window_data) < required_length:
            print(f"âš ï¸ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ (éœ€è¦{required_length}å¤©ï¼Œå®é™…{len(window_data)}å¤©)")
            return {'success': False, 'reason': 'insufficient_data'}
        
        # ä½¿ç”¨NEMoTSè¿›è¡Œè®­ç»ƒ
        nemots_model = self.nemots_models.get(code)
        if nemots_model is None:
            print(f"âš ï¸ NEMoTSæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è®­ç»ƒ")
            return {
                'success': True,
                'topk_models': ['simplified_model'] * self.topk,
                'metrics': {'mae': 0.02, 'mse': 0.001, 'corr': 0.5, 'reward': 0.01, 'loss': 0.01},
                'model_object': None
            }
        
        # ä¸´æ—¶éšè—stderrè¾“å‡º
        sys.stderr = NullWriter()
        try:
            training_result = nemots_model.sliding_fit(window_data)
        finally:
            # æ¢å¤stderr
            sys.stderr = original_stderr
        
        if training_result['success']:
            # è¿™é‡Œç®€åŒ–ä¸ºå•ä¸ªæœ€ä½³æ‹Ÿåˆï¼Œå®é™…å¯ä»¥æ‰©å±•ä¸ºtopkä¸ª
            topk_models = [training_result['best_expression']] * self.topk
            
            return {
                'success': True,
                'topk_models': topk_models,
                'metrics': training_result['metrics'],
                'model_object': nemots_model
            }
        else:
            return training_result
    
    def generate_predictions(self, topk_models: List[str], data: pd.DataFrame, n_days: int = 20) -> np.ndarray:
        """
        ä½¿ç”¨topkä¸ªæ‹Ÿåˆå¯¹æœªæ¥næ—¥åšé¢„æµ‹ï¼Œç”Ÿæˆ200ä¸ªä»·æ ¼é¢„æµ‹
        
        Args:
            topk_models: topkä¸ªæœ€ä½³æ‹Ÿåˆæ¨¡å‹
            data: å†å²æ•°æ®
            n_days: é¢„æµ‹å¤©æ•°
            
        Returns:
            shapeä¸º(200, n_days)çš„ä»·æ ¼é¢„æµ‹çŸ©é˜µ
        """
        print(f"ğŸ”® ç”Ÿæˆä»·æ ¼é¢„æµ‹: {len(topk_models)}ä¸ªæ¨¡å‹ Ã— {n_days}å¤©")
        
        # ç®€åŒ–å®ç°ï¼šæ¯ä¸ªæ¨¡å‹ç”Ÿæˆ20ä¸ªé¢„æµ‹ï¼ˆå…±200ä¸ªï¼‰
        predictions_per_model = 200 // self.topk
        all_predictions = []
        
        for model_expr in topk_models:
            # åŸºäºæ¨¡å‹è¡¨è¾¾å¼ç”Ÿæˆé¢„æµ‹ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºéšæœºæ¸¸èµ°+è¶‹åŠ¿ï¼‰
            last_price = data['close'].iloc[-1]
            
            for _ in range(predictions_per_model):
                # ç”Ÿæˆå•æ¡é¢„æµ‹è·¯å¾„
                prediction_path = []
                current_price = last_price
                
                for day in range(n_days):
                    # ç®€åŒ–çš„ä»·æ ¼é¢„æµ‹é€»è¾‘ï¼ˆå®é™…åº”è¯¥åŸºäºNEMoTSæ¨¡å‹ï¼‰
                    trend = np.random.normal(0.001, 0.02)  # å°å¹…ä¸Šæ¶¨è¶‹åŠ¿+å™ªå£°
                    current_price = current_price * (1 + trend)
                    prediction_path.append(current_price)
                
                all_predictions.append(prediction_path)
        
        predictions = np.array(all_predictions)
        print(f"   é¢„æµ‹çŸ©é˜µå½¢çŠ¶: {predictions.shape}")
        return predictions
    
    def generate_trading_signals(self, predictions: np.ndarray) -> List[int]:
        """
        åŸºäº200ä¸ªä»·æ ¼çš„(Q25,Q75)ç”Ÿæˆäº¤æ˜“ä¿¡å·
        æœªæ¥æ›¿æ¢æˆå…±å½¢é¢„æµ‹
        
        Args:
            predictions: ä»·æ ¼é¢„æµ‹çŸ©é˜µ (200, n_days)
            
        Returns:
            äº¤æ˜“ä¿¡å·åˆ—è¡¨ [-1, 0, 1] å¯¹åº” [å–å‡º, æŒæœ‰, ä¹°å…¥]
        """
        print(f"ğŸ“ˆ ç”Ÿæˆäº¤æ˜“ä¿¡å·åŸºäºé¢„æµ‹åˆ†ä½æ•°")
        
        signals = []
        
        for day in range(predictions.shape[1]):
            day_predictions = predictions[:, day]
            
            # è®¡ç®—åˆ†ä½æ•°
            q25 = np.percentile(day_predictions, 25)
            q75 = np.percentile(day_predictions, 75)
            median = np.percentile(day_predictions, 50)
            
            # åŸºäºåˆ†ä½æ•°ç”Ÿæˆä¿¡å·ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é˜ˆå€¼ï¼‰
            # ä»é…ç½®æ–‡ä»¶è¯»å–äº¤æ˜“å‚æ•°
            buy_threshold, sell_threshold, uncertainty_threshold = self._get_trading_thresholds()
            
            iqr = q75 - q25
            if iqr > median * uncertainty_threshold:  # ä½¿ç”¨é…ç½®çš„ä¸ç¡®å®šæ€§é˜ˆå€¼
                signal = 0  # æŒæœ‰
            elif median > day_predictions[0] * buy_threshold:  # ä½¿ç”¨é…ç½®çš„ä¹°å…¥é˜ˆå€¼
                signal = 1  # ä¹°å…¥
            elif median < day_predictions[0] * sell_threshold:  # ä½¿ç”¨é…ç½®çš„å–å‡ºé˜ˆå€¼
                signal = -1  # å–å‡º
            else:
                signal = 0  # æŒæœ‰
            
            signals.append(signal)
        
        print(f"   ç”Ÿæˆä¿¡å·: ä¹°å…¥{signals.count(1)}, æŒæœ‰{signals.count(0)}, å–å‡º{signals.count(-1)}")
        return signals
    
    def _get_trading_thresholds(self):
        """ä»é…ç½®æ–‡ä»¶è¯»å–äº¤æ˜“é˜ˆå€¼"""
        try:
            import json
            import os
            
            config_file = 'config.json'
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                trading_config = config.get('trading', {})
                buy_threshold = trading_config.get('buy_threshold', 1.012)
                sell_threshold = trading_config.get('sell_threshold', 0.988)
                uncertainty_threshold = trading_config.get('uncertainty_threshold', 0.12)
                
                return buy_threshold, sell_threshold, uncertainty_threshold
        except:
            pass
        
        # é»˜è®¤å€¼
        return 1.012, 0.988, 0.12
    
    def calculate_reward_loss(self, signals: List[int], ground_truth: pd.DataFrame) -> Tuple[float, float]:
        """
        äº¤æ˜“ä¿¡å·ä¸ground truthæ¯”å¯¹ï¼Œè·å¾—losså’Œreward
        
        Args:
            signals: äº¤æ˜“ä¿¡å·åˆ—è¡¨
            ground_truth: lookAheadçª—å£çš„çœŸå®æ•°æ®
            
        Returns:
            (reward, loss) å…ƒç»„
        """
        print(f"âš–ï¸ è®¡ç®—rewardå’Œloss")
        
        if len(ground_truth) == 0:
            return 0.0, 1.0
        
        # è®¡ç®—å®é™…æ”¶ç›Š
        actual_returns = ground_truth['close'].pct_change().fillna(0)
        
        # æ ¹æ®ä¿¡å·è®¡ç®—ç­–ç•¥æ”¶ç›Š
        strategy_returns = []
        for i, signal in enumerate(signals[:len(actual_returns)]):
            if i < len(actual_returns):
                if signal == 1:  # ä¹°å…¥
                    strategy_returns.append(actual_returns.iloc[i])
                elif signal == -1:  # å–å‡º
                    strategy_returns.append(-actual_returns.iloc[i])
                else:  # æŒæœ‰
                    strategy_returns.append(0)
            else:
                strategy_returns.append(0)
        
        # è®¡ç®—ç´¯ç§¯æ”¶ç›Šä½œä¸ºreward
        cumulative_return = np.sum(strategy_returns)
        reward = max(0, cumulative_return)  # æ­£æ”¶ç›Šä¸ºreward
        
        # è®¡ç®—lossï¼ˆè´Ÿæ”¶ç›Šçš„ç»å¯¹å€¼ï¼‰
        loss = max(0, -cumulative_return)
        
        print(f"   ç´¯ç§¯æ”¶ç›Š: {cumulative_return:.4f}, Reward: {reward:.4f}, Loss: {loss:.4f}")
        return reward, loss
    
    def extract_market_state(self, data: pd.DataFrame, current_day: int) -> np.ndarray:
        """
        ä»å¸‚åœºæ•°æ®ä¸­æå–çŠ¶æ€ç‰¹å¾å‘é‡
        
        Args:
            data: å¸‚åœºæ•°æ®
            current_day: å½“å‰æ—¥æœŸç´¢å¼•
            
        Returns:
            10ç»´å¸‚åœºçŠ¶æ€ç‰¹å¾å‘é‡
        """
        # è·å–æœ€è¿‘å‡ å¤©çš„æ•°æ®ç”¨äºç‰¹å¾æå–
        lookback_days = min(10, current_day)
        recent_data = data.iloc[current_day-lookback_days:current_day]
        
        if len(recent_data) == 0:
            return np.zeros(10)
        
        # æå–10ç»´ç‰¹å¾
        features = []
        
        # 1. ä»·æ ¼å˜åŒ–ç‡
        price_change = (recent_data['close'].iloc[-1] - recent_data['close'].iloc[0]) / recent_data['close'].iloc[0]
        features.append(np.clip(price_change, -0.1, 0.1))
        
        # 2. ä»·æ ¼æ³¢åŠ¨ç‡
        price_volatility = recent_data['close'].pct_change().std()
        features.append(np.clip(price_volatility, 0, 0.1))
        
        # 3. æˆäº¤é‡å˜åŒ–
        volume_change = (recent_data['volume'].iloc[-1] - recent_data['volume'].iloc[0]) / recent_data['volume'].iloc[0]
        features.append(np.clip(volume_change, -1, 1))
        
        # 4. æœ€é«˜ä»·ç›¸å¯¹ä½ç½®
        high_position = (recent_data['close'].iloc[-1] - recent_data['low'].min()) / (recent_data['high'].max() - recent_data['low'].min())
        features.append(np.clip(high_position, 0, 1))
        
        # 5-7. ç§»åŠ¨å¹³å‡çº¿ç›¸å¯¹ä½ç½®ï¼ˆ3æ—¥ã€5æ—¥ã€10æ—¥ï¼‰
        for window in [3, 5, min(10, len(recent_data))]:
            if len(recent_data) >= window:
                ma = recent_data['close'].rolling(window).mean().iloc[-1]
                ma_position = (recent_data['close'].iloc[-1] - ma) / ma
                features.append(np.clip(ma_position, -0.1, 0.1))
            else:
                features.append(0.0)
        
        # 8. RSIæŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        price_changes = recent_data['close'].pct_change().dropna()
        if len(price_changes) > 0:
            gains = price_changes[price_changes > 0].mean()
            losses = abs(price_changes[price_changes < 0].mean())
            rsi = gains / (gains + losses) if (gains + losses) > 0 else 0.5
            features.append(rsi)
        else:
            features.append(0.5)
        
        # 9. æˆäº¤é‡ç›¸å¯¹å¼ºåº¦
        volume_strength = recent_data['volume'].iloc[-1] / recent_data['volume'].mean()
        features.append(np.clip(volume_strength, 0, 3))
        
        # 10. è¶‹åŠ¿å¼ºåº¦
        if len(recent_data) >= 3:
            trend = np.polyfit(range(len(recent_data)), recent_data['close'], 1)[0]
            trend_strength = trend / recent_data['close'].mean()
            features.append(np.clip(trend_strength, -0.01, 0.01))
        else:
            features.append(0.0)
        
        return np.array(features[:10])  # ç¡®ä¿è¿”å›10ç»´å‘é‡

    def update_agent_with_feedback(self, code: str, reward: float, loss: float, signals: List[int], 
                                 market_state: np.ndarray, action: int, data: pd.DataFrame, current_day: int):
        """
        å°†rewardå’Œlossåé¦ˆåˆ°æ™ºèƒ½ä½“è¿›è¡Œæ›´æ–°
        ä½¿ç”¨é›†æˆçš„RLåé¦ˆç³»ç»Ÿ
        """
        print(f"ğŸ”„ æ›´æ–°æ™ºèƒ½ä½“ {code} - Reward: {reward:.4f}, Loss: {loss:.4f}")
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context = {
            'signals': signals,
            'current_day': current_day,
            'market_volatility': market_state[1] if len(market_state) > 1 else 0.0,
            'trading_volume': market_state[2] if len(market_state) > 2 else 0.0,
            'price_trend': market_state[0] if len(market_state) > 0 else 0.0,
            'prediction_confidence': 0.8,  # å¯ä»¥ä»NEMoTSæ¨¡å‹è·å–
            'asset_code': code
        }
        
        # ä½¿ç”¨é›†æˆåé¦ˆç³»ç»Ÿå¤„ç†rewardå’Œloss
        feedback_result = self.feedback_system.process_episode_feedback(
            code=code,
            reward=reward,
            loss=loss,
            market_state=market_state,
            action=action,
            context=context
        )
        
        # åº”ç”¨åé¦ˆç»“æœåˆ°å…·ä½“ç»„ä»¶
        self._apply_feedback_to_components(code, feedback_result)
        
        # è®°å½•åˆ°å†å²
        self.reward_history.append({
            'code': code,
            'reward': reward,
            'loss': loss,
            'signals': signals,
            'feedback_result': feedback_result,
            'timestamp': pd.Timestamp.now()
        })
        
        return feedback_result
    
    def _apply_feedback_to_components(self, code: str, feedback_result: Dict[str, Any]):
        """
        å°†åé¦ˆç»“æœåº”ç”¨åˆ°å…·ä½“ç»„ä»¶
        """
        print(f"ğŸ”§ åº”ç”¨åé¦ˆåˆ°ç»„ä»¶: {code}")
        
        # 1. åº”ç”¨åˆ°NEMoTSæ¨¡å‹
        if 'loss_processing' in feedback_result and 'nemots' in feedback_result['loss_processing']:
            nemots_feedback = feedback_result['loss_processing']['nemots']
            if code in self.nemots_models:
                self._apply_nemots_feedback(code, nemots_feedback)
        
        # 2. åº”ç”¨åˆ°Agent
        if 'loss_processing' in feedback_result and 'agent' in feedback_result['loss_processing']:
            agent_feedback = feedback_result['loss_processing']['agent']
            self._apply_agent_feedback(code, agent_feedback)
        
        # 3. åº”ç”¨rewardå¼ºåŒ–
        if 'reward_processing' in feedback_result:
            reward_feedback = feedback_result['reward_processing']
            self._apply_reward_reinforcement(code, reward_feedback)
    
    def _apply_nemots_feedback(self, code: str, nemots_feedback: Dict[str, Any]):
        """åº”ç”¨NEMoTSåé¦ˆ"""
        if code not in self.nemots_models:
            return
        
        nemots_model = self.nemots_models[code]
        
        # è°ƒæ•´å­¦ä¹ ç‡
        if 'learning_rate_multiplier' in nemots_feedback:
            lr_mult = nemots_feedback['learning_rate_multiplier']
            if hasattr(nemots_model, 'hyperparams') and hasattr(nemots_model.hyperparams, 'lr'):
                nemots_model.hyperparams.lr *= lr_mult
                print(f"   ğŸ“‰ è°ƒæ•´NEMoTSå­¦ä¹ ç‡: Ã—{lr_mult:.3f}")
        
        # è°ƒæ•´æ¢ç´¢ç‡
        if 'exploration_rate_multiplier' in nemots_feedback:
            exp_mult = nemots_feedback['exploration_rate_multiplier']
            if hasattr(nemots_model, 'hyperparams') and hasattr(nemots_model.hyperparams, 'exploration_rate'):
                nemots_model.hyperparams.exploration_rate *= exp_mult
                print(f"   ğŸ” è°ƒæ•´æ¢ç´¢ç‡: Ã—{exp_mult:.3f}")
    
    def _apply_agent_feedback(self, code: str, agent_feedback: Dict[str, Any]):
        """åº”ç”¨Agentåé¦ˆ"""
        if code not in self.agents:
            # åˆ›å»ºæ™ºèƒ½ä½“
            stock_df = pd.DataFrame({'code': [code], 'name': [code], 'weight': [1.0], 'sector': ['default']})
            self.agents[code] = Agent(stock_df)
        
        agent = self.agents[code]
        
        # è°ƒæ•´æƒé‡ï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•Agentç±»æ¥æ”¯æŒåŠ¨æ€æƒé‡è°ƒæ•´ï¼‰
        print(f"   ğŸ¤– åº”ç”¨Agentæƒé‡è°ƒæ•´: {agent_feedback.get('reason', 'unknown')}")
    
    def _apply_reward_reinforcement(self, code: str, reward_feedback: Dict[str, Any]):
        """åº”ç”¨å¥–åŠ±å¼ºåŒ–"""
        if reward_feedback.get('action') == 'reinforce':
            print(f"   âœ… å¼ºåŒ–ç­–ç•¥: åŠ¨ä½œ{reward_feedback.get('target_action')} å¢å¼º{reward_feedback.get('enhancement', 0):.3f}")
        elif reward_feedback.get('action') == 'penalize':
            print(f"   âŒ æƒ©ç½šç­–ç•¥: åŠ¨ä½œ{reward_feedback.get('target_action')} æƒ©ç½š{reward_feedback.get('penalty', 0):.3f}")
    
    def run_rl_iteration(self, code: str, data: pd.DataFrame, current_day: int) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•æ¬¡RLè¿­ä»£
        
        Args:
            code: èµ„äº§ä»£ç 
            data: å®Œæ•´æ•°æ®
            current_day: å½“å‰æ—¥æœŸç´¢å¼•
            
        Returns:
            è¿­ä»£ç»“æœ
        """
        print(f"\nğŸ”„ RLè¿­ä»£: {code}, ç¬¬{current_day}å¤©")
        
        # 1. æ»‘åŠ¨çª—å£è®­ç»ƒ
        training_result = self.sliding_window_training(code, data, current_day)
        if not training_result['success']:
            return training_result
        
        # 2. ç”Ÿæˆé¢„æµ‹
        topk_models = training_result['topk_models']
        historical_data = data.iloc[:current_day]
        predictions = self.generate_predictions(topk_models, historical_data, self.lookahead)
        
        # 3. ç”Ÿæˆäº¤æ˜“ä¿¡å·
        signals = self.generate_trading_signals(predictions)
        
        # 4. è·å–ground truthï¼ˆlookAheadçª—å£ï¼‰
        lookahead_end = min(current_day + self.lookahead, len(data))
        ground_truth = data.iloc[current_day:lookahead_end]
        
        # 5. è®¡ç®—rewardå’Œloss
        reward, loss = self.calculate_reward_loss(signals, ground_truth)
        
        # 6. æå–å¸‚åœºçŠ¶æ€ç‰¹å¾
        market_state = self.extract_market_state(data, current_day)
        
        # 7. ç¡®å®šä¸»è¦äº¤æ˜“åŠ¨ä½œï¼ˆç®€åŒ–ä¸ºä¿¡å·çš„ä¼—æ•°ï¼‰
        main_action = max(set(signals), key=signals.count) if signals else 0
        main_action = {-1: 0, 0: 1, 1: 2}.get(main_action, 1)  # è½¬æ¢ä¸º0,1,2æ ¼å¼
        
        # 8. åé¦ˆåˆ°æ™ºèƒ½ä½“
        feedback = self.update_agent_with_feedback(code, reward, loss, signals, market_state, main_action, data, current_day)
        
        return {
            'success': True,
            'training_metrics': training_result['metrics'],
            'predictions_shape': predictions.shape,
            'signals': signals,
            'reward': reward,
            'loss': loss,
            'feedback': feedback
        }
    
    def run_algorithm(self):
        """
        è¿è¡Œå®Œæ•´çš„Bandwagonç®—æ³•
        """
        print(f"\nğŸš€ å¯åŠ¨Bandwagonç®—æ³•")
        print("=" * 60)
        
        for code in self.asset_codes:
            print(f"\nå¤„ç†èµ„äº§: {code}")
            
            # 1. åŠ è½½æ•°æ®
            data = self.load_asset_data(code)
            if data.empty:
                continue
            
            # 2. æ»‘åŠ¨çª—å£RLè®­ç»ƒ
            total_days = len(data)
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
            start_day = self.window_size + self.lookahead  # è‡³å°‘éœ€è¦window_size + lookaheadçš„å†å²æ•°æ®
            
            print(f"   æ•°æ®æ€»é•¿åº¦: {total_days}, å¼€å§‹è®­ç»ƒæ—¥: {start_day}")
            
            if start_day >= total_days - self.lookahead:
                print(f"âš ï¸ æ•°æ®ä¸è¶³ä»¥è¿›è¡Œè®­ç»ƒï¼Œè·³è¿‡èµ„äº§ {code}")
                continue
            
            for current_day in range(start_day, total_days - self.lookahead):
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰lookAheadçª—å£
                remaining_days = total_days - current_day
                if remaining_days < self.lookahead:
                    print(f"ğŸ“Š lookAheadçª—å£è€—å°½ï¼Œåˆ‡æ¢åˆ°ç›´æ¥é¢„æµ‹æ¨¡å¼")
                    # ç›´æ¥é¢„æµ‹æ¨¡å¼
                    self.direct_prediction_mode(code, data, current_day)
                    break
                
                # æ‰§è¡ŒRLè¿­ä»£
                iteration_result = self.run_rl_iteration(code, data, current_day)
                
                if iteration_result['success']:
                    self.training_history.append({
                        'code': code,
                        'day': current_day,
                        'result': iteration_result
                    })
                    
                    print(f"   ç¬¬{current_day}å¤©å®Œæˆ - Reward: {iteration_result['reward']:.4f}")
                else:
                    print(f"   ç¬¬{current_day}å¤©å¤±è´¥: {iteration_result.get('reason', 'unknown')}")
        
        print(f"\nâœ… Bandwagonç®—æ³•æ‰§è¡Œå®Œæˆ")
        self.print_summary()
        
        # ä¿å­˜åé¦ˆç³»ç»ŸçŠ¶æ€
        self.feedback_system.save_system_state("bandwagon_feedback_state.pkl")
    
    def direct_prediction_mode(self, code: str, data: pd.DataFrame, current_day: int):
        """
        ç›´æ¥é¢„æµ‹æ¨¡å¼ï¼ˆå½“lookAheadçª—å£è€—å°½æ—¶ï¼‰
        """
        print(f"ğŸ”® ç›´æ¥é¢„æµ‹æ¨¡å¼: {code}")
        
        # ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        if code in self.nemots_models:
            nemots_model = self.nemots_models[code]
            historical_data = data.iloc[:current_day]
            
            # ç”Ÿæˆæœ€ç»ˆé¢„æµ‹
            final_predictions = self.generate_predictions(['final_model'], historical_data, self.lookahead)
            final_signals = self.generate_trading_signals(final_predictions)
            
            print(f"   æœ€ç»ˆé¢„æµ‹ä¿¡å·: {final_signals}")
            return final_signals
        
        return [0] * self.lookahead  # é»˜è®¤æŒæœ‰
    
    def print_summary(self):
        """æ‰“å°ç®—æ³•æ‰§è¡Œæ‘˜è¦"""
        print(f"\nğŸ“Š ç®—æ³•æ‰§è¡Œæ‘˜è¦")
        print(f"   æ€»è®­ç»ƒè½®æ•°: {len(self.training_history)}")
        print(f"   æ€»åé¦ˆè®°å½•: {len(self.reward_history)}")
        
        if self.reward_history:
            total_reward = sum(r['reward'] for r in self.reward_history)
            total_loss = sum(r['loss'] for r in self.reward_history)
            print(f"   æ€»Reward: {total_reward:.4f}")
            print(f"   æ€»Loss: {total_loss:.4f}")
            print(f"   å‡€æ”¶ç›Š: {total_reward - total_loss:.4f}")
            
        # æ‰“å°åé¦ˆç³»ç»Ÿç»Ÿè®¡
        feedback_stats = self.feedback_system.get_system_statistics()
        print(f"\nğŸ”§ åé¦ˆç³»ç»Ÿç»Ÿè®¡:")
        print(f"   æˆåŠŸç‡: {feedback_stats['success_rate']:.2%}")
        print(f"   é€‚åº”æ¬¡æ•°: {feedback_stats['adaptation_count']}")
        if 'reward_statistics' in feedback_stats:
            print(f"   å¹³å‡å¥–åŠ±: {feedback_stats['reward_statistics']['average_reward']:.4f}")


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨ç®—æ³•"""
    # ä»æ–‡ä»¶ä¸­è¯»å–èµ„äº§ä»£ç ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºç¡¬ç¼–ç ï¼‰
    asset_codes = ['sh.600000', 'sh.600036', 'sh.600519']  # ç¤ºä¾‹è‚¡ç¥¨ä»£ç 
    
    # åˆ›å»ºBandwagon RLç®—æ³•å®ä¾‹ - å¢åŠ çª—å£å¤§å°ä»¥æ”¹å–„å­¦ä¹ 
    bandwagon = BandwagonRL(
        asset_codes=asset_codes,
        window_size=30,    # å¢åŠ çª—å£å¤§å°ï¼Œæä¾›æ›´å¤šå­¦ä¹ æ•°æ®
        lookahead=10,      # ä¿æŒå‰ç»çª—å£
        topk=5            # å‡å°‘topkæ•°é‡
    )
    
    # è¿è¡Œç®—æ³•
    bandwagon.run_algorithm()


if __name__ == "__main__":
    main()



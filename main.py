import pandas as pd
import numpy as np
from typing import List, Dict, Any, Tuple
import warnings
import logging

# éšè—è­¦å‘Šå’Œæ—¥å¿—å™ªéŸ³
warnings.filterwarnings('ignore')
logging.getLogger('MCTSAdapter').setLevel(logging.CRITICAL)
logging.getLogger('NEMoTS').setLevel(logging.CRITICAL)
logging.getLogger('nemots').setLevel(logging.CRITICAL)
logging.getLogger('engine').setLevel(logging.CRITICAL)
logging.getLogger('model').setLevel(logging.CRITICAL)
logging.getLogger('mcts').setLevel(logging.CRITICAL)
logging.getLogger().setLevel(logging.CRITICAL)

# å®Œå…¨ç¦ç”¨æ‰€æœ‰æ—¥å¿—è¾“å‡º
import sys
class NullWriter:
    def write(self, txt): pass
    def flush(self): pass

# ä¸´æ—¶é‡å®šå‘stderræ¥éšè—æ—¥å¿—
original_stderr = sys.stderr

from data import BaostockDataWorker
from sliding_window_nemots import SlidingWindowNEMoTS
from agent import Agent
from env import StockmarketEnv
from rl import IntegratedRLFeedbackSystem
from tin_metrics import TradingMetrics, compare_strategies
import torch
import time

class BandwagonRL:
    """
    Bandwagonå¼ºåŒ–å­¦ä¹ ä¸»ç®—æ³•
    æ•´åˆæ»‘åŠ¨çª—å£ã€NEMoTSè®­ç»ƒã€é¢„æµ‹å’ŒRLåé¦ˆæœºåˆ¶
    """
    
    def __init__(self, asset_codes: List[str], window_size: int = 20, lookahead: int = 20, topk: int = 10):
        """
        åˆå§‹åŒ–Bandwagon RLç®—æ³•
        
        Args:
            asset_codes: èµ„äº§ä»£ç åˆ—è¡¨
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            lookahead: å‰ç»çª—å£å¤§å°
            topk: æœ€ä½³æ‹Ÿåˆæ•°é‡
        """
        self.asset_codes = asset_codes
        self.window_size = window_size
        self.lookahead = lookahead
        self.topk = topk
        
        # æ•°æ®å·¥ä½œå™¨
        try:
            self.dataworker = BaostockDataWorker()
            print(f"   âœ… æ•°æ®å·¥ä½œå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"   âš ï¸ æ•°æ®å·¥ä½œå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.dataworker = None
        
        # ä¸ºæ¯ä¸ªèµ„äº§åˆ›å»ºæ»‘åŠ¨çª—å£NEMoTS
        self.nemots_models = {}
        for code in asset_codes:
            try:
                # ä¸´æ—¶éšè—stderrè¾“å‡º
                sys.stderr = NullWriter()
                self.nemots_models[code] = SlidingWindowNEMoTS(
                    lookback=window_size, 
                    lookahead=lookahead
                )
                sys.stderr = original_stderr
                print(f"   âœ… NEMoTSæ¨¡å‹åˆ›å»ºæˆåŠŸ: {code}")
            except Exception as e:
                sys.stderr = original_stderr
                print(f"   âš ï¸ NEMoTSæ¨¡å‹åˆ›å»ºå¤±è´¥: {code}, {e}")
                self.nemots_models[code] = None
        
        # RLç›¸å…³ç»„ä»¶
        self.agents = {}  # æ¯ä¸ªèµ„äº§çš„æ™ºèƒ½ä½“
        self.envs = {}    # æ¯ä¸ªèµ„äº§çš„ç¯å¢ƒ
        
        # è®­ç»ƒå†å²å’Œåé¦ˆæœºåˆ¶
        self.training_history = []
        self.reward_history = []
        self.loss_history = []
        
        # é›†æˆRLåé¦ˆç³»ç»Ÿ
        self.feedback_system = IntegratedRLFeedbackSystem()
        
        # æ—¶é—´ç»Ÿè®¡ - æŒ‰è€å¸ˆå»ºè®®æ·»åŠ 
        self.timing_stats = {
            'total_time': 0,
            'training_times': [],
            'prediction_times': [],
            'signal_generation_times': [],
            'reward_calculation_times': [],
            'feedback_times': [],
            'iteration_times': []
        }
        
        print(f"ğŸš€ Bandwagon RLç®—æ³•åˆå§‹åŒ–å®Œæˆ")
        print(f"   èµ„äº§æ•°é‡: {len(asset_codes)}")
        print(f"   çª—å£å¤§å°: {window_size}, å‰ç»: {lookahead}, TopK: {topk}")
        print(f"   â±ï¸ æ—¶é—´ç»Ÿè®¡åŠŸèƒ½å·²å¯ç”¨")
    
    def load_asset_data(self, code: str, days: int = 500) -> pd.DataFrame:
        """ä»æ–‡ä»¶ä¸­è¯»å–èµ„äº§ä»£ç æ•°æ®"""
        if self.dataworker is None:
            print(f"ğŸ“Š æ•°æ®å·¥ä½œå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {code}")
            return self._create_mock_data(code, days)
            
        try:
            data = self.dataworker.latest(code, ktype='d', days=days)
            print(f"ğŸ“Š åŠ è½½èµ„äº§ {code}: {len(data)} å¤©æ•°æ®")
            
            # ç¡®ä¿æ•°æ®åŒ…å«å¿…è¦çš„åˆ—
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            if not all(col in data.columns for col in required_cols):
                print(f"âš ï¸ æ•°æ®ç¼ºå°‘å¿…è¦åˆ—ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return self._create_mock_data(code, days)
            
            # ç¡®ä¿æœ‰amountåˆ—
            if 'amount' not in data.columns:
                data['amount'] = data['volume'] * data['close']
            
            return data
        except Exception as e:
            print(f"âŒ åŠ è½½èµ„äº§ {code} å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return self._create_mock_data(code, days)
    
    def _create_mock_data(self, code: str, days: int) -> pd.DataFrame:
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        import datetime
        
        dates = pd.date_range(end=datetime.datetime.now(), periods=days, freq='D')
        
        # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
        base_price = 10.0
        prices = []
        current_price = base_price
        
        for i in range(days):
            # éšæœºæ¸¸èµ° + å°å¹…è¶‹åŠ¿
            change = np.random.normal(0.001, 0.02)  # 0.1%å‡å€¼ï¼Œ2%æ ‡å‡†å·®
            current_price = current_price * (1 + change)
            current_price = max(current_price, 1.0)  # ç¡®ä¿ä»·æ ¼ä¸ºæ­£
            prices.append(current_price)
        
        # ç”ŸæˆOHLCVæ•°æ®
        data = []
        for i, price in enumerate(prices):
            high = price * (1 + np.random.uniform(0, 0.02))
            low = price * (1 - np.random.uniform(0, 0.02))
            open_price = prices[i-1] if i > 0 else price
            volume = np.random.uniform(1000, 10000)
            
            data.append({
                'date': dates[i],
                'open': open_price,
                'high': high,
                'low': low,
                'close': price,
                'volume': volume,
                'amount': volume * price
            })
        
        df = pd.DataFrame(data)
        print(f"   ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®: {len(df)} å¤©")
        return df
    
    def sliding_window_training(self, code: str, data: pd.DataFrame, current_day: int) -> Dict[str, Any]:
        """
        æ»‘åŠ¨çª—å£è®­ç»ƒ - è·å¾—topkä¸ªæœ€ä½³æ‹Ÿåˆ
        
        Args:
            code: èµ„äº§ä»£ç 
            data: å†å²æ•°æ®
            current_day: å½“å‰è®­ç»ƒæ—¥ç´¢å¼•
            
        Returns:
            è®­ç»ƒç»“æœåŒ…å«topkä¸ªæœ€ä½³æ‹Ÿåˆ
        """
        start_time = time.time()  # å¼€å§‹è®¡æ—¶
        print(f"\nğŸ§  å¼€å§‹æ»‘åŠ¨çª—å£è®­ç»ƒ: {code}, ç¬¬{current_day}å¤©")
        
        # è·å–è®­ç»ƒçª—å£æ•°æ® - éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®ç”¨äºè®­ç»ƒ
        # è‡³å°‘éœ€è¦ window_size + lookahead çš„æ•°æ®
        required_length = self.window_size + self.lookahead
        start_idx = max(0, current_day - required_length)
        end_idx = current_day
        window_data = data.iloc[start_idx:end_idx].copy()
        
        print(f"   æ•°æ®èŒƒå›´: {start_idx} -> {end_idx}, é•¿åº¦: {len(window_data)}, éœ€è¦: {required_length}")
        
        if len(window_data) < required_length:
            print(f"âš ï¸ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®­ç»ƒ (éœ€è¦{required_length}å¤©ï¼Œå®é™…{len(window_data)}å¤©)")
            return {'success': False, 'reason': 'insufficient_data'}
        
        # ä½¿ç”¨NEMoTSè¿›è¡Œè®­ç»ƒ
        nemots_model = self.nemots_models.get(code)
        if nemots_model is None:
            print(f"âš ï¸ NEMoTSæ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–è®­ç»ƒ")
            return {
                'success': True,
                'topk_models': ['simplified_model'] * self.topk,
                'metrics': {'mae': 0.02, 'mse': 0.001, 'corr': 0.5, 'reward': 0.01, 'loss': 0.01},
                'model_object': None
            }
        
        # ä¸´æ—¶éšè—stderrè¾“å‡º
        sys.stderr = NullWriter()
        try:
            training_result = nemots_model.sliding_fit(window_data)
        finally:
            # æ¢å¤stderr
            sys.stderr = original_stderr
        
        if training_result['success']:
            # è¿™é‡Œç®€åŒ–ä¸ºå•ä¸ªæœ€ä½³æ‹Ÿåˆï¼Œå®é™…å¯ä»¥æ‰©å±•ä¸ºtopkä¸ª
            topk_models = [training_result['best_expression']] * self.topk
            
            # è®°å½•è®­ç»ƒæ—¶é—´
            training_time = time.time() - start_time
            self.timing_stats['training_times'].append(training_time)
            print(f"   â±ï¸ è®­ç»ƒè€—æ—¶: {training_time:.3f}ç§’")
            
            return {
                'success': True,
                'topk_models': topk_models,
                'metrics': training_result.get('metrics', {
                    'mae': training_result.get('mae', 0.02),
                    'mse': training_result.get('mse', 0.001), 
                    'corr': training_result.get('corr', 0.5),
                    'reward': training_result.get('reward', 0.01),
                    'loss': training_result.get('loss', 0.01)
                }),
                'model_object': nemots_model,
                'training_time': training_time
            }
        else:
            # ç¡®ä¿å¤±è´¥æ—¶ä¹Ÿæœ‰metricså­—æ®µ
            return {
                'success': False,
                'reason': training_result.get('reason', 'training_failed'),
                'topk_models': [],
                'metrics': {
                    'mae': training_result.get('mae', 1.0),
                    'mse': training_result.get('mse', 1.0),
                    'corr': training_result.get('corr', 0.0),
                    'reward': training_result.get('reward', 0.0),
                    'loss': training_result.get('loss', 1.0)
                },
                'model_object': None
            }
    
    def generate_predictions(self, topk_models: List[str], data: pd.DataFrame, n_days: int = 20) -> np.ndarray:
        """
        ä½¿ç”¨topkä¸ªæ‹Ÿåˆå¯¹æœªæ¥næ—¥åšé¢„æµ‹ï¼Œç”Ÿæˆ200ä¸ªä»·æ ¼é¢„æµ‹
        
        Args:
            topk_models: topkä¸ªæœ€ä½³æ‹Ÿåˆæ¨¡å‹
            data: å†å²æ•°æ®
            n_days: é¢„æµ‹å¤©æ•°
            
        Returns:
            shapeä¸º(200, n_days)çš„ä»·æ ¼é¢„æµ‹çŸ©é˜µ
        """
        start_time = time.time()
        print(f"ğŸ”® ç”Ÿæˆä»·æ ¼é¢„æµ‹: {len(topk_models)}ä¸ªæ¨¡å‹ Ã— {n_days}å¤©")
        
        # ç®€åŒ–å®ç°ï¼šæ¯ä¸ªæ¨¡å‹ç”Ÿæˆ20ä¸ªé¢„æµ‹ï¼ˆå…±200ä¸ªï¼‰
        predictions_per_model = 200 // self.topk
        all_predictions = []
        
        for model_expr in topk_models:
            # åŸºäºæ¨¡å‹è¡¨è¾¾å¼ç”Ÿæˆé¢„æµ‹ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºéšæœºæ¸¸èµ°+è¶‹åŠ¿ï¼‰
            last_price = data['close'].iloc[-1]
            
            for _ in range(predictions_per_model):
                # ç”Ÿæˆå•æ¡é¢„æµ‹è·¯å¾„
                prediction_path = []
                current_price = last_price
                
                for day in range(n_days):
                    # ç®€åŒ–çš„ä»·æ ¼é¢„æµ‹é€»è¾‘ï¼ˆå®é™…åº”è¯¥åŸºäºNEMoTSæ¨¡å‹ï¼‰
                    trend = np.random.normal(0.001, 0.02)  # å°å¹…ä¸Šæ¶¨è¶‹åŠ¿+å™ªå£°
                    current_price = current_price * (1 + trend)
                    prediction_path.append(current_price)
                
                all_predictions.append(prediction_path)
        
        predictions = np.array(all_predictions)
        
        # è®°å½•é¢„æµ‹æ—¶é—´
        prediction_time = time.time() - start_time
        self.timing_stats['prediction_times'].append(prediction_time)
        print(f"   é¢„æµ‹çŸ©é˜µå½¢çŠ¶: {predictions.shape}")
        print(f"   â±ï¸ é¢„æµ‹è€—æ—¶: {prediction_time:.3f}ç§’")
        return predictions
    
    def generate_trading_signals(self, predictions: np.ndarray, current_price: float = None) -> List[int]:
        """
        åŸºäº200ä¸ªä»·æ ¼é¢„æµ‹çš„(Q25,Q75)ç”Ÿæˆäº¤æ˜“ä¿¡å·
        æœªæ¥æ›¿æ¢æˆå…±å½¢é¢„æµ‹
        
        Args:
            predictions: ä»·æ ¼é¢„æµ‹çŸ©é˜µ (200, n_days)
            current_price: å½“å‰ä»·æ ¼ä½œä¸ºåŸºå‡†
            
        Returns:
            äº¤æ˜“ä¿¡å·åˆ—è¡¨ [-1, 0, 1] å¯¹åº” [å–å‡º, æŒæœ‰, ä¹°å…¥]
        """
        start_time = time.time()
        print(f"ğŸ“ˆ ç”Ÿæˆäº¤æ˜“ä¿¡å·åŸºäºé¢„æµ‹åˆ†ä½æ•°")
        
        signals = []
        
        # å¦‚æœæ²¡æœ‰æä¾›å½“å‰ä»·æ ¼ï¼Œä½¿ç”¨ç¬¬ä¸€å¤©é¢„æµ‹çš„ä¸­ä½æ•°ä½œä¸ºåŸºå‡†
        if current_price is None:
            current_price = np.percentile(predictions[:, 0], 50)
        
        for day in range(predictions.shape[1]):
            day_predictions = predictions[:, day]
            
            # è®¡ç®—åˆ†ä½æ•°
            q25 = np.percentile(day_predictions, 25)
            q75 = np.percentile(day_predictions, 75)
            median = np.percentile(day_predictions, 50)
            
            # ä»é…ç½®æ–‡ä»¶è¯»å–äº¤æ˜“å‚æ•° - è°ƒæ•´ä¸ºæ›´åˆç†çš„é˜ˆå€¼
            buy_threshold, sell_threshold, uncertainty_threshold = self._get_trading_thresholds()
            
            # è®¡ç®—é¢„æµ‹çš„ç›¸å¯¹å˜åŒ–
            predicted_change = (median - current_price) / current_price
            uncertainty = (q75 - q25) / median if median > 0 else 0
            
            # æ”¹è¿›çš„ä¿¡å·ç”Ÿæˆé€»è¾‘
            if uncertainty > uncertainty_threshold:  # ä¸ç¡®å®šæ€§å¤ªé«˜
                signal = 0  # æŒæœ‰
            elif predicted_change > (buy_threshold - 1.0):  # é¢„æœŸä¸Šæ¶¨è¶…è¿‡é˜ˆå€¼
                signal = 1  # ä¹°å…¥
            elif predicted_change < -(1.0 - sell_threshold):  # é¢„æœŸä¸‹è·Œè¶…è¿‡é˜ˆå€¼
                signal = -1  # å–å‡º
            else:
                signal = 0  # æŒæœ‰
            
            signals.append(signal)
            
            # æ›´æ–°åŸºå‡†ä»·æ ¼ä¸ºå½“å‰é¢„æµ‹çš„ä¸­ä½æ•°
            current_price = median
        
        # è®°å½•ä¿¡å·ç”Ÿæˆæ—¶é—´
        signal_time = time.time() - start_time
        self.timing_stats['signal_generation_times'].append(signal_time)
        print(f"   ç”Ÿæˆä¿¡å·: ä¹°å…¥{signals.count(1)}, æŒæœ‰{signals.count(0)}, å–å‡º{signals.count(-1)}")
        print(f"   â±ï¸ ä¿¡å·ç”Ÿæˆè€—æ—¶: {signal_time:.3f}ç§’")
        return signals
    
    def _get_trading_thresholds(self):
        """ä»é…ç½®æ–‡ä»¶è¯»å–äº¤æ˜“é˜ˆå€¼"""
        try:
            import json
            import os
            
            config_file = 'config.json'
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                trading_config = config.get('trading', {})
                buy_threshold = trading_config.get('buy_threshold', 1.012)
                sell_threshold = trading_config.get('sell_threshold', 0.988)
                uncertainty_threshold = trading_config.get('uncertainty_threshold', 0.12)
                
                return buy_threshold, sell_threshold, uncertainty_threshold
        except:
            pass
        
        # é»˜è®¤å€¼ - è°ƒæ•´ä¸ºæ›´åˆç†çš„é˜ˆå€¼
        return 1.005, 0.995, 0.05  # 0.5%æ¶¨è·Œå¹…é˜ˆå€¼ï¼Œ5%ä¸ç¡®å®šæ€§é˜ˆå€¼
    
    def calculate_reward_loss(self, signals: List[int], ground_truth: pd.DataFrame) -> Tuple[float, float]:
        """
        äº¤æ˜“ä¿¡å·ä¸ground truthæ¯”å¯¹ï¼Œè·å¾—losså’Œreward
        
        Args:
            signals: äº¤æ˜“ä¿¡å·åˆ—è¡¨
            ground_truth: lookAheadçª—å£çš„çœŸå®æ•°æ®
            
        Returns:
            (reward, loss) å…ƒç»„
        """
        print(f"âš–ï¸ è®¡ç®—rewardå’Œloss")
        
        if len(ground_truth) == 0:
            return 0.0, 1.0
        
        # è®¡ç®—å®é™…æ”¶ç›Š
        actual_returns = ground_truth['close'].pct_change().fillna(0)
        
        # è®¡ç®—åŸºå‡†æ”¶ç›Šï¼ˆä¹°å…¥æŒæœ‰ç­–ç•¥ï¼‰
        benchmark_return = np.sum(actual_returns)
        
        # æ ¹æ®ä¿¡å·è®¡ç®—ç­–ç•¥æ”¶ç›Š
        strategy_returns = []
        for i, signal in enumerate(signals[:len(actual_returns)]):
            if i < len(actual_returns):
                if signal == 1:  # ä¹°å…¥
                    strategy_returns.append(actual_returns.iloc[i])
                elif signal == -1:  # å–å‡º (åšç©º)
                    strategy_returns.append(-actual_returns.iloc[i])
                else:  # æŒæœ‰ - æ”¹ä¸ºè·å¾—å¸‚åœºæ”¶ç›Šè€Œä¸æ˜¯0
                    strategy_returns.append(actual_returns.iloc[i] * 0.5)  # æŒæœ‰è·å¾—50%å¸‚åœºæ”¶ç›Š
            else:
                strategy_returns.append(0)
        
        # è®¡ç®—ç­–ç•¥æ”¶ç›Š
        strategy_return = np.sum(strategy_returns)
        
        # è®¡ç®—è¶…é¢æ”¶ç›Šï¼ˆç›¸å¯¹äºåŸºå‡†ï¼‰
        excess_return = strategy_return - benchmark_return
        
        # æ”¹è¿›çš„reward/lossè®¡ç®—
        if excess_return > 0:
            reward = excess_return + max(0, strategy_return)  # è¶…é¢æ”¶ç›Š + ç»å¯¹æ”¶ç›Š
            loss = 0
        else:
            reward = max(0, strategy_return)  # è‡³å°‘è·å¾—æ­£çš„ç»å¯¹æ”¶ç›Š
            loss = max(0, -excess_return)  # ç›¸å¯¹åŸºå‡†çš„æŸå¤±
        
        print(f"   ç­–ç•¥æ”¶ç›Š: {strategy_return:.4f}, è¶…é¢æ”¶ç›Š: {excess_return:.4f}, Reward: {reward:.4f}, Loss: {loss:.4f}")
        return reward, loss
    
    def extract_market_state(self, data: pd.DataFrame, current_day: int) -> np.ndarray:
        """
        ä»å¸‚åœºæ•°æ®ä¸­æå–çŠ¶æ€ç‰¹å¾å‘é‡
        
        Args:
            data: å¸‚åœºæ•°æ®
            current_day: å½“å‰æ—¥æœŸç´¢å¼•
            
        Returns:
            10ç»´å¸‚åœºçŠ¶æ€ç‰¹å¾å‘é‡
        """
        # è·å–æœ€è¿‘å‡ å¤©çš„æ•°æ®ç”¨äºç‰¹å¾æå–
        lookback_days = min(10, current_day)
        recent_data = data.iloc[current_day-lookback_days:current_day]
        
        if len(recent_data) == 0:
            return np.zeros(10)
        
        # æå–10ç»´ç‰¹å¾
        features = []
        
        # 1. ä»·æ ¼å˜åŒ–ç‡
        price_change = (recent_data['close'].iloc[-1] - recent_data['close'].iloc[0]) / recent_data['close'].iloc[0]
        features.append(np.clip(price_change, -0.1, 0.1))
        
        # 2. ä»·æ ¼æ³¢åŠ¨ç‡
        price_volatility = recent_data['close'].pct_change().std()
        features.append(np.clip(price_volatility, 0, 0.1))
        
        # 3. æˆäº¤é‡å˜åŒ–
        volume_change = (recent_data['volume'].iloc[-1] - recent_data['volume'].iloc[0]) / recent_data['volume'].iloc[0]
        features.append(np.clip(volume_change, -1, 1))
        
        # 4. æœ€é«˜ä»·ç›¸å¯¹ä½ç½®
        high_position = (recent_data['close'].iloc[-1] - recent_data['low'].min()) / (recent_data['high'].max() - recent_data['low'].min())
        features.append(np.clip(high_position, 0, 1))
        
        # 5-7. ç§»åŠ¨å¹³å‡çº¿ç›¸å¯¹ä½ç½®ï¼ˆ3æ—¥ã€5æ—¥ã€10æ—¥ï¼‰
        for window in [3, 5, min(10, len(recent_data))]:
            if len(recent_data) >= window:
                ma = recent_data['close'].rolling(window).mean().iloc[-1]
                ma_position = (recent_data['close'].iloc[-1] - ma) / ma
                features.append(np.clip(ma_position, -0.1, 0.1))
            else:
                features.append(0.0)
        
        # 8. RSIæŒ‡æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        price_changes = recent_data['close'].pct_change().dropna()
        if len(price_changes) > 0:
            gains = price_changes[price_changes > 0].mean()
            losses = abs(price_changes[price_changes < 0].mean())
            rsi = gains / (gains + losses) if (gains + losses) > 0 else 0.5
            features.append(rsi)
        else:
            features.append(0.5)
        
        # 9. æˆäº¤é‡ç›¸å¯¹å¼ºåº¦
        volume_strength = recent_data['volume'].iloc[-1] / recent_data['volume'].mean()
        features.append(np.clip(volume_strength, 0, 3))
        
        # 10. è¶‹åŠ¿å¼ºåº¦
        if len(recent_data) >= 3:
            trend = np.polyfit(range(len(recent_data)), recent_data['close'], 1)[0]
            trend_strength = trend / recent_data['close'].mean()
            features.append(np.clip(trend_strength, -0.01, 0.01))
        else:
            features.append(0.0)
        
        return np.array(features[:10])  # ç¡®ä¿è¿”å›10ç»´å‘é‡

    def update_agent_with_feedback(self, code: str, reward: float, loss: float, signals: List[int], 
                                 market_state: np.ndarray, action: int, data: pd.DataFrame, current_day: int):
        """
        å°†rewardå’Œlossåé¦ˆåˆ°æ™ºèƒ½ä½“è¿›è¡Œæ›´æ–°
        ä½¿ç”¨é›†æˆçš„RLåé¦ˆç³»ç»Ÿ
        """
        print(f"ğŸ”„ æ›´æ–°æ™ºèƒ½ä½“ {code} - Reward: {reward:.4f}, Loss: {loss:.4f}")
        
        # æ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯
        context = {
            'signals': signals,
            'current_day': current_day,
            'market_volatility': market_state[1] if len(market_state) > 1 else 0.0,
            'trading_volume': market_state[2] if len(market_state) > 2 else 0.0,
            'price_trend': market_state[0] if len(market_state) > 0 else 0.0,
            'prediction_confidence': 0.8,  # å¯ä»¥ä»NEMoTSæ¨¡å‹è·å–
            'asset_code': code
        }
        
        # ä½¿ç”¨é›†æˆåé¦ˆç³»ç»Ÿå¤„ç†rewardå’Œloss
        feedback_result = self.feedback_system.process_episode_feedback(
            code=code,
            reward=reward,
            loss=loss,
            market_state=market_state,
            action=action,
            context=context
        )
        
        # åº”ç”¨åé¦ˆç»“æœåˆ°å…·ä½“ç»„ä»¶
        self._apply_feedback_to_components(code, feedback_result)
        
        # è®°å½•åˆ°å†å²
        self.reward_history.append({
            'code': code,
            'reward': reward,
            'loss': loss,
            'signals': signals,
            'feedback_result': feedback_result,
            'timestamp': pd.Timestamp.now()
        })
        
        return feedback_result
    
    def _apply_feedback_to_components(self, code: str, feedback_result: Dict[str, Any]):
        """
        å°†åé¦ˆç»“æœåº”ç”¨åˆ°å…·ä½“ç»„ä»¶
        """
        print(f"ğŸ”§ åº”ç”¨åé¦ˆåˆ°ç»„ä»¶: {code}")
        
        # 1. åº”ç”¨åˆ°NEMoTSæ¨¡å‹
        if 'loss_processing' in feedback_result and 'nemots' in feedback_result['loss_processing']:
            nemots_feedback = feedback_result['loss_processing']['nemots']
            if code in self.nemots_models:
                self._apply_nemots_feedback(code, nemots_feedback)
        
        # 2. åº”ç”¨åˆ°Agent
        if 'loss_processing' in feedback_result and 'agent' in feedback_result['loss_processing']:
            agent_feedback = feedback_result['loss_processing']['agent']
            self._apply_agent_feedback(code, agent_feedback)
        
        # 3. åº”ç”¨rewardå¼ºåŒ–
        if 'reward_processing' in feedback_result:
            reward_feedback = feedback_result['reward_processing']
            self._apply_reward_reinforcement(code, reward_feedback)
    
    def _apply_nemots_feedback(self, code: str, nemots_feedback: Dict[str, Any]):
        """åº”ç”¨NEMoTSåé¦ˆ"""
        if code not in self.nemots_models:
            return
        
        nemots_model = self.nemots_models[code]
        
        # è°ƒæ•´å­¦ä¹ ç‡
        if 'learning_rate_multiplier' in nemots_feedback:
            lr_mult = nemots_feedback['learning_rate_multiplier']
            if hasattr(nemots_model, 'hyperparams') and hasattr(nemots_model.hyperparams, 'lr'):
                nemots_model.hyperparams.lr *= lr_mult
                print(f"   ğŸ“‰ è°ƒæ•´NEMoTSå­¦ä¹ ç‡: Ã—{lr_mult:.3f}")
        
        # è°ƒæ•´æ¢ç´¢ç‡
        if 'exploration_rate_multiplier' in nemots_feedback:
            exp_mult = nemots_feedback['exploration_rate_multiplier']
            if hasattr(nemots_model, 'hyperparams') and hasattr(nemots_model.hyperparams, 'exploration_rate'):
                nemots_model.hyperparams.exploration_rate *= exp_mult
                print(f"   ğŸ” è°ƒæ•´æ¢ç´¢ç‡: Ã—{exp_mult:.3f}")
    
    def _apply_agent_feedback(self, code: str, agent_feedback: Dict[str, Any]):
        """åº”ç”¨Agentåé¦ˆ"""
        if code not in self.agents:
            # åˆ›å»ºæ™ºèƒ½ä½“
            stock_df = pd.DataFrame({'code': [code], 'name': [code], 'weight': [1.0], 'sector': ['default']})
            self.agents[code] = Agent(stock_df)
        
        agent = self.agents[code]
        
        # è°ƒæ•´æƒé‡ï¼ˆè¿™é‡Œå¯ä»¥æ‰©å±•Agentç±»æ¥æ”¯æŒåŠ¨æ€æƒé‡è°ƒæ•´ï¼‰
        print(f"   ğŸ¤– åº”ç”¨Agentæƒé‡è°ƒæ•´: {agent_feedback.get('reason', 'unknown')}")
    
    def _apply_reward_reinforcement(self, code: str, reward_feedback: Dict[str, Any]):
        """åº”ç”¨å¥–åŠ±å¼ºåŒ–"""
        if reward_feedback.get('action') == 'reinforce':
            print(f"   âœ… å¼ºåŒ–ç­–ç•¥: åŠ¨ä½œ{reward_feedback.get('target_action')} å¢å¼º{reward_feedback.get('enhancement', 0):.3f}")
        elif reward_feedback.get('action') == 'penalize':
            print(f"   âŒ æƒ©ç½šç­–ç•¥: åŠ¨ä½œ{reward_feedback.get('target_action')} æƒ©ç½š{reward_feedback.get('penalty', 0):.3f}")
    
    def run_rl_iteration(self, code: str, data: pd.DataFrame, current_day: int) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•æ¬¡RLè¿­ä»£
        
        Args:
            code: èµ„äº§ä»£ç 
            data: å®Œæ•´æ•°æ®
            current_day: å½“å‰æ—¥æœŸç´¢å¼•
            
        Returns:
            è¿­ä»£ç»“æœ
        """
        iteration_start_time = time.time()  # å¤§å¾ªç¯è®¡æ—¶
        print(f"\nğŸ”„ RLè¿­ä»£: {code}, ç¬¬{current_day}å¤©")
        
        # 1. æ»‘åŠ¨çª—å£è®­ç»ƒ
        training_result = self.sliding_window_training(code, data, current_day)
        if not training_result['success']:
            return training_result
        
        # 2. ç”Ÿæˆé¢„æµ‹
        topk_models = training_result['topk_models']
        historical_data = data.iloc[:current_day]
        predictions = self.generate_predictions(topk_models, historical_data, self.lookahead)
        
        # 3. ç”Ÿæˆäº¤æ˜“ä¿¡å·
        current_price = historical_data['close'].iloc[-1]
        signals = self.generate_trading_signals(predictions, current_price)
        
        # 4. è·å–ground truthï¼ˆlookAheadçª—å£ï¼‰
        lookahead_end = min(current_day + self.lookahead, len(data))
        ground_truth = data.iloc[current_day:lookahead_end]
        
        # 5. è®¡ç®—rewardå’Œloss
        reward, loss = self.calculate_reward_loss(signals, ground_truth)
        
        # 6. æå–å¸‚åœºçŠ¶æ€ç‰¹å¾
        market_state = self.extract_market_state(data, current_day)
        
        # 7. ç¡®å®šä¸»è¦äº¤æ˜“åŠ¨ä½œï¼ˆç®€åŒ–ä¸ºä¿¡å·çš„ä¼—æ•°ï¼‰
        main_action = max(set(signals), key=signals.count) if signals else 0
        main_action = {-1: 0, 0: 1, 1: 2}.get(main_action, 1)  # è½¬æ¢ä¸º0,1,2æ ¼å¼
        
        # 8. åé¦ˆåˆ°æ™ºèƒ½ä½“
        feedback = self.update_agent_with_feedback(code, reward, loss, signals, market_state, main_action, data, current_day)
        
        # è®°å½•æ•´ä¸ªè¿­ä»£æ—¶é—´
        iteration_time = time.time() - iteration_start_time
        self.timing_stats['iteration_times'].append(iteration_time)
        print(f"   â±ï¸ å®Œæ•´è¿­ä»£è€—æ—¶: {iteration_time:.3f}ç§’")
        
        return {
            'success': True,
            'training_metrics': training_result['metrics'],
            'predictions_shape': predictions.shape,
            'signals': signals,
            'reward': reward,
            'loss': loss,
            'feedback': feedback,
            'iteration_time': iteration_time
        }
    
    def run_algorithm(self):
        """
        è¿è¡Œå®Œæ•´çš„Bandwagonç®—æ³•
        """
        total_start_time = time.time()  # æ€»æ—¶é—´è®¡æ—¶
        print(f"\nğŸš€ å¯åŠ¨Bandwagonç®—æ³•")
        print("=" * 60)
        
        for code in self.asset_codes:
            print(f"\nå¤„ç†èµ„äº§: {code}")
            
            # 1. åŠ è½½æ•°æ®
            data = self.load_asset_data(code)
            if data.empty:
                continue
            
            # 2. æ»‘åŠ¨çª—å£RLè®­ç»ƒ
            total_days = len(data)
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
            start_day = self.window_size + self.lookahead  # è‡³å°‘éœ€è¦window_size + lookaheadçš„å†å²æ•°æ®
            
            print(f"   æ•°æ®æ€»é•¿åº¦: {total_days}, å¼€å§‹è®­ç»ƒæ—¥: {start_day}")
            
            if start_day >= total_days - self.lookahead:
                print(f"âš ï¸ æ•°æ®ä¸è¶³ä»¥è¿›è¡Œè®­ç»ƒï¼Œè·³è¿‡èµ„äº§ {code}")
                continue
            
            for current_day in range(start_day, total_days - self.lookahead):
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰lookAheadçª—å£
                remaining_days = total_days - current_day
                if remaining_days < self.lookahead:
                    print(f"ğŸ“Š lookAheadçª—å£è€—å°½ï¼Œåˆ‡æ¢åˆ°ç›´æ¥é¢„æµ‹æ¨¡å¼")
                    # ç›´æ¥é¢„æµ‹æ¨¡å¼
                    self.direct_prediction_mode(code, data, current_day)
                    break
                
                # æ‰§è¡ŒRLè¿­ä»£
                iteration_result = self.run_rl_iteration(code, data, current_day)
                
                if iteration_result['success']:
                    self.training_history.append({
                        'code': code,
                        'day': current_day,
                        'result': iteration_result
                    })
                    
                    print(f"   ç¬¬{current_day}å¤©å®Œæˆ - Reward: {iteration_result['reward']:.4f}")
                else:
                    print(f"   ç¬¬{current_day}å¤©å¤±è´¥: {iteration_result.get('reason', 'unknown')}")
        
        # è®°å½•æ€»æ—¶é—´
        self.timing_stats['total_time'] = time.time() - total_start_time
        
        print(f"\nâœ… Bandwagonç®—æ³•æ‰§è¡Œå®Œæˆ")
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {self.timing_stats['total_time']:.2f}ç§’")
        
        # æ‰“å°æ—¶é—´åˆ†æ
        self._print_timing_analysis()
        
        self.print_summary()
        
        # ä¿å­˜åé¦ˆç³»ç»ŸçŠ¶æ€
        self.feedback_system.save_system_state("bandwagon_feedback_state.pkl")
    
    def direct_prediction_mode(self, code: str, data: pd.DataFrame, current_day: int):
        """
        ç›´æ¥é¢„æµ‹æ¨¡å¼ï¼ˆå½“lookAheadçª—å£è€—å°½æ—¶ï¼‰
        """
        print(f"ğŸ”® ç›´æ¥é¢„æµ‹æ¨¡å¼: {code}")
        
        # ä½¿ç”¨æœ€æ–°è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
        if code in self.nemots_models:
            nemots_model = self.nemots_models[code]
            historical_data = data.iloc[:current_day]
            
            # ç”Ÿæˆæœ€ç»ˆé¢„æµ‹
            final_predictions = self.generate_predictions(['final_model'], historical_data, self.lookahead)
            current_price = historical_data['close'].iloc[-1]
            final_signals = self.generate_trading_signals(final_predictions, current_price)
            
            print(f"   æœ€ç»ˆé¢„æµ‹ä¿¡å·: {final_signals}")
            return final_signals
        
        return [0] * self.lookahead  # é»˜è®¤æŒæœ‰
    
    def _print_timing_analysis(self):
        """æŒ‰è€å¸ˆå»ºè®®ï¼šæ‰“å°è¯¦ç»†çš„æ—¶é—´åˆ†æ"""
        print(f"\nâ±ï¸ è¯¦ç»†æ—¶é—´åˆ†æ (æŒ‰è€å¸ˆå»ºè®®)")
        print("=" * 60)
        
        stats = self.timing_stats
        
        if stats['training_times']:
            print(f"ğŸ§  è®­ç»ƒæ—¶é—´ç»Ÿè®¡:")
            print(f"   æ€»æ¬¡æ•°: {len(stats['training_times'])}")
            print(f"   å¹³å‡è€—æ—¶: {np.mean(stats['training_times']):.3f}ç§’")
            print(f"   æœ€é•¿è€—æ—¶: {np.max(stats['training_times']):.3f}ç§’")
            print(f"   æœ€çŸ­è€—æ—¶: {np.min(stats['training_times']):.3f}ç§’")
            print(f"   æ€»è®­ç»ƒæ—¶é—´: {np.sum(stats['training_times']):.2f}ç§’")
            print(f"   å æ€»æ—¶é—´: {np.sum(stats['training_times'])/stats['total_time']*100:.1f}%")
        
        if stats['prediction_times']:
            print(f"\nğŸ”® é¢„æµ‹æ—¶é—´ç»Ÿè®¡:")
            print(f"   æ€»æ¬¡æ•°: {len(stats['prediction_times'])}")
            print(f"   å¹³å‡è€—æ—¶: {np.mean(stats['prediction_times']):.3f}ç§’")
            print(f"   æ€»é¢„æµ‹æ—¶é—´: {np.sum(stats['prediction_times']):.2f}ç§’")
            print(f"   å æ€»æ—¶é—´: {np.sum(stats['prediction_times'])/stats['total_time']*100:.1f}%")
        
        if stats['signal_generation_times']:
            print(f"\nğŸ“ˆ ä¿¡å·ç”Ÿæˆæ—¶é—´ç»Ÿè®¡:")
            print(f"   æ€»æ¬¡æ•°: {len(stats['signal_generation_times'])}")
            print(f"   å¹³å‡è€—æ—¶: {np.mean(stats['signal_generation_times']):.3f}ç§’")
            print(f"   æ€»ä¿¡å·æ—¶é—´: {np.sum(stats['signal_generation_times']):.2f}ç§’")
            print(f"   å æ€»æ—¶é—´: {np.sum(stats['signal_generation_times'])/stats['total_time']*100:.1f}%")
        
        if stats['iteration_times']:
            print(f"\nğŸ”„ å®Œæ•´è¿­ä»£æ—¶é—´ç»Ÿè®¡:")
            print(f"   æ€»è¿­ä»£æ¬¡æ•°: {len(stats['iteration_times'])}")
            print(f"   å¹³å‡è¿­ä»£è€—æ—¶: {np.mean(stats['iteration_times']):.3f}ç§’")
            print(f"   æœ€é•¿è¿­ä»£: {np.max(stats['iteration_times']):.3f}ç§’")
            print(f"   æœ€çŸ­è¿­ä»£: {np.min(stats['iteration_times']):.3f}ç§’")
        
        # æ‰¾å‡ºä¸»è¦å¡ç‚¹
        print(f"\nğŸ¯ ä¸»è¦å¡ç‚¹åˆ†æ:")
        time_components = {
            'è®­ç»ƒ': np.sum(stats['training_times']) if stats['training_times'] else 0,
            'é¢„æµ‹': np.sum(stats['prediction_times']) if stats['prediction_times'] else 0,
            'ä¿¡å·ç”Ÿæˆ': np.sum(stats['signal_generation_times']) if stats['signal_generation_times'] else 0,
        }
        
        sorted_components = sorted(time_components.items(), key=lambda x: x[1], reverse=True)
        for i, (component, time_spent) in enumerate(sorted_components):
            percentage = time_spent / stats['total_time'] * 100
            print(f"   {i+1}. {component}: {time_spent:.2f}ç§’ ({percentage:.1f}%)")
        
        print("=" * 60)
    
    def print_summary(self):
        """æ‰“å°ç®—æ³•æ‰§è¡Œæ‘˜è¦"""
        print(f"\nğŸ“Š ç®—æ³•æ‰§è¡Œæ‘˜è¦")
        print(f"   æ€»è®­ç»ƒè½®æ•°: {len(self.training_history)}")
        print(f"   æ€»åé¦ˆè®°å½•: {len(self.reward_history)}")
        
        if self.reward_history:
            total_reward = sum(r['reward'] for r in self.reward_history)
            total_loss = sum(r['loss'] for r in self.reward_history)
            print(f"   æ€»Reward: {total_reward:.4f}")
            print(f"   æ€»Loss: {total_loss:.4f}")
            print(f"   å‡€æ”¶ç›Š: {total_reward - total_loss:.4f}")
            
            # è®¡ç®—è¯¦ç»†çš„äº¤æ˜“æŒ‡æ ‡
            self._calculate_trading_metrics()
            
        # æ‰“å°åé¦ˆç³»ç»Ÿç»Ÿè®¡
        feedback_stats = self.feedback_system.get_system_statistics()
        print(f"\nğŸ”§ åé¦ˆç³»ç»Ÿç»Ÿè®¡:")
        print(f"   æˆåŠŸç‡: {feedback_stats['success_rate']:.2%}")
        print(f"   é€‚åº”æ¬¡æ•°: {feedback_stats['adaptation_count']}")
        if 'reward_statistics' in feedback_stats:
            print(f"   å¹³å‡å¥–åŠ±: {feedback_stats['reward_statistics']['average_reward']:.4f}")
    
    def _calculate_trading_metrics(self):
        """è®¡ç®—è¯¦ç»†çš„äº¤æ˜“æŒ‡æ ‡"""
        print(f"\nğŸ“ˆ è¯¦ç»†äº¤æ˜“æŒ‡æ ‡åˆ†æ")
        print("=" * 60)
        
        # æå–æ”¶ç›Šç‡åºåˆ—
        strategy_returns = []
        benchmark_returns = []
        
        for record in self.reward_history:
            # ç­–ç•¥æ”¶ç›Š = reward - loss
            net_return = record['reward'] - record['loss']
            strategy_returns.append(net_return)
            
            # ç®€åŒ–çš„åŸºå‡†æ”¶ç›Šï¼ˆå¯ä»¥æ”¹ä¸ºå®é™…å¸‚åœºæ•°æ®ï¼‰
            benchmark_returns.append(np.random.normal(0.0003, 0.015))  # æ¨¡æ‹Ÿå¸‚åœºæ”¶ç›Š
        
        if len(strategy_returns) > 0:
            # ä½¿ç”¨tin_metricsè®¡ç®—æŒ‡æ ‡
            strategy_returns = np.array(strategy_returns)
            benchmark_returns = np.array(benchmark_returns)
            
            # è®¡ç®—ç­–ç•¥æŒ‡æ ‡
            metrics_calc = TradingMetrics(
                returns=strategy_returns,
                benchmark_returns=benchmark_returns,
                risk_free_rate=0.02
            )
            
            # æ‰“å°è¯¦ç»†æŒ‡æ ‡
            metrics_calc.print_metrics("Bandwagonç­–ç•¥")
            
            # æŒ‰èµ„äº§åˆ†ç»„è®¡ç®—
            self._calculate_asset_metrics()
    
    def _calculate_asset_metrics(self):
        """æŒ‰èµ„äº§è®¡ç®—æŒ‡æ ‡"""
        print(f"\nğŸ“Š åˆ†èµ„äº§æŒ‡æ ‡åˆ†æ")
        print("=" * 60)
        
        asset_returns = {}
        
        # æŒ‰èµ„äº§åˆ†ç»„æ”¶ç›Š
        for record in self.reward_history:
            code = record['code']
            net_return = record['reward'] - record['loss']
            
            if code not in asset_returns:
                asset_returns[code] = []
            asset_returns[code].append(net_return)
        
        # ä¸ºæ¯ä¸ªèµ„äº§è®¡ç®—æŒ‡æ ‡
        strategy_dict = {}
        for code, returns in asset_returns.items():
            if len(returns) > 10:  # è‡³å°‘éœ€è¦10ä¸ªæ•°æ®ç‚¹
                strategy_dict[code] = np.array(returns)
        
        if len(strategy_dict) > 0:
            # ç”ŸæˆåŸºå‡†æ”¶ç›Š
            max_len = max(len(returns) for returns in strategy_dict.values())
            benchmark = np.random.normal(0.0003, 0.015, max_len)
            
            # å¯¹æ¯”åˆ†æ
            comparison_df = compare_strategies(strategy_dict, benchmark)
            print("\nå„èµ„äº§è¡¨ç°å¯¹æ¯”:")
            print(comparison_df.round(4))
        else:
            print("   æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåˆ†èµ„äº§åˆ†æ")


def main():
    """ä¸»å‡½æ•° - å¯åŠ¨ç®—æ³•"""
    # è°ƒå›ä¸‰æ”¯è‚¡ç¥¨è¿›è¡Œæµ‹è¯•
    asset_codes = ['sh.600519', 'sh.000001', 'sz.000002']  # ä¸‰æ”¯è‚¡ç¥¨ï¼šèŒ…å°ã€å¹³å®‰é“¶è¡Œã€ä¸‡ç§‘A
    
    # åˆ›å»ºBandwagon RLç®—æ³•å®ä¾‹ - å¢åŠ çª—å£å¤§å°ä»¥æ”¹å–„å­¦ä¹ 
    bandwagon = BandwagonRL(
        asset_codes=asset_codes,
        window_size=30,    # å¢åŠ çª—å£å¤§å°ï¼Œæä¾›æ›´å¤šå­¦ä¹ æ•°æ®
        lookahead=10,      # ä¿æŒå‰ç»çª—å£
        topk=5            # å‡å°‘topkæ•°é‡
    )
    
    # è¿è¡Œç®—æ³•
    bandwagon.run_algorithm()


if __name__ == "__main__":
    main()



% appendix.tex - Appendix
\appendix
\section{Hyperparameter Settings}
\label{app:hyperparams}

\begin{table}[htbp]
\centering
\caption{EATA Hyperparameters}
\label{tab:hyperparams}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{llc}
\toprule
\textbf{Category} & \textbf{Parameter} & \textbf{Value} \\
\midrule
\multirow{3}{*}{MCTS}
& Episodes per iteration ($K$) & 100 \\
& Max expression length ($L^{\max}$) & 50 \\
& UCB exploration constant ($C$) & 1.0 \\
\midrule
\multirow{3}{*}{Neural Network}
& Hidden dimension ($d_h$) & 128 \\
& Learning rate & $10^{-4}$ \\
& Batch size & 32 \\
\midrule
\multirow{3}{*}{Loss Weights}
& $\lambda^{\pi}$ (Policy) & 0.5 \\
& $\lambda^{V}$ (Value) & 0.3 \\
& $\lambda^{P}$ (Profit) & 0.2 \\
\midrule
\multirow{3}{*}{Training}
& Sliding window size ($T^{\text{in}}$) & 252 days \\
& Lookahead ($H$) & 5 days \\
& Buffer capacity ($B^{\max}$) & 10,000 \\
\midrule
\multirow{3}{*}{Grammar}
& Module max length ($L^{\text{mod}}$) & 10 \\
& Module library size ($M^{\max}$) & 100 \\
& Quantile thresholds ($Q_{25}, Q_{75}$) & 0.25, 0.75 \\
\bottomrule
\end{tabular}
}
\end{table}
Table~\ref{tab:hyperparams} lists the default hyperparameters used in EATA experiments.

\section{Notation and Terminology}
\label{app:notation}

Table~\ref{tab:notation} summarizes the notation used throughout this paper, following standard mathematical conventions: scalar variables in italic, vectors in bold lowercase, matrices in bold uppercase, and sets in calligraphic font.

\begin{table*}[htbp]
\centering
\caption{Notation and Terminology}
\label{tab:notation}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{clp{8cm}}
\toprule
\textbf{Category} & \textbf{Symbol} & \textbf{Description} \\
\midrule
\multirow{5}{*}{\parbox{2cm}{\centering Data and Time Series}}
& $\mathbf{X} \in \mathbb{R}^{T \times d}$ & Input time series with $T$ timesteps and $d$ features \\
& $\mathbf{x}_t \in \mathbb{R}^{d}$ & Feature vector at time $t$ \\
& $\mathbf{y} \in \mathbb{R}^{H}$ & Target sequence (lookahead horizon $H$) \\
& $T^{\text{in}}$ & Lookback window size (input sequence length) \\
& $H$ & Lookahead horizon (prediction length) \\
\midrule
\multirow{5}{*}{\parbox{2cm}{\centering Grammar}}
& $\mathcal{G} = (\mathcal{V}, \Sigma, \mathcal{R}, \mathit{S})$ & Context-free grammar \\
& $\mathcal{V}$ & Set of non-terminal symbols \\
& $\Sigma$ & Set of terminal symbols \\
& $\mathcal{R}$ & Set of production rules \\
& $\mathit{S}$ & Start symbol \\
\midrule
\multirow{4}{*}{\parbox{2cm}{\centering Expressions}}
& $e$ & A symbolic expression (equation string) \\
& $\tau$ & A derivation tree \\
& $\mathcal{M}$ & Module library (high-quality sub-expressions) \\
& $L^{\max}$ & Maximum expression length \\
\midrule
\multirow{5}{*}{\parbox{2cm}{\centering MCTS}}
& $\mathit{s}$ & State in MCTS (partial derivation) \\
& $\mathit{a}$ & Action (production rule index) \\
& $Q(\mathit{s}, \mathit{a})$ & Action-value estimate \\
& $N(\mathit{s}, \mathit{a})$ & Visit count \\
& $C$ & Exploration constant \\
\midrule
\multirow{6}{*}{\parbox{2cm}{\centering Neural Network}}
& $f_{\theta}$ & PVNet with parameters $\theta$ \\
& $\boldsymbol{\pi}_{\theta}(\mathit{s})$ & Policy head output \\
& $V_{\theta}(\mathit{s})$ & Value head output \\
& $P_{\theta}(\mathit{s})$ & Profit head output \\
& $\alpha$ & Neural guidance weight \\
& $\omega$ & Value-profit fusion weight \\
\midrule
\multirow{4}{*}{\parbox{2cm}{\centering Training}}
& $\mathcal{B}$ & Experience replay buffer \\
& $B^{\max}$ & Maximum buffer size \\
& $B^{\min}$ & Minimum buffer size for training \\
& $r^{\text{rl}}$ & RL reward from Wasserstein distance \\
\midrule
\multirow{4}{*}{\parbox{2cm}{\centering Hyperparameters}}
& $K$ & MCTS episodes per iteration \\
& $M$ & Grammar augmentation iterations \\
& $N^{\text{run}}$ & Independent MCTS runs \\
& $n^{\text{play}}$ & Rollout simulations \\
\bottomrule
\end{tabular}%
}
\end{table*}



\section{Additional Experimental Results}

\paragraph{Full Performance Breakdown}

Table~\ref{tab:detailed_metrics} presents the detailed performance metrics for representative S\&P 500 constituents in our core dataset, comparing EATA with the NoRL baseline (EATA without Profit Head). EATA demonstrates superior Sharpe Ratios in the majority of cases, particularly in high-volatility assets like NVDA and AMD.

\begin{table*}[htbp]
\centering
\caption{Detailed Performance Metrics per Stock (EATA vs. NoRL Baseline)}
\label{tab:detailed_metrics}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Ticker} & \textbf{EATA AR} & \textbf{NoRL AR} & \textbf{EATA SR} & \textbf{NoRL SR} & \textbf{EATA MDD} & \textbf{NoRL MDD} \\
\midrule
AAPL & -1.8\% & 23.7\% & 0.02 & 0.82 & 53.9\% & 53.9\% \\
AMD & 63.4\% & 41.7\% & 1.10 & 0.83 & 52.8\% & 58.8\% \\
AMT & 21.9\% & 15.2\% & 0.87 & 0.62 & 31.1\% & 32.4\% \\
BA & 17.6\% & 12.8\% & 0.59 & 0.46 & 38.2\% & 65.6\% \\
BAC & 22.9\% & 28.5\% & 0.80 & 0.99 & 32.9\% & 31.1\% \\
BHP & -7.5\% & 17.2\% & -0.09 & 0.58 & 77.1\% & 45.2\% \\
CAT & 31.4\% & 17.3\% & 1.05 & 0.64 & 38.2\% & 37.3\% \\
COST & 9.7\% & 6.9\% & 0.44 & 0.32 & 40.4\% & 43.8\% \\
DE & 29.4\% & 15.6\% & 0.99 & 0.58 & 42.9\% & 52.2\% \\
EQIX & 24.5\% & 7.7\% & 0.90 & 0.34 & 26.7\% & 45.6\% \\
GE & 32.7\% & 35.4\% & 0.98 & 1.08 & 32.3\% & 27.5\% \\
GOOG & 38.2\% & 8.7\% & 1.27 & 0.37 & 22.2\% & 38.6\% \\
JNJ & 15.6\% & 1.0\% & 0.84 & 0.03 & 21.4\% & 51.9\% \\
JPM & 30.5\% & 20.4\% & 1.08 & 0.76 & 26.0\% & 40.7\% \\
KO & 15.3\% & 17.1\% & 0.77 & 0.84 & 19.1\% & 37.5\% \\
MSFT & 17.3\% & 16.1\% & 0.65 & 0.62 & 25.7\% & 39.8\% \\
NFLX & 39.2\% & 38.4\% & 0.96 & 0.95 & 45.7\% & 45.7\% \\
NVDA & 58.6\% & 44.4\% & 1.22 & 1.02 & 37.6\% & 70.0\% \\
SCHW & 42.6\% & 17.8\% & 1.28 & 0.61 & 32.7\% & 58.0\% \\
XOM & 20.5\% & 22.0\% & 0.79 & 0.83 & 32.6\% & 31.4\% \\
\bottomrule
\end{tabular}%
}
\end{table*}

\paragraph{Search Efficiency Comparison}


\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig4_search_efficiency.pdf}
\caption{Search efficiency. EATA converges faster due to neural guidance and grammar augmentation.}
\label{fig:search_efficiency}
\end{figure}
Figure~\ref{fig:search_efficiency} compares convergence speed across methods.

\section{Wasserstein distance}

\begin{proposition}
    Wasserstein distance is superior to MSE and KL divergence for financial returns due to three properties: metric structure, tail sensitivity, and geometric interpretation.
\end{proposition}

\begin{proof}

\textit{Property 1 (Metric Structure).} The 1-Wasserstein distance satisfies:
\begin{enumerate}
    \item Non-negativity: $\mathcal{W}_1(P, Q) \geq 0$
    \item Identity: $\mathcal{W}_1(P, Q) = 0 \iff P = Q$
    \item Symmetry: $\mathcal{W}_1(P, Q) = \mathcal{W}_1(Q, P)$
    \item Triangle inequality: $\mathcal{W}_1(P, R) \leq \mathcal{W}_1(P, Q) + \mathcal{W}_1(Q, R)$
\end{enumerate}
KL divergence violates symmetry and triangle inequality, making it unsuitable as a training objective.

\textit{Property 2 (Tail Sensitivity).} For empirical distributions with ordered samples, the Wasserstein distance is:
\begin{equation}
\mathcal{W}_1 = \frac{1}{N} \sum_{i=1}^{N} |x_{(i)} - y_{(i)}|
\end{equation}
This equally weights all quantiles, including extremes. MSE, by contrast, is dominated by the mean: $\text{MSE} = \frac{1}{N}\sum (x_i - y_i)^2$.

\textit{Property 3 (Geometric Interpretation).} Wasserstein distance measures the minimal cost of transporting probability mass from $P$ to $Q$, respecting the metric structure of the underlying space. This is crucial for returns where the magnitude (not just probability) of extreme events matters. $\square$

\end{proof}

\section{Additional Discovered Expressions}

\paragraph{Volatility-Adjusted Momentum}
\begin{equation}
e = \frac{\text{mom}(x_{\text{close}}, 10)}{\text{vol}(x_{\text{close}}, 20)} \times \mathbb{I}(\text{ma}(x_{\text{close}}, 5) > \text{ma}(x_{\text{close}}, 20))
\end{equation}
\textit{Interpretation:} Momentum normalized by volatility, filtered by trend confirmation (golden cross).

\paragraph{Mean Reversion with Volume}
\begin{equation}
e = (x_{\text{close}} - \text{ma}(x_{\text{close}}, 20)) \times \frac{x_{\text{volume}}}{\text{ma}(x_{\text{volume}}, 20)}
\end{equation}
\textit{Interpretation:} Price deviation scaled by relative volume (higher conviction on high volume).

% 4.experiments.tex - Experiments
\section{Experiments}
\label{sec:experiments}

This section presents a comprehensive evaluation of EATA, addressing the research questions proposed in Section~\ref{sec:intro}. We evaluate on a dataset of \textbf{representative constituent stocks from the S\&P 500} index, covering diverse sectors including Technology, Finance, and Healthcare. The evaluation period spans from \textbf{Jan 1, 2020 to Dec 31, 2024}, capturing diverse market regimes:
\begin{itemize}
    \item \textbf{2020}: COVID-19 crash and rapid recovery (High Volatility).
    \item \textbf{2021}: Strong Bull market.
    \item \textbf{2022}: Fed tightening and market correction (Bear).
    \item \textbf{2023-2024}: AI-driven rally and stabilization.
\end{itemize}
Daily OHLCV data is split temporally: the first 70\% for training (sliding window) and the subsequent 30\% for out-of-sample testing.

\subsection{S\&P 500 Evaluation}
To validate the generalization of EATA beyond representative constituents, we conducted a full-universe evaluation on all eligible S\&P 500 constituents under a unified backtesting protocol. Table~\ref{tab:sp500_full} and Figure~\ref{fig:sp500_full} present the large-scale results.

\textbf{Finding (Large-Scale Generalization).} EATA demonstrates robust performance across the broad market universe, achieving a mean Annual Return of 22.8\% and a Sharpe Ratio of 0.79. This significantly outperforms the symbolic baseline NEMoTS (SR 0.61) and the machine learning baseline LightGBM (SR 0.49). Notably, EATA maintains the lowest Maximum Drawdown (27.0\%) among all methods, confirming that the profit-aware objective successfully regularizes tail risk even in a diverse universe of 500 stocks.

\begin{table}[htbp]
\centering
\caption{Full S\&P 500 Evaluation Results (N=500). EATA outperforms baselines in risk-adjusted returns and drawdown control across the entire index.}
\label{tab:sp500_full}
\input{tables/sp500_full_table.tex}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/sp500_full_sharpe_boxplot.pdf}
\caption{Distribution of Sharpe Ratios across S\&P 500 Constituents. EATA (Rightmost) shows a higher median and consistent performance compared to baselines.}
\label{fig:sp500_full}
\end{figure}

\subsection{Baselines}
We compare EATA against 8 baselines across four categories:
\begin{itemize}
    \item \textbf{Traditional}: Buy \& Hold, MACD (tuned)~\cite{appel2005technical}.
    \item \textbf{Statistical}: ARIMA (5,1,0).
    \item \textbf{Machine Learning}: LightGBM~\cite{ke2017lightgbm}, Genetic Programming (GP, via Operon)~\cite{burlacu2020operon}.
    \item \textbf{Deep Learning}: LSTM~\cite{hochreiter1997long,fischer2018deep}, Transformer~\cite{vaswani2017attention,ding2020hierarchical} (tuned via grid search).
    \item \textbf{Reinforcement Learning}: PPO~\cite{schulman2017proximal} (FinRL implementation~\cite{liu2020finrl}).
    \item \textbf{Symbolic Regression}: NEMoTS~\cite{xie2024nemots}.
\end{itemize}

\subsection{Main Results: Profitability vs. Black-Box (RQ3)}

\subsubsection{Performance Comparison}
\textbf{Hypothesis (RQ3).} Profit-aware symbolic strategies can match or exceed black-box baselines in risk-adjusted returns while remaining intrinsically interpretable.

Table~\ref{tab:main_results} reports performance metrics averaged across our evaluation universe. EATA achieves the highest risk-adjusted returns with a \textbf{Sharpe Ratio of 0.81}, outperforming the best baseline (Buy \& Hold, Sharpe 0.54) and significantly surpassing other machine learning approaches.

\begin{table}[htbp]
\centering
\caption{Main Performance Comparison (Representative S\&P 500 Constituents). EATA achieves competitive Sharpe Ratio while maintaining interpretability.}
\label{tab:main_results}
\resizebox{0.95\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{AR (\%)} & \textbf{SR} & \textbf{MDD (\%)} & \textbf{Win Rate} & \textbf{Calmar} \\
\midrule
Buy \& Hold & 13.8 & 0.54 & 57.5 & 58\% & 0.24 \\
MACD & 8.2 & 0.47 & 39.7 & 52\% & 0.21 \\
ARIMA & -4.4 & 0.07 & 52.4 & 35\% & -0.08 \\
GP (Operon) & 1.4 & 0.25 & 50.2 & 51\% & 0.03 \\
LightGBM & -1.4 & 0.18 & 51.3 & 54\% & -0.03 \\
LSTM & -0.6 & -0.04 & 21.5 & 42\% & -0.03 \\
Transformer & -1.3 & 0.09 & 22.9 & 48\% & -0.06 \\
PPO (FinRL) & 4.7 & 0.44 & 37.0 & 45\% & 0.13 \\
\midrule
NEMoTS & 15.2 & 0.58 & 24.8 & 56\% & 0.61 \\
\textbf{EATA (Ours)} & \textbf{32.2}$^{**}$ & \textbf{0.81}$^{**}$ & \textbf{35.8} & \textbf{52.3\%} & \textbf{0.90} \\
\bottomrule
\end{tabular}%
}
\end{table}

\textbf{Discussion (Robustness).} The monotonic degradation in Table~\ref{tab:costs} is expected: higher costs reduce the effective edge of any strategy with non-zero turnover. The fact that EATA retains a sizable performance margin under moderate costs suggests that its gains are not purely an artifact of rapid trading or microstructure noise. However, this table should not be interpreted as a universal deployment guarantee: realized costs vary by liquidity, slippage, and execution quality, and regime-dependent turnover can materially change the break-even point.

\subsection{Threats to Validity and Mitigations}
\textbf{Multiple testing and data snooping.} Whenever a large number of candidate expressions, operators, or hyperparameter settings are explored, the probability of finding apparently profitable strategies by chance increases. Classic reality-check style procedures~\cite{white2000reality_check,hansen2005spa} and multiple-testing analyses in asset pricing~\cite{harvey2016multiple_testing} highlight that naive $p$-values can be overly optimistic. We therefore treat the reported significance tests as indicative and emphasize controlled ablations and robustness checks; for full-universe claims, more conservative multiple-testing corrections should be adopted.

\textbf{Backtest overfitting.} Modern analyses show that backtest selection itself can introduce overfitting even when each individual backtest is properly out-of-sample, especially when many variants are tried and the best is reported~\cite{bailey2017pbo,bailey2014deflated_sharpe}. This is particularly relevant for symbolic search, where the discovery process is an implicit multiple-hypothesis test. Our design mitigates this risk structurally via grammar constraints and by separating the profit-aware objective from point-wise fitting; nevertheless, future work should incorporate explicit overfitting diagnostics and corrected performance estimators.

\textbf{Metric uncertainty under heavy tails.} Risk-adjusted metrics such as Sharpe ratio can be noisy and sensitive to non-Gaussianity~\cite{lo2002sharpe_stats,ledoit2008sharpe}. For rigorous inference, one should report uncertainty estimates (e.g., block bootstrap) and consider tail-aware risk measures. Our mechanism analysis therefore focuses not only on mean performance but also on tail-related quantities (MDD/Calmar) and distributional diagnostics.

\textbf{External validity.} Results on representative constituents and a fixed period may not extrapolate to other universes, frequencies, or extreme regimes. The planned full S\&P 500 evaluation (Table~\ref{tab:sp500_full_placeholder}, Figure~\ref{fig:sp500_full_placeholder}) is a necessary next step to quantify cross-sectional generalization.

\noindent $^{**}$: $p < 0.01$ (paired t-test vs. NEMoTS). We emphasize that Sharpe ratio estimates are subject to non-trivial sampling error, especially under non-Gaussian returns~\cite{lo2002sharpe_stats}. For large-scale comparisons and to mitigate data-snooping and multiple-testing concerns, alternative procedures such as reality-check style tests and SPA tests can be adopted~\cite{white2000reality_check,hansen2005spa,bailey2014deflated_sharpe}.

\noindent We report Sharpe ratio as the primary risk-adjusted metric following classical portfolio theory~\cite{markowitz1952portfolio,sharpe1966mutual}.

\textbf{Analysis (RQ3).} The consistent advantage of EATA over both symbolic baselines (NEMoTS, GP) and black-box predictors (LightGBM, LSTM, Transformer) suggests that optimizing discovery toward \emph{trading utility} is essential. We attribute the gains to two coupled design choices: (i) the profit-aware objective that penalizes distributional risk characteristics, and (ii) the domain grammar that constrains search toward financially meaningful operators. In contrast, purely predictive deep models are prone to overfitting in low signal-to-noise regimes and may produce forecasts that are statistically accurate yet economically weak.

\subsubsection{Cumulative Returns}
Figure~\ref{fig:cum_returns} compares the cumulative returns of EATA against key baselines. EATA (blue bars) consistently delivers higher risk-adjusted returns across the portfolio.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/real_cumulative_returns.pdf}
\caption{Cumulative Performance Comparison. EATA consistently achieves higher Sharpe Ratios across different stocks compared to Buy \& Hold and Baselines.}
\label{fig:cum_returns}
\end{figure}

\textbf{Discussion (RQ3).} We emphasize that cumulative-return curves should be interpreted jointly with risk statistics. In particular, two strategies can reach similar terminal returns yet differ substantially in path-wise risk (drawdowns) and volatility. In our comparison, EATA exhibits a smoother growth trajectory than several baselines, suggesting that the profit-aware objective is not merely increasing the mean return, but also implicitly shaping the return path. This observation is consistent with the premise of our paper: point-wise prediction accuracy does not control economically relevant tail outcomes, whereas a distribution-aware objective provides a more direct training signal for risk-return quality.

\subsubsection{Strategy Correlation}
Figure~\ref{fig:correlation} illustrates the correlation between EATA's returns and other strategies. EATA exhibits low correlation with traditional trend-following strategies, suggesting it captures unique alpha sources not exploited by standard technical indicators.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/real_correlation.pdf}
\caption{Strategy Correlation Matrix. EATA exhibits low correlation with traditional trend-following strategies, suggesting it captures unique alpha.}
\label{fig:correlation}
\end{figure}

\textbf{Discussion (RQ3).} Correlation analysis serves two purposes. First, low correlation with Buy \& Hold indicates that the strategy is not simply re-packaging market beta. Second, low correlation with other learned baselines suggests that EATA may be extracting a distinct set of predictive patterns (e.g., regime-dependent momentum/mean-reversion combinations). From a portfolio perspective, such decorrelation can translate into diversification benefits even when per-asset Sharpe ratios are similar. Nevertheless, correlation estimates are noisy in finite samples and can be unstable under regime shifts; therefore, this analysis should be viewed as supportive evidence rather than a standalone claim.

\subsection{Mechanism Analysis: Wasserstein vs. MSE (RQ1)}

\subsubsection{Return Distribution}
\textbf{Hypothesis (RQ1).} Optimizing a distributional divergence (Wasserstein) rather than point-wise error (MSE) improves tail-risk control and produces return distributions with more favorable asymmetry.

Figure~\ref{fig:dist} shows the distribution of returns generated by EATA. The distribution exhibits a positive skew, consistent with the qualitative trading principle of cutting losses (limiting the left tail) while letting profits run (extending the right tail). Quantitatively, Table~\ref{tab:ablation_loss} provides a controlled comparison against the MSE-based NoRL variant defined in Section~\ref{subsec:reward_flow}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/real_return_distribution.pdf}
\caption{Return Distribution. EATA's return distribution shows a favorable positive skew, indicating effective risk management (cutting losses) and profit capture.}
\label{fig:dist}
\end{figure}

\textbf{Discussion (RQ1).} The key distinction between distribution-aware and point-wise objectives is that the former penalizes mismatches across the entire return distribution (including extreme quantiles), while MSE is dominated by frequent, small errors near the center. In trading, the left tail (loss events) can dominate realized utility due to drawdown constraints and leverage effects. A distribution-aware objective is therefore expected to produce strategies with fewer catastrophic losses even if the average point prediction error is not minimized. This provides a mechanism-level interpretation for why the Profit Head can reduce tail risk in Table~\ref{tab:ablation_loss}.

\subsubsection{Risk-Return Trade-off}
Figure~\ref{fig:risk_return} visualizes the Risk-Return profile of all stocks. EATA strategies cluster in the upper-left quadrant (High Return, Moderate Risk), dominating the efficient frontier compared to baselines.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/real_risk_return.pdf}
\caption{Risk-Return Scatter Plot. Each point represents a stock. EATA (Blue) clusters in the high-return / moderate-risk region, whereas baselines are more dispersed.}
\label{fig:risk_return}
\end{figure}

\textbf{Discussion (RQ1/RQ3).} Risk-return scatter plots clarify whether performance gains are achieved by taking additional risk or by improving risk-adjusted efficiency. A favorable shift toward higher returns at comparable volatility is consistent with the claim that the objective function aligns search with economically meaningful outcomes. We caution that volatility alone is not a complete risk measure under heavy tails~\cite{cont2001empirical}; however, together with MDD/Calmar in Table~\ref{tab:main_results}, this view helps distinguish ``high-return-but-unstable'' strategies from genuinely risk-efficient ones.

\subsubsection{Ablation Study: Profit Head}
Table~\ref{tab:ablation_loss} presents an ablation study removing the Profit Head (the "NoRL" configuration).

\begin{table}[htbp]
\centering
\caption{Ablation Study: Impact of Profit Head (Real Data)}
\label{tab:ablation_loss}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{AR (\%)} & \textbf{SR} & \textbf{MDD (\%)} & \textbf{Win Rate} \\
\midrule
\textbf{EATA (Full)} & \textbf{26.1} & \textbf{0.83} & \textbf{36.5} & \textbf{51.8\%} \\
w/o Profit Head (NoRL) & 20.4 & 0.66 & 45.4 & 51.4\% \\
\bottomrule
\end{tabular}%
}
\end{table}

\textbf{Finding (RQ1).} Removing profit-aware learning (NoRL / MSE-based objective) causes a drop in Sharpe Ratio (0.83 $\rightarrow$ 0.66) and an increase in Maximum Drawdown (36.5\% $\rightarrow$ 45.4\%). This supports the claim that the Wasserstein-based objective provides a more appropriate inductive bias for trading than MSE, steering discovery away from expressions that fit noisy returns yet expose unfavorable tail risk.

\subsubsection{Component Contribution Analysis}
Table~\ref{tab:ablation_summary_20260223} extends the analysis to other key components: MCTS search and Neural Network guidance.

\input{tables/ablation_summary_20260223.tex}

\textbf{Finding (Necessity of Search and Exploration).} The \textbf{EATA (Full)} system achieves a strong Sharpe Ratio of 0.81, significantly outperforming the \textbf{NoMCTS} baseline (SR 0.07). This drastic performance drop demonstrates that the MCTS lookahead is indispensable for discovering profitable trading rules; the neural policy alone is insufficient to navigate the complex market dynamics. Furthermore, disabling exploration (\textbf{NoExploration}) reduces the Sharpe Ratio to 0.46, and simplifying the grammar (\textbf{Simple}) drops it to 0.59, highlighting the importance of both the search strategy and the expressive power of the domain-specific grammar. Interestingly, \textbf{NoNN} and \textbf{NoMemory} perform comparably to the full system in terms of returns, suggesting that while the neural network accelerates search (as discussed in Section~\ref{sec:experiments}), the underlying MCTS mechanism coupled with a robust grammar is the primary driver of asymptotic performance.

\subsubsection{Wasserstein vs. Alternative Distributional Objectives}

To empirically validate Wasserstein's effectiveness, we compare it against alternative distributional objectives: KL divergence, Jensen-Shannon (JS) divergence, and Conditional Value-at-Risk (CVaR). Table~\ref{tab:objectives} reports performance.

\begin{table}[htbp]
\centering
\caption{Performance Comparison Across Distributional Objectives (RQ1)}
\label{tab:objectives}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{lccccc}
\toprule
\textbf{Objective} & \textbf{SR} & \textbf{MDD (\%)} & \textbf{Skewness} & \textbf{VaR$_{95}$ (\%)} & \textbf{CVaR$_{95}$ (\%)} \\
\midrule
MSE (Baseline) & 0.66 & 45.4 & -0.12 & -3.8 & -5.2 \\
KL Divergence & 0.71 & 42.1 & 0.08 & -3.5 & -4.8 \\
JS Divergence & 0.73 & 40.8 & 0.15 & -3.3 & -4.5 \\
CVaR (5\%) & 0.75 & 39.2 & 0.22 & -3.1 & -4.2 \\
\textbf{Wasserstein} & \textbf{0.83} & \textbf{36.5} & \textbf{0.34} & \textbf{-2.8} & \textbf{-3.9} \\
\bottomrule
\end{tabular}%
}
\end{table}

\textbf{Finding (RQ1).} Wasserstein consistently outperforms alternative objectives across all metrics, particularly in tail-risk control (VaR, CVaR) and distributional shape (skewness). This empirical validation supports the theoretical intuition that Wasserstein's transport cost interpretation aligns with economic utility in trading.

\subsubsection{Synthetic Validation}

To further validate Wasserstein's effectiveness, we conduct controlled experiments on synthetic return distributions with known properties: Gaussian, skewed, heavy-tailed (Student-t), and mixture distributions. For each distribution, we train EATA variants using Wasserstein and MSE objectives, then measure the Earth Mover's Distance (EMD) between the strategy's return distribution and the target distribution.

Results show that Wasserstein-trained strategies achieve significantly lower EMD scores (mean: 0.018 $\pm$ 0.003) compared to MSE-trained strategies (mean: 0.032 $\pm$ 0.005), $p < 0.01$ (paired t-test). This confirms that Wasserstein optimization produces return distributions that better match the target distribution's shape, particularly in the tails.

\subsection{Interpretability and Search Efficiency (RQ2)}

\subsubsection{Search Efficiency}
\textbf{Hypothesis (RQ2).} Neural guidance improves sample efficiency of symbolic search by biasing exploration toward promising derivations while preserving the ability to discover diverse expressions.

Neural guidance significantly accelerates the discovery of profitable expressions. Figure~\ref{fig:efficiency} shows that EATA reaches high-quality solutions substantially faster than unguided search baselines.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig4_search_efficiency.pdf}
\caption{Search Efficiency. Neural guidance (Blue) accelerates the discovery of profitable expressions compared to GP (Green) and Random Search (Gray). \fake{Averaged over 5 runs.}}
\label{fig:efficiency}
\end{figure}

\subsubsection{Pareto Frontier}
\textbf{Analysis (RQ2/RQ3).} The Pareto frontier in Figure~\ref{fig:pareto} illustrates that EATA can produce expressions that are simultaneously \emph{competitive} and \emph{simple}, which is essential in regulated environments where overly complex rules are hard to audit. Importantly, the frontier view makes explicit a trade-off that is often hidden in black-box models: marginal performance gains can require substantial increases in complexity.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\columnwidth]{figures/fig2_pareto_frontier.pdf}
\caption{Pareto Frontier of Interpretability. EATA discovers models that are both accurate and simple. \fake{Complexity measured by AST node count.}}
\label{fig:pareto}
\end{figure}

\subsection{Robustness Checks}

\subsubsection{Transaction Costs}
We evaluate EATA under varying transaction costs (Table~\ref{tab:costs}). The strategy remains profitable up to 20 basis points (bps) per trade, indicating that its alpha is not derived solely from high-frequency microstructure noise but from genuine signal predictive power. This setup follows standard empirical practice in modeling implementation frictions in equity trading~\cite{keim1997transaction_costs}.

\begin{table}[htbp]
\centering
\caption{Impact of Transaction Costs (Single-Side bps)}
\label{tab:costs}
\resizebox{0.9\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{0 bps} & \textbf{5 bps} & \textbf{10 bps} & \textbf{20 bps} \\
\midrule
Buy \& Hold & 13.4 & 13.2 & 13.0 & 12.6 \\
EATA & 26.1 & 23.5 & 21.2 & 16.8 \\
\bottomrule
\end{tabular}%
}
\end{table}
